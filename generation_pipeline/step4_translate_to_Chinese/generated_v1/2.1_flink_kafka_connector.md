# Flink Kafka 连接器

**Learning Point**: 2.1 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.2 (DataStream API), Kafka knowledge (you already have this)
**Version**: Flink 2.2.0, Kafka 3.7.2

---

## 工程化定义

**Flink Kafka Connector**: 使 Flink 能够从 Kafka topic 读取和写入的库。为 Kafka 集成提供 source 和 sink 实现。

**FlinkKafkaConsumer** (deprecated): 旧版 Kafka source connector。在 Flink 1.12+ 中被 KafkaSource 取代。

**KafkaSource** (new): 具有改进性能和功能的现代 Kafka source connector。使用 Kafka 的新 consumer API。

**FlinkKafkaProducer** (deprecated): 旧版 Kafka sink connector。在 Flink 1.12+ 中被 KafkaSink 取代。

**KafkaSink** (new): 具有改进可靠性和功能的现代 Kafka sink connector。

**Consumer Group Management**: Flink 自动管理 Kafka consumer groups，处理分区分配和 offset 提交。

---

## 通俗解释

将 Flink Kafka connector 视为 Flink 和 Kafka 之间的桥梁：

- **KafkaSource** = 读取桥梁：Flink 从 Kafka topic 读取数据
- **KafkaSink** = 写入桥梁：Flink 向 Kafka topic 写入数据
- **Consumer Group** = 读取团队：Flink workers 协调从不同的 Kafka 分区读取

**为什么需要**: MarketLag 项目通过 Kafka 流动所有数据 - RSS 事件和 Polymarket 价格在 Kafka topic 中，Flink 作业从 Kafka 读取并写入。

---

## 类比理解

如果您了解 **Spark 的 Kafka connector**（您从 Spark 经验中了解），Flink 的 connector 类似：
- Spark readStream from Kafka = Flink KafkaSource
- Spark writeStream to Kafka = Flink KafkaSink
- Consumer groups 工作方式相同

关键区别：Flink 使用更新的 Kafka consumer API，提供更好的性能和 exactly-once 语义。

---

## 与已学内容的关系

- **1.2 DataStream API**: KafkaSource 创建 DataStreams
- **Kafka**: 您已经从经验中了解 Kafka - Flink 与它集成
- **2.2 Kafka Table Connector**: 更高级的 SQL 接口（在底层使用 KafkaSource）
- **Consumer Groups**: 类似于您已经了解的 Kafka consumer groups

---

## 伪代码（基于源码）

Based on Flink 2.2.0 source code:

```java
// KafkaSource (simplified)
class KafkaSource<T> implements SourceFunction<T> {
    List<String> topics;
    KafkaConsumerConfig config;

    void run(SourceContext<T> ctx) {
        KafkaConsumer consumer = createConsumer();
        consumer.subscribe(topics);

        while (running) {
            ConsumerRecords records = consumer.poll();
            for (ConsumerRecord record : records) {
                T value = deserialize(record.value());
                ctx.collectWithTimestamp(value, record.timestamp());
            }

            // Commit offsets (for checkpointing)
            commitOffsets();
        }
    }
}
```

---

## 源码参考

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java

**Key Classes**:
- `KafkaSource`: Modern Kafka source
- `KafkaSink`: Modern Kafka sink
- `FlinkKafkaConsumer`: Legacy source (deprecated)
- `FlinkKafkaProducer`: Legacy sink (deprecated)

---

## Flink 中的 Kafka 消费者配置

### Basic KafkaSource Setup

```java
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.api.common.serialization.SimpleStringSchema;

KafkaSource<String> source = KafkaSource.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setTopics("rss.events")
    .setGroupId("flink-consumer-group")
    .setStartingOffsets(OffsetsInitializer.latest())
    .setValueOnlyDeserializer(new SimpleStringSchema())
    .build();

DataStream<String> stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), "Kafka Source");
```

### Confluent Cloud Authentication

```java
Properties props = new Properties();
props.setProperty("bootstrap.servers", "<confluent-cloud-endpoint>");
props.setProperty("security.protocol", "SASL_SSL");
props.setProperty("sasl.mechanism", "PLAIN");
props.setProperty("sasl.jaas.config",
    "org.apache.kafka.common.security.plain.PlainLoginModule required " +
    "username='<api-key>' password='<api-secret>';");

KafkaSource<String> source = KafkaSource.<String>builder()
    .setBootstrapServers(props.getProperty("bootstrap.servers"))
    .setKafkaConsumerConfig(props)
    .setTopics("rss.events")
    .setGroupId("flink-consumer-group")
    .setStartingOffsets(OffsetsInitializer.latest())
    .setValueOnlyDeserializer(new SimpleStringSchema())
    .build();
```

**MarketLag Project**: Uses Confluent Cloud with SASL_SSL authentication.

---

## Flink 中的 Kafka 生产者配置

### Basic KafkaSink Setup

```java
import org.apache.flink.connector.kafka.sink.KafkaSink;
import org.apache.flink.api.common.serialization.SimpleStringSchema;

KafkaSink<String> sink = KafkaSink.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setRecordSerializer(KafkaRecordSerializationSchema.builder()
        .setTopic("output.topic")
        .setValueSerializationSchema(new SimpleStringSchema())
        .build())
    .setKafkaProducerConfig(producerProps)
    .build();

stream.sinkTo(sink);
```

### Exactly-Once Semantics

```java
// Enable exactly-once (requires idempotent producer)
Properties producerProps = new Properties();
producerProps.setProperty("acks", "all");
producerProps.setProperty("retries", "3");
producerProps.setProperty("enable.idempotence", "true");

KafkaSink<String> sink = KafkaSink.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setRecordSerializer(...)
    .setKafkaProducerConfig(producerProps)
    .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)  // Requires idempotence
    .build();
```

---

## Kafka 源：FlinkKafkaConsumer（已弃用）vs KafkaSource（新）

### FlinkKafkaConsumer (Deprecated - Don't Use)

```java
// OLD WAY - Don't use
FlinkKafkaConsumer<String> consumer = new FlinkKafkaConsumer<>(
    "topic",
    new SimpleStringSchema(),
    properties
);
```

**Why Deprecated**: Uses old Kafka consumer API, less efficient, fewer features.

### KafkaSource (New - Use This)

```java
// NEW WAY - Use this
KafkaSource<String> source = KafkaSource.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setTopics("topic")
    .setGroupId("group")
    .setStartingOffsets(OffsetsInitializer.latest())
    .setValueOnlyDeserializer(new SimpleStringSchema())
    .build();
```

**Advantages**: Better performance, exactly-once semantics, improved offset management.

**MarketLag Project**: Should use KafkaSource (though project primarily uses Flink SQL which uses KafkaSource internally).

---

## Kafka 汇：FlinkKafkaProducer（已弃用）vs KafkaSink（新）

### FlinkKafkaProducer (Deprecated - Don't Use)

```java
// OLD WAY - Don't use
FlinkKafkaProducer<String> producer = new FlinkKafkaProducer<>(
    "topic",
    new SimpleStringSchema(),
    properties
);
```

### KafkaSink (New - Use This)

```java
// NEW WAY - Use this
KafkaSink<String> sink = KafkaSink.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setRecordSerializer(KafkaRecordSerializationSchema.builder()
        .setTopic("topic")
        .setValueSerializationSchema(new SimpleStringSchema())
        .build())
    .build();
```

---

## 消费者组管理与偏移量处理

### Automatic Offset Management

Flink automatically manages Kafka offsets:
- **Checkpoint Integration**: Offsets are included in checkpoints
- **Automatic Commits**: Offsets committed on checkpoint completion
- **Recovery**: On restart, Flink resumes from last checkpointed offsets

### Starting Offsets

```java
// Start from latest (skip old messages)
.setStartingOffsets(OffsetsInitializer.latest())

// Start from earliest (read all messages)
.setStartingOffsets(OffsetsInitializer.earliest())

// Start from specific offsets
.setStartingOffsets(OffsetsInitializer.offsets(Map<TopicPartition, Long>))
```

**MarketLag Project**: Typically starts from latest for production (process new events only).

---

## 最小可用代码

```java
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.sink.KafkaSink;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;

public class KafkaConnectorDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // Kafka Source
        KafkaSource<String> source = KafkaSource.<String>builder()
            .setBootstrapServers("localhost:9092")
            .setTopics("input.topic")
            .setGroupId("flink-consumer")
            .setStartingOffsets(OffsetsInitializer.latest())
            .setValueOnlyDeserializer(new SimpleStringSchema())
            .build();

        DataStream<String> stream = env.fromSource(
            source,
            WatermarkStrategy.noWatermarks(),
            "Kafka Source"
        );

        // Process
        DataStream<String> processed = stream.map(s -> s.toUpperCase());

        // Kafka Sink
        KafkaSink<String> sink = KafkaSink.<String>builder()
            .setBootstrapServers("localhost:9092")
            .setRecordSerializer(KafkaRecordSerializationSchema.builder()
                .setTopic("output.topic")
                .setValueSerializationSchema(new SimpleStringSchema())
                .build())
            .build();

        processed.sinkTo(sink);

        env.execute("Kafka Connector Demo");
    }
}
```

---

## 常见错误

1. **使用已弃用的 connector**:
   - ❌ 使用 FlinkKafkaConsumer/FlinkKafkaProducer
   - ✅ 使用 KafkaSource/KafkaSink（新 API）

2. **未配置身份验证**:
   - ❌ 缺少 Confluent Cloud 的 SASL_SSL 配置
   - ✅ 始终配置 security.protocol、sasl.mechanism、sasl.jaas.config

3. **错误的起始 offset**:
   - ❌ 在生产中使用 earliest（处理所有历史数据）
   - ✅ 在生产中使用 latest（仅处理新事件）

4. **不处理反序列化错误**:
   - ❌ 作业在格式错误的消息上失败
   - ✅ 使用错误处理、死信队列（第 9.1 节）

5. **Consumer group 冲突**:
   - ❌ 多个作业使用相同的 consumer group
   - ✅ 每个作业使用唯一的 consumer group

---

## 触发点：什么时候想到这个概念

在以下情况下考虑 Flink Kafka connector：
- **从 Kafka 读取**: MarketLag 从 Kafka 读取 RSS 事件和 Polymarket 价格
- **写入 Kafka**: Flink 作业可能将中间结果写入 Kafka
- **配置身份验证**: Confluent Cloud 需要 SASL_SSL
- **Offset 管理**: 理解 Flink 如何管理 offset 有助于恢复
- **使用 Flink SQL**: 第 2.2 节涵盖更高级的 SQL 接口（在内部使用 KafkaSource）

**在 MarketLag 项目中**: 所有数据通过 Kafka 流动。Flink 作业从 Kafka topic（rss.events、polymarket.price_hourly）读取并写入结果。虽然项目使用 Flink SQL（第 2.2 节），但理解底层 connector 有助于故障排除和优化。

---

## 小结

Flink 提供 KafkaSource（新）和 KafkaSink（新）用于 Kafka 集成。旧版 FlinkKafkaConsumer/FlinkKafkaProducer 已弃用。Flink 自动管理 consumer groups 和 offsets，与 checkpoints 集成。Confluent Cloud 需要 SASL_SSL 身份验证。即使使用 Flink SQL（第 2.2 节），理解 connector 也至关重要，因为 SQL 在底层使用 connector。

