# Schema Registry 集成

**Learning Point**: 2.3 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 2.2 (Kafka Table Connector), understanding of schema evolution
**Version**: Flink 2.2.0

---

## 工程化定义

**Schema Registry**: 用于管理和版本化 Kafka 消息 schema 的集中式服务。支持 schema 演进和兼容性检查。

**Schema Evolution**: 随时间更新 schema 同时保持向后/向前兼容性的过程。

**Schema Compatibility**: 确定新 schema 版本是否与现有版本兼容的规则（BACKWARD、FORWARD、FULL）。

**Avro Format with Schema Registry**: 使用 Avro 序列化，schema 存储在 Schema Registry 中，用于类型安全和演进。

**JSON Schema with Schema Registry**: 使用 JSON Schema 格式，schema 存储在 Schema Registry 中。

---

## 通俗解释

将 Schema Registry 视为数据结构的版本控制系统：

- **Schema Registry** = 版本控制：存储数据结构的所有版本
- **Schema Evolution** = 更新结构：添加新字段而不破坏旧代码
- **Compatibility** = 安全规则：确保新旧代码可以协同工作

**为什么需要**: 随着 MarketLag 项目的发展，您可能会向 RSS 事件或价格数据添加新字段。Schema Registry 确保新旧消费者可以共存。

---

## 类比理解

如果您了解 **数据库迁移**（您了解），schema 演进类似：
- Database migration = Schema evolution（添加列）
- Backward compatibility = 旧代码可以使用新数据
- Forward compatibility = 新代码可以使用旧数据

关键区别：Schema Registry 为 Kafka 消息自动处理此问题。

---

## 与已学内容的关系

- **2.2 Kafka Table Connector**: Schema Registry 与 Kafka table connector 集成
- **Kafka**: Schema Registry 是 Confluent Cloud 中与 Kafka 一起的服务
- **Data Quality**: Schema 验证确保数据质量（第 9.2 节）

---

## 伪代码（基于源码）

Based on Flink 2.2.0 and Schema Registry integration:

```java
// Schema Registry integration (simplified)
class SchemaRegistryDeserializer {
    SchemaRegistryClient client;
    String registryUrl;

    Object deserialize(byte[] data, String topic) {
        // 1. Get schema ID from message
        int schemaId = extractSchemaId(data);

        // 2. Fetch schema from registry
        Schema schema = client.getSchemaById(schemaId);

        // 3. Deserialize using schema
        return deserializeWithSchema(data, schema);
    }
}
```

---

## 源码参考

**Schema Registry**: Confluent Schema Registry (not in Flink source, but integrated)

**Flink Integration**: `generation_pipeline/step0_context/source_codes/flink/flink-formats/flink-format-avro/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentRegistryAvroDeserializationSchema.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-formats/flink-format-avro/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentRegistryAvroDeserializationSchema.java

---

## Schema Registry 概念与用途

### Why Schema Registry?

1. **Type Safety**: 确保消息匹配预期结构
2. **Schema Evolution**: 添加字段而不破坏消费者
3. **Versioning**: 跟踪随时间变化的 schema 更改
4. **Compatibility Checking**: 自动验证 schema 兼容性

### How It Works

1. **Producer**: 注册 schema，获取 schema ID，将 ID 嵌入消息中
2. **Consumer**: 从消息中读取 schema ID，从 registry 获取 schema，反序列化

---

## Confluent Schema Registry 与 Flink 集成

### Avro Format with Schema Registry

```sql
CREATE TABLE rss_events (
    title STRING,
    published_at TIMESTAMP(3),
    source STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'format' = 'avro-confluent',
    'avro-confluent.schema-registry.url' = 'https://<schema-registry-endpoint>',
    'avro-confluent.schema-registry.subject' = 'rss-events-value',
    'properties.schema.registry.url' = 'https://<schema-registry-endpoint>',
    'properties.basic.auth.credentials.source' = 'USER_INFO',
    'properties.basic.auth.user.info' = '<api-key>:<api-secret>'
);
```

**MarketLag Project**: Can use Avro with Schema Registry for better type safety and evolution.

### JSON Schema with Schema Registry

```sql
CREATE TABLE rss_events (
    title STRING,
    published_at TIMESTAMP(3),
    source STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'format' = 'json',
    'json.schema.registry.url' = 'https://<schema-registry-endpoint>',
    'json.schema.registry.subject' = 'rss-events-value'
);
```

---

## 模式演进与兼容性

### Compatibility Modes

1. **BACKWARD**: 新 schema 可以读取使用旧 schema 写入的数据
   - 旧消费者可以读取新数据
   - 使用场景：添加可选字段

2. **FORWARD**: 旧 schema 可以读取使用新 schema 写入的数据
   - 新消费者可以读取旧数据
   - 使用场景：删除可选字段

3. **FULL**: 向后和向前都兼容
   - 任何版本都可以读取任何版本
   - 使用场景：添加/删除可选字段

### Schema Evolution Example

**Version 1**:
```json
{
  "title": "string",
  "published_at": "timestamp"
}
```

**Version 2** (BACKWARD compatible - adding optional field):
```json
{
  "title": "string",
  "published_at": "timestamp",
  "source": "string"  // New optional field
}
```

旧消费者仍然可以读取 Version 2 消息（忽略新字段）。新消费者可以读取两个版本。

---

## 配置：添加 Schema Registry URL 与凭据

### Confluent Cloud Schema Registry

```sql
CREATE TABLE rss_events (
    -- schema definition
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'format' = 'avro-confluent',
    -- Schema Registry configuration
    'avro-confluent.schema-registry.url' = 'https://<schema-registry-endpoint>',
    'avro-confluent.schema-registry.subject' = 'rss-events-value',
    'properties.schema.registry.url' = 'https://<schema-registry-endpoint>',
    'properties.basic.auth.credentials.source' = 'USER_INFO',
    'properties.basic.auth.user.info' = '<api-key>:<api-secret>',
    -- Kafka authentication (still needed)
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'PLAIN',
    'properties.sasl.username' = '<api-key>',
    'properties.sasl.password' = '<api-secret>'
);
```

**MarketLag Project**: Uses Confluent Cloud Schema Registry for schema management.

---

## 最小可用代码

```java
// Using Avro with Schema Registry in DataStream API
import org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroDeserializationSchema;

KafkaSource<GenericRecord> source = KafkaSource.<GenericRecord>builder()
    .setBootstrapServers("localhost:9092")
    .setTopics("rss.events")
    .setStartingOffsets(OffsetsInitializer.latest())
    .setValueOnlyDeserializer(
        ConfluentRegistryAvroDeserializationSchema.forGeneric(
            "rss-events-value",
            "http://schema-registry:8081"
        )
    )
    .build();
```

---

## 常见错误

1. **未配置 Schema Registry URL**:
   - ❌ 表定义中缺少 schema-registry.url
   - ✅ 始终包含 Schema Registry URL 和凭据

2. **错误的 subject 名称**:
   - ❌ 使用 topic 名称而不是 subject 名称
   - ✅ Subject 名称通常是 `<topic-name>-value` 或 `<topic-name>-key`

3. **兼容性模式冲突**:
   - ❌ 注册不兼容的 schema
   - ✅ 理解兼容性模式，测试 schema 演进

4. **缺少身份验证**:
   - ❌ Schema Registry 在 Confluent Cloud 中需要身份验证
   - ✅ 包含 basic.auth.credentials.source 和 basic.auth.user.info

5. **不处理 schema 演进**:
   - ❌ 进行不兼容的破坏性更改
   - ✅ 使用兼容的演进（添加可选字段，不删除必需字段）

---

## 触发点：什么时候想到这个概念

在以下情况下考虑 Schema Registry：
- **使用 Avro 格式**: Avro 从 Schema Registry 中受益最多
- **Schema 演进**: 需要随时间添加/更改字段
- **类型安全**: 希望确保消息结构匹配预期
- **Confluent Cloud**: Schema Registry 包含在 Confluent Cloud 中
- **数据质量**: Schema 验证确保数据质量（第 9.2 节）

**在 MarketLag 项目中**: Confluent Cloud 包含 Schema Registry。虽然项目目前使用 JSON 格式，但随着项目的发展，可以添加 Schema Registry 以获得更好的类型安全和 schema 演进。

---

## 小结

Schema Registry 管理和版本化 Kafka 消息的 schema，支持 schema 演进和兼容性检查。Confluent Cloud 包含 Schema Registry。Flink 与 Schema Registry 集成，支持 Avro 和 JSON Schema 格式。理解 Schema Registry 有助于随着 MarketLag 项目的发展进行 schema 演进、类型安全和数据质量。

