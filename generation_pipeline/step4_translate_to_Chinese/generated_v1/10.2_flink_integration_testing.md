# Flink 集成测试

**Learning Point**: 10.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 10.1 (Local Testing), understanding of Docker, integration testing
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink Integration Testing**: 在集成环境中使用真实外部系统（Kafka、PostgreSQL、Schema Registry）测试 Flink 应用程序。

**Docker Compose Setup**: 使用 Docker Compose 编排多个服务（Kafka、PostgreSQL、Schema Registry）进行集成测试。

**End-to-End Testing**: 测试从 source（Kafka）通过 Flink 处理到 sink（PostgreSQL）的完整数据管道。

**Test Data Replay**: 使用历史数据重放场景并验证正确性。

---

## Plain Language Explanation

将集成测试视为完整的彩排：

- **Docker Compose** = 舞台设置："为测试设置所有系统（Kafka、数据库）"
- **End-to-End Testing** = 完整彩排："从头到尾测试整个管道"
- **Test Data Replay** = 彩排脚本："使用真实数据测试"

**为什么需要**: MarketLag 需要在生产前使用真实的 Kafka 和 PostgreSQL 测试完整管道。

---

## Analogy

如果您了解 **集成测试**（您了解），Flink 集成测试类似：
- Integration tests = Flink integration tests（两者都使用真实系统测试）
- Test environment = Docker Compose（两者都设置测试基础设施）
- End-to-end tests = Full pipeline tests（两者都测试完整流程）

关键区别：Flink 集成测试使用流系统（Kafka）而不是批处理系统。

---

## Relationship to Already Learned Topics

- **10.1 Local Testing**: 集成测试使用真实系统扩展本地测试
- **2.2 Kafka Table Connector**: 使用真实 Kafka 测试
- **5.1 JDBC Connector**: 使用真实 PostgreSQL 测试
- **Docker**: 您已经了解 Docker - 用于测试基础设施

---

## Docker Compose Setup: Kafka, PostgreSQL, Schema Registry

### docker-compose.yml

```yaml
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  postgres:
    image: postgres:14
    environment:
      POSTGRES_DB: testdb
      POSTGRES_USER: testuser
      POSTGRES_PASSWORD: testpass
    ports:
      - "5432:5432"

  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    depends_on:
      - kafka
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
    ports:
      - "8081:8081"
```

**MarketLag**: Uses Docker Compose for integration testing environment.

---

## End-to-End Testing: Source → Flink → Sink

### Test Flow

1. **Setup**: Start Docker Compose services
2. **Produce Test Data**: Send test data to Kafka topics
3. **Run Flink Job**: Execute Flink job locally
4. **Verify Sink**: Check PostgreSQL for results
5. **Cleanup**: Stop services

### Implementation

```java
@Test
public void testEndToEnd() throws Exception {
    // 1. Start Docker Compose (via testcontainers or manual)
    // 2. Produce test data to Kafka
    produceTestDataToKafka("rss.events", testRssEvents);
    produceTestDataToKafka("polymarket.price_hourly", testPrices);

    // 3. Run Flink job
    StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
    // Configure with test Kafka and PostgreSQL
    env.getConfig().setGlobalJobParameters(createTestConfig());
    // Run job
    env.execute("Integration Test");

    // 4. Verify results in PostgreSQL
    List<Result> results = queryPostgreSQL("SELECT * FROM lag_signals_history");
    assertEquals(expectedCount, results.size());
    // Verify specific results
    verifyResults(results);
}
```

**MarketLag**: Tests complete pipeline from Kafka → Flink → PostgreSQL.

---

## Test Data Replay: Using Historical Data

### Replay Pattern

**Concept**: Use real historical data to test with realistic scenarios.

**Implementation**:
1. Store historical data (Kafka messages, API responses)
2. Replay in test environment
3. Verify Flink job produces expected results

```java
@Test
public void testWithHistoricalData() throws Exception {
    // Load historical data
    List<Event> historicalEvents = loadHistoricalData("rss_events_2024-01-15.json");

    // Replay to Kafka
    for (Event event : historicalEvents) {
        produceToKafka("rss.events", event);
    }

    // Run Flink job
    env.execute("Historical Replay Test");

    // Verify against known results
    verifyAgainstExpectedResults();
}
```

**MarketLag**: Can replay historical RSS events and prices for testing.

---

## Minimum Viable Code

```java
import org.testcontainers.containers.KafkaContainer;
import org.testcontainers.containers.PostgreSQLContainer;
import org.testcontainers.junit.jupiter.Testcontainers;

@Testcontainers
public class FlinkIntegrationTest {
    static KafkaContainer kafka = new KafkaContainer();
    static PostgreSQLContainer postgres = new PostgreSQLContainer("postgres:14");

    @BeforeAll
    static void setup() {
        kafka.start();
        postgres.start();
    }

    @Test
    public void testIntegration() throws Exception {
        // Configure Flink with test containers
        String kafkaBootstrap = kafka.getBootstrapServers();
        String jdbcUrl = postgres.getJdbcUrl();

        // Create test data
        List<Event> testEvents = createTestEvents();

        // Produce to Kafka
        produceToKafka(kafkaBootstrap, "rss.events", testEvents);

        // Run Flink job
        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
        // Configure with test containers
        runFlinkJob(env, kafkaBootstrap, jdbcUrl);

        // Verify in PostgreSQL
        List<Result> results = queryPostgreSQL(jdbcUrl, "SELECT * FROM results");
        assertResults(results);
    }
}
```

---

## Common Mistakes

1. **不使用 testcontainers**:
   - ❌ 手动 Docker 设置（繁琐、容易出错）
   - ✅ 使用 testcontainers 进行自动化设置

2. **不清理**:
   - ❌ 测试数据在测试之间持续存在
   - ✅ 每次测试后清理测试数据

3. **不等待处理**:
   - ❌ 在 Flink 完成前检查结果
   - ✅ 在验证前等待 Flink 作业完成

4. **使用生产配置**:
   - ❌ 意外连接到生产系统
   - ✅ 使用测试特定配置

5. **不测试错误场景**:
   - ❌ 仅测试正常路径
   - ✅ 测试错误场景（缺失数据、格式错误的数据）

---

## Mind Trigger: When to Think About This

在以下情况下考虑 Flink 集成测试：
- **测试完整管道**: MarketLag 测试端到端管道
- **部署前**: 集成测试在生产前验证
- **调试问题**: 集成测试有助于重现问题
- **数据重放**: 使用历史数据测试
- **本地测试**: 第 10.1 节涵盖本地测试，集成测试扩展它

**在 MarketLag 项目中**: 使用 Docker Compose（Kafka、PostgreSQL）测试完整管道。测试从 Kafka → Flink → PostgreSQL 的端到端流程。可以重放历史数据进行测试。理解集成测试对于验证完整管道至关重要。

---

## Summary

Flink 集成测试使用真实外部系统测试 Flink 应用程序。使用 Docker Compose 设置 Kafka、PostgreSQL、Schema Registry。测试从 source 到 sink 的端到端管道。重放历史数据以进行真实测试。MarketLag 使用集成测试在部署前验证完整管道。理解集成测试对于生产就绪至关重要。

