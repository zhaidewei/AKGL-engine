# 流处理中的数据验证

**Learning Point**: 9.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 2.3 (Schema Registry)、理解数据质量
**Version**: Flink 2.2.0

---

## 工程化定义

**Data Validation in Streaming**: 在记录流经流处理管道时检查数据质量和正确性。确保只处理有效数据。

**Schema Validation**: 验证记录是否匹配预期的 schema 结构。使用 Schema Registry 进行 schema 验证。

**Data Range Validation**: 检查值是否在预期范围内（例如，预测市场的价格 ∈ [0, 1]）。

**Timestamp Validation**: 验证时间戳是否合理（不是未来，不太旧）。

**Missing Field Handling**: 处理缺失或 null 字段的策略：默认值、跳过记录或错误处理。

---

## 通俗解释

将数据验证视为质量控制：

- **Schema Validation** = 结构检查："此记录是否具有所有必需字段？"
- **Range Validation** = 值检查："价格是否在 0 和 1 之间？"
- **Timestamp Validation** = 时间检查："时间戳是否合理？"
- **Missing Fields** = 完整性检查："所有字段是否存在？"

**为什么需要**: MarketLag 需要在处理前确保数据质量。无效数据可能导致错误的 lag 检测。

---

## 类比理解

如果您了解 **数据质量框架**（您了解），流验证类似：
- Data quality checks = Streaming validation（两者都检查数据质量）
- ETL validation = Streaming validation（两者都在处理前验证）
- Data profiling = Validation rules（两者都定义什么是有效的）

关键区别：流验证在数据流动时实时发生。

---

## 与已学内容的关系

- **2.3 Schema Registry**: Schema 验证使用 Schema Registry
- **9.1 Dead Letter Queue**: 无效记录发送到 DLQ
- **9.3 Exception Handling**: 验证失败被优雅处理
- **Data Quality**: 您已经了解数据质量 - 这适用于流处理

---

## 模式校验：使用 Schema Registry

### Schema Registry Validation

**How It Works**:
1. Schema Registry 存储预期的 schema
2. 根据 schema 验证记录
3. 无效记录被拒绝或发送到 DLQ

**MarketLag**: 可以使用 Schema Registry 进行 schema 验证（第 2.3 节）。

### Flink SQL Schema Validation

```sql
CREATE TABLE rss_events (
    title STRING,
    published_at TIMESTAMP(3),
    source STRING
) WITH (
    'connector' = 'kafka',
    'format' = 'json',
    'json.ignore-parse-errors' = 'false'  -- Reject invalid JSON
);
```

**MarketLag**: Uses `json.ignore-parse-errors` = 'true' to skip malformed JSON, but could validate schema.

---

## 数据范围校验：price ∈ [0,1]

### Validation Rule

**Rule**: 价格必须在 0 和 1 之间（预测市场价格）。

**Implementation**:

```sql
-- In Flink SQL
SELECT *
FROM polymarket_price_hourly
WHERE price >= 0 AND price <= 1
  AND price IS NOT NULL
```

**MarketLag**: Validates price range in Flink SQL queries.

### Lambda Validation

```python
def validate_price(price):
    """
    Validate price is in valid range.

    Args:
        price: Price value

    Returns:
        bool: True if valid
    """
    if price is None:
        return False
    return 0.0 <= float(price) <= 1.0

# Usage
if not validate_price(price):
    send_to_dlq(record)  # Invalid price, send to DLQ
    return
```

**MarketLag**: Can validate in Lambda before sending to Kafka.

---

## 时间戳校验：不早不晚（不过期且非未来时间）

### Validation Rules

1. **Not Future**: 时间戳不应该是未来的
2. **Not Too Old**: 时间戳不应该太旧（例如，> 7 天）

### Implementation

```python
from datetime import datetime, timedelta
import pytz

def validate_timestamp(timestamp_str, max_age_days=7):
    """
    Validate timestamp is reasonable.

    Args:
        timestamp_str: ISO 8601 timestamp string
        max_age_days: Maximum age in days

    Returns:
        bool: True if valid
    """
    try:
        timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        now = datetime.now(pytz.UTC)

        # Not future
        if timestamp > now:
            return False

        # Not too old
        max_age = timedelta(days=max_age_days)
        if timestamp < now - max_age:
            return False

        return True
    except Exception:
        return False
```

**MarketLag**: Validates timestamps in Lambda or Flink.

---

## Missing Field Handling: Default Values, Skipping Records

### Strategy 1: Default Values

```sql
-- Use COALESCE for default values
SELECT
    market_slug,
    COALESCE(price, 0.0) as price,  -- Default to 0.0 if null
    COALESCE(source, 'unknown') as source
FROM rss_events
```

**Use Case**: 当缺失字段可以安全地使用默认值时。

### Strategy 2: Skipping Records

```sql
-- Filter out records with missing required fields
SELECT *
FROM rss_events
WHERE market_slug IS NOT NULL
  AND published_at IS NOT NULL
  AND price IS NOT NULL
```

**Use Case**: 当缺失字段使记录无法使用时。

**MarketLag**: Filters out records with missing required fields.

### Strategy 3: Error Handling

```python
# In Lambda or Flink
if required_field is None:
    send_to_dlq(record)  # Send to DLQ for analysis
    return
```

**Use Case**: 当缺失字段表明数据质量问题时。

---

## 最小可用代码

```python
# Data validation in Lambda
def validate_record(record):
    """
    Validate record before sending to Kafka.

    Args:
        record: Record to validate

    Returns:
        tuple: (is_valid, errors)
    """
    errors = []

    # Schema validation: required fields
    if 'market_slug' not in record:
        errors.append("Missing market_slug")
    if 'price' not in record:
        errors.append("Missing price")

    # Range validation
    if 'price' in record:
        price = record['price']
        if price is not None and (price < 0 or price > 1):
            errors.append(f"Price out of range: {price}")

    # Timestamp validation
    if 'published_at' in record:
        if not validate_timestamp(record['published_at']):
            errors.append("Invalid timestamp")

    return len(errors) == 0, errors

# Usage
is_valid, errors = validate_record(record)
if not is_valid:
    print(f"Validation failed: {errors}")
    send_to_dlq(record)
else:
    send_to_kafka(record)
```

---

## 常见错误

1. **不验证数据**:
   - ❌ 处理无效数据，导致下游错误
   - ✅ 始终在处理前验证数据

2. **验证过于严格**:
   - ❌ 由于过于严格的规则而拒绝有效数据
   - ✅ 平衡验证严格性与数据丢失

3. **不处理缺失字段**:
   - ❌ 假设所有字段都存在，导致错误
   - ✅ 始终检查缺失字段，使用默认值或跳过

4. **不验证时间戳**:
   - ❌ 接受未来时间戳，导致窗口问题
   - ✅ 验证时间戳是否合理

5. **不将无效数据发送到 DLQ**:
   - ❌ 静默丢弃无效数据
   - ✅ 发送到 DLQ 进行分析和可能的重新处理

---

## 触发点：什么时候想到这个概念

在以下情况下考虑数据验证：
- **摄取数据**: 在摄取时验证数据（Lambda）
- **处理数据**: 在 Flink 处理前验证
- **Schema 演进**: 第 2.3 节涵盖 schema 验证
- **错误处理**: 第 9.1 节涵盖无效数据的 DLQ
- **数据质量**: 确保整个管道的数据质量

**在 MarketLag 项目中**: 在多个点验证数据：Lambda（Kafka 之前）、Flink（处理之前）。验证价格范围 [0,1]、时间戳和必需字段。无效数据发送到 DLQ。理解数据验证对于数据质量至关重要。

---

## 小结

流处理中的数据验证在记录流经管道时检查数据质量。Schema 验证使用 Schema Registry。范围验证检查值（例如，价格 ∈ [0,1]）。时间戳验证确保时间戳合理。使用默认值、跳过或错误处理来处理缺失字段。MarketLag 在多个点验证数据。理解数据验证对于流应用程序中的数据质量至关重要。

