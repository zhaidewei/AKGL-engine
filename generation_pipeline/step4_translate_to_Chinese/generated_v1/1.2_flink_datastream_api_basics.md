# Flink DataStream API 基础

**Learning Point**: 1.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.1 (Flink Architecture), Java 基础知识，理解函数式编程
**Version**: Flink 2.2.0

---

## 工程化定义

**DataStream API**: Flink 用于构建流处理应用的核心 API。`DataStream<T>` 表示类型为 `T` 的无界记录流。算子将 DataStream 转换为新的 DataStream，形成有向无环图（DAG）操作。

**Key Concepts**:
- **Source**: 从外部系统（Kafka、文件、集合）创建 DataStream
- **Transformation**: 将一个 DataStream 转换为另一个的算子（map、filter、flatMap、keyBy）
- **Sink**: 将 DataStream 结果写入外部系统
- **Operator Chaining**: Flink 通过将多个算子链接到单个任务中来优化，以减少序列化开销

---

## 通俗解释

DataStream API 就像一条装配线：

- **Source** = 原材料传送带（数据输入）
- **Transformations** = 修改材料的工作站（map、filter 等）
- **Sink** = 成品传送带（数据输出）
- **Operator Chaining** = 合并工作站以减少交接时间

每个转换都会创建一个新的"传送带"（DataStream），流向下一个工作站。Flink 在可能的情况下自动优化，将工作站链接在一起。

---

## 类比理解

如果您了解 **Spark 的 RDD API**（您了解），DataStream 类似：
- Spark RDD = Flink DataStream（都表示分布式数据）
- Spark 转换（map、filter）= Flink 转换（相同名称，类似概念）
- Spark 操作（collect、count）= Flink sink（写入结果）

关键区别：RDD 是面向批处理的，DataStream 是连续且无界的。

---

## 与已学内容的关系

- **1.1 Flink Architecture**: DataStream 算子成为在 TaskManager slot 上调度的任务
- **Kafka**: DataStream source 从 Kafka topic 读取（您将在第 2.1 节看到）
- **Functional Programming**: 转换使用 lambda 函数（类似于您了解的 Spark）
- **Event-Driven Architecture**: DataStream 中的每条记录都会立即触发处理

---

## 伪代码（基于源码）

基于 Flink 2.2.0 源代码：

```java
// DataStream (simplified)
class DataStream<T> {
    StreamExecutionEnvironment environment;
    StreamTransformation<T> transformation;

    <R> DataStream<R> map(MapFunction<T, R> mapper) {
        // Creates a new transformation node in the DAG
        return new DataStream<>(
            environment,
            new OneInputTransformation<>(this.transformation, mapper)
        );
    }

    DataStream<T> filter(FilterFunction<T> filter) {
        return new DataStream<>(
            environment,
            new OneInputTransformation<>(this.transformation, filter)
        );
    }
}

// Operator chaining (simplified)
class StreamGraph {
    void optimize() {
        // Chain operators that can be chained
        // Conditions: same parallelism, no shuffle between them
        chainOperators();
    }
}
```

---

## 源码参考

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java

**Key Classes**:
- `DataStream`: 主 API 类
- `StreamExecutionEnvironment`: 创建 DataStream 的入口点
- `MapFunction`, `FilterFunction`: 转换函数

---

## 核心转换操作

### 1. map() - 一对一转换

将每条记录转换为恰好一条输出记录。

```java
DataStream<String> words = ...;
DataStream<String> upper = words.map(s -> s.toUpperCase());
// Input: ["hello", "world"]
// Output: ["HELLO", "WORLD"]
```

### 2. filter() - 记录过滤

仅保留通过条件的记录。

```java
DataStream<Integer> numbers = ...;
DataStream<Integer> evens = numbers.filter(n -> n % 2 == 0);
// Input: [1, 2, 3, 4, 5]
// Output: [2, 4]
```

### 3. flatMap() - 一对多转换

将每条记录转换为零条、一条或多条输出记录。

```java
DataStream<String> sentences = ...;
DataStream<String> words = sentences.flatMap(s ->
    Arrays.asList(s.split(" ")).iterator()
);
// Input: ["hello world", "flink streaming"]
// Output: ["hello", "world", "flink", "streaming"]
```

### 4. keyBy() - 键控流

按键对流进行分区，启用键控状态和键控操作。

```java
DataStream<Event> events = ...;
KeyedStream<Event, String> keyed = events.keyBy(e -> e.getUserId());
// Partitions stream by userId
// All events with same userId go to same subtask
```

---

## 最小可用代码

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;
import java.util.Arrays;

public class DataStreamAPIDemo {
    public static void main(String[] args) throws Exception {
        // Create execution environment
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(2);

        // SOURCE: Create DataStream from collection
        DataStream<String> sentences = env.fromElements(
            "Apache Flink is a streaming framework",
            "Flink processes data in real-time",
            "Streaming is different from batch processing"
        );

        // TRANSFORMATION 1: Split sentences into words
        DataStream<String> words = sentences.flatMap(sentence ->
            Arrays.asList(sentence.split(" ")).iterator()
        );

        // TRANSFORMATION 2: Filter words longer than 4 characters
        DataStream<String> longWords = words.filter(word -> word.length() > 4);

        // TRANSFORMATION 3: Convert to uppercase
        DataStream<String> upper = longWords.map(String::toUpperCase);

        // TRANSFORMATION 4: Key by first character
        DataStream<String> keyed = upper.keyBy(word -> word.charAt(0));

        // SINK: Print results
        keyed.print();

        // Execute job
        env.execute("DataStream API Demo");
    }
}
```

**Output** (example):
```
2> APACHE
1> FLINK
2> STREAMING
1> FRAMEWORK
2> PROCESSES
1> REAL-TIME
2> STREAMING
1> DIFFERENT
2> BATCH
2> PROCESSING
```

---

## 算子链（Operator Chaining）

Flink 自动链接算子以优化性能：

```java
stream
    .map(...)      // Operator 1
    .filter(...)   // Operator 2
    .map(...)      // Operator 3
    .print();      // Operator 4
```

**不链接**: 4 个独立任务，3 次网络 shuffle
**链接后**: 1 个任务，0 次网络 shuffle（都在同一 JVM 中）

**链接条件**:
- 相同的并行度
- 算子之间没有 shuffle（没有 keyBy、rebalance 等）
- 相同的 slot 共享组

**禁用链接**（如需要）:
```java
stream.map(...).disableChaining();  // Starts new chain
```

---

## 从数据源创建 DataStream

### 从集合创建
```java
DataStream<String> stream = env.fromElements("a", "b", "c");
DataStream<Integer> numbers = env.fromCollection(Arrays.asList(1, 2, 3));
```

### 从文件创建
```java
DataStream<String> lines = env.readTextFile("path/to/file.txt");
```

### 从 Kafka 创建（详见第 2.1 节）
```java
// Will be covered in Flink-Kafka Integration section
DataStream<String> kafkaStream = env.addSource(new FlinkKafkaConsumer<>(...));
```

---

## 常见错误

1. **忘记调用 execute()**:
   - ❌ 创建 DataStream 但不调用 `env.execute()`
   - ✅ 始终调用 `env.execute("job name")` 以启动执行

2. **不理解 keyBy 语义**:
   - ❌ 在不理解的情况下使用 keyBy 会导致 shuffle
   - ✅ keyBy 会重新分区数据 - 仅在需要状态操作时使用

3. **忽略并行度**:
   - ❌ 不设置并行度，使用默认值（CPU 核心数）
   - ✅ 显式设置并行度：`env.setParallelism(4)` 或按算子设置

4. **算子链接混淆**:
   - ❌ 不理解为什么算子被链接
   - ✅ 理解链接减少开销但可能使调试复杂化

5. **类型安全问题**:
   - ❌ 使用原始类型，丢失类型信息
   - ✅ 使用泛型类型：`DataStream<MyEvent>` 而不是 `DataStream`

---

## 触发点：什么时候想到这个概念

在以下情况下考虑 DataStream API：
- **构建流处理管道**: 每个 Flink 流处理作业都使用 DataStream API
- **转换数据**: map、filter、flatMap 是主要工具
- **数据分区**: 在有状态操作（窗口、状态）之前需要 keyBy
- **性能优化**: 理解算子链接有助于优化作业
- **调试**: 了解 DAG 结构有助于追踪数据流问题

**在 MarketLag 项目中**: 虽然项目主要使用 Flink SQL（第 1.3 节），但理解 DataStream API 有助于在需要自定义 ProcessFunction（第 4.1 节）或调试 SQL 执行计划时。

---

## 小结

DataStream API 是 Flink 的核心流处理 API。Source 创建流，转换（map、filter、flatMap、keyBy）修改它们，sink 写入结果。Flink 通过链接算子进行优化。理解这些基础知识对于使用 Flink SQL 也至关重要，因为 SQL 在底层编译为 DataStream 操作。

