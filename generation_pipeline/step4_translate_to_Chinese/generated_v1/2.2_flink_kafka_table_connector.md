# Flink Kafka Table 连接器

**Learning Point**: 2.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.3 (Table API/SQL), 2.1 (Kafka Connector), Kafka knowledge
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink Kafka Table Connector**: 一个 Flink SQL 表连接器，支持使用 SQL 语法从 Kafka topic 读取和写入。提供比 DataStream KafkaSource/KafkaSink API 更高级的抽象。

**CREATE TABLE 语法**: SQL DDL 语句，用于将 Kafka topic 定义为具有 schema、格式和连接器属性的 Flink 表。

**Table Format**: 定义如何序列化/反序列化数据（JSON、Avro、CSV 等）。MarketLag 项目使用 JSON 格式。

**Schema Definition**: 定义表 schema（列、类型），映射到 Kafka 消息结构。

---

## Plain Language Explanation

将 Kafka table connector 视为 Kafka 上的 SQL 视图：

- **CREATE TABLE** = 定义视图："这个 Kafka topic 是一个具有这些列的表"
- **SELECT FROM** = 从 Kafka 读取：像查询任何 SQL 表一样查询该表
- **INSERT INTO** = 写入 Kafka：向表中插入行（写入 Kafka topic）

**为什么更好**: 不需要编写使用 KafkaSource 的 Java 代码，而是编写 SQL。MarketLag 项目的所有三个作业都使用这种方式。

---

## Analogy

如果您了解 **Spark SQL with Kafka**（您了解），Flink 的方法非常相似：
- Spark `spark.readStream.format("kafka")` = Flink `CREATE TABLE ... WITH ('connector' = 'kafka')`
- 两者都允许将 Kafka topic 作为 SQL 表进行查询

关键区别：Flink SQL 更加集成，并提供更好的流处理语义。

---

## Relationship to Already Learned Topics

- **1.3 Table API/SQL**: Kafka table connector 使用 Flink SQL 语法
- **2.1 Kafka Connector**: Table connector 在底层使用 KafkaSource/KafkaSink
- **1.4 Time Concepts**: 表定义包括事件时间和 watermarks
- **1.5 Watermarks**: CREATE TABLE 中的 WATERMARK 声明

---

## Pseudocode (From Source Code)

基于 Flink 2.2.0 源代码：

```java
// Kafka table connector (simplified)
class KafkaTableSource implements TableSource {
    String topic;
    String format;  // "json", "avro", etc.
    Schema schema;

    DataStream<Row> getDataStream(StreamExecutionEnvironment env) {
        // Convert to KafkaSource
        KafkaSource source = createKafkaSource(topic, format, schema);
        return env.fromSource(source, ...);
    }
}

// CREATE TABLE parsing (simplified)
class TableParser {
    Table parseCreateTable(String sql) {
        // Parse: CREATE TABLE ... WITH ('connector' = 'kafka', ...)
        // Extract: topic, format, schema, properties
        return new KafkaTable(topic, format, schema, properties);
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-connectors/flink-sql-connector-kafka/src/main/java/org/apache/flink/table/descriptors/Kafka.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-connectors/flink-sql-connector-kafka/src/main/java/org/apache/flink/table/descriptors/Kafka.java

**Key Classes**:
- `KafkaTableSource`: Kafka 表 source 实现
- `KafkaTableSink`: Kafka 表 sink 实现

---

## Kafka Table Connector Configuration in Flink SQL

### Basic CREATE TABLE Syntax

```sql
CREATE TABLE rss_events (
    title STRING,
    link STRING,
    published_at TIMESTAMP(3),
    source STRING,
    keywords ARRAY<STRING>,
    WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true'
);
```

**MarketLag Project**: 对 rss_events 表使用此确切模式。

### Confluent Cloud Configuration

```sql
CREATE TABLE rss_events (
    title STRING,
    link STRING,
    published_at TIMESTAMP(3),
    source STRING,
    keywords ARRAY<STRING>,
    WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'PLAIN',
    'properties.sasl.username' = '<api-key>',
    'properties.sasl.password' = '<api-secret>',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true'
);
```

**MarketLag Project**: 使用带有 SASL_SSL 身份验证的 Confluent Cloud。

---

## Schema Definition: Format (JSON, Avro)

### JSON Format (MarketLag Uses This)

```sql
CREATE TABLE events (
    id BIGINT,
    value DOUBLE,
    timestamp TIMESTAMP(3)
) WITH (
    'connector' = 'kafka',
    'topic' = 'events',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true'  -- Skip malformed JSON
);
```

**JSON Message Format**:
```json
{"id": 1, "value": 10.5, "timestamp": "2024-01-15T14:30:00Z"}
```

### Avro Format (With Schema Registry - Section 2.3)

```sql
CREATE TABLE events (
    id BIGINT,
    value DOUBLE,
    timestamp TIMESTAMP(3)
) WITH (
    'connector' = 'kafka',
    'topic' = 'events',
    'format' = 'avro',
    'avro.schema-registry.url' = 'http://schema-registry:8081'
);
```

---

## Kafka Topic as Flink Table: CREATE TABLE ... WITH (...)

### Required Properties

- **connector**: 必须是 'kafka'
- **topic**: Kafka topic 名称
- **properties.bootstrap.servers**: Kafka broker 地址
- **format**: 数据格式（'json'、'avro'、'csv' 等）

### Optional Properties

- **properties.group.id**: Consumer group ID（如果未设置则自动生成）
- **scan.startup.mode**: 'earliest'、'latest'、'group-offsets'、'timestamp'
- **json.ignore-parse-errors**: 跳过格式错误的 JSON（true/false）
- **sink.partitioner**: 如何分区输出（'fixed'、'round-robin'、'custom'）

---

## Reading from Kafka: SELECT FROM kafka_table

### Basic Read

```sql
SELECT * FROM rss_events;
```

### Filtered Read

```sql
SELECT title, published_at, source
FROM rss_events
WHERE source = 'Reuters'
  AND published_at > CURRENT_TIMESTAMP - INTERVAL '1' HOUR;
```

### Windowed Aggregation (Job 1 Pattern)

```sql
SELECT
    market_slug,
    TUMBLE_START(published_at, INTERVAL '1' HOUR) as window_start,
    TUMBLE_END(published_at, INTERVAL '1' HOUR) as window_end,
    COUNT(*) as mention_count,
    SUM(keyword_score) as keyword_score,
    AVG(article_score * source_weight) as source_weighted_signal
FROM rss_events
GROUP BY market_slug, TUMBLE(published_at, INTERVAL '1' HOUR);
```

**MarketLag Job 1**: 使用此模式按小时聚合 RSS 事件。

---

## Writing to Kafka: INSERT INTO kafka_table

### Basic Write

```sql
INSERT INTO output_topic
SELECT market_slug, window_start, signal
FROM processed_events;
```

### Write to Multiple Topics (Not Directly Supported)

使用单独的 INSERT 语句或创建多个 sink 表。

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class KafkaTableConnectorDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        // Create Kafka source table
        tableEnv.executeSql(
            "CREATE TABLE rss_events (" +
            "  title STRING," +
            "  published_at TIMESTAMP(3)," +
            "  source STRING," +
            "  WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE" +
            ") WITH (" +
            "  'connector' = 'kafka'," +
            "  'topic' = 'rss.events'," +
            "  'properties.bootstrap.servers' = 'localhost:9092'," +
            "  'format' = 'json'" +
            ")"
        );

        // Create Kafka sink table
        tableEnv.executeSql(
            "CREATE TABLE rss_signals_hourly (" +
            "  market_slug STRING," +
            "  window_start TIMESTAMP(3)," +
            "  signal DOUBLE" +
            ") WITH (" +
            "  'connector' = 'kafka'," +
            "  'topic' = 'rss.signals.hourly'," +
            "  'properties.bootstrap.servers' = 'localhost:9092'," +
            "  'format' = 'json'" +
            ")"
        );

        // Query and write
        tableEnv.executeSql(
            "INSERT INTO rss_signals_hourly " +
            "SELECT " +
            "  'market-1' as market_slug," +
            "  TUMBLE_START(published_at, INTERVAL '1' HOUR) as window_start," +
            "  COUNT(*) as signal " +
            "FROM rss_events " +
            "GROUP BY TUMBLE(published_at, INTERVAL '1' HOUR)"
        );
    }
}
```

---

## MarketLag Project Table Definitions

### RSS Events Table (Job 1 Input)

```sql
CREATE TABLE rss_events (
    title STRING,
    link STRING,
    published_at TIMESTAMP(3),
    source STRING,
    keywords ARRAY<STRING>,
    market_slug STRING,
    keyword_score DOUBLE,
    article_score DOUBLE,
    source_weight DOUBLE,
    WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'PLAIN',
    'properties.sasl.username' = '<api-key>',
    'properties.sasl.password' = '<api-secret>',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true'
);
```

### Polymarket Price Table (Job 3 Input)

```sql
CREATE TABLE polymarket_price_hourly (
    market_slug STRING,
    outcome STRING,
    event_time TIMESTAMP(3),
    price DOUBLE,
    price_delta DOUBLE,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' MINUTE
) WITH (
    'connector' = 'kafka',
    'topic' = 'polymarket.price_hourly',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'PLAIN',
    'properties.sasl.username' = '<api-key>',
    'properties.sasl.password' = '<api-secret>',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true'
);
```

---

## Common Mistakes

1. **缺少 WATERMARK 声明**:
   - ❌ 为事件时间窗口创建表时没有 WATERMARK
   - ✅ 始终为事件时间处理声明 WATERMARK

2. **时间戳精度错误**:
   - ❌ 使用 TIMESTAMP 而不是 TIMESTAMP(3)
   - ✅ 使用 TIMESTAMP(3) 以获得毫秒精度（watermark 所需）

3. **未配置身份验证**:
   - ❌ 缺少 Confluent Cloud 的 SASL_SSL 配置
   - ✅ 始终包含 security.protocol、sasl.mechanism、sasl.username、sasl.password

4. **JSON 解析错误**:
   - ❌ 作业在格式错误的 JSON 上失败
   - ✅ 设置 'json.ignore-parse-errors' = 'true' 以跳过错误记录

5. **Schema 不匹配**:
   - ❌ 表 schema 与 JSON 结构不匹配
   - ✅ 确保 schema 匹配实际的 Kafka 消息格式

---

## Mind Trigger: When to Think About This

在以下情况下考虑 Kafka table connector：
- **编写 Flink SQL 作业**: MarketLag 的所有三个作业都使用此方式
- **定义表 schema**: CREATE TABLE 语句定义 Kafka topic 结构
- **配置 watermarks**: CREATE TABLE 中的 WATERMARK 声明（第 1.5 节）
- **处理格式**: JSON vs Avro（第 2.3 节涵盖 Schema Registry）
- **故障排除**: 理解 table connector 有助于调试读写问题

**在 MarketLag 项目中**: 所有三个作业都使用 Kafka table connector。Job 1 读取 rss_events，Job 3 读取 polymarket.price_hourly。表使用 WATERMARK 声明定义，用于事件时间处理。

---

## Summary

Flink Kafka table connector 支持使用 SQL 从 Kafka 读取和写入。CREATE TABLE 将 Kafka topic 定义为具有 schema、格式和连接器属性的 Flink 表。MarketLag 使用带有 Confluent Cloud SASL_SSL 身份验证的 JSON 格式。表包括用于事件时间处理的 WATERMARK 声明。这是 MarketLag 项目中所有三个作业使用的主要模式。

