# Flink JDBC 连接器配置

**Learning Point**: 5.1 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.3 (Table API/SQL), understanding of JDBC/PostgreSQL
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink JDBC Connector**: 使用 JDBC 从关系数据库读取和写入的 Flink 表连接器。支持 PostgreSQL、MySQL 和其他 JDBC 兼容数据库。

**JDBC Sink**: 通过 JDBC 将 Flink 结果写入数据库表。支持批量插入以提高性能。

**JDBC Table Connector**: 使用 CREATE TABLE 语法将数据库表定义为 Flink 表，支持对数据库进行 SQL 查询。

**Batch Insert Configuration**: 配置批量大小和刷新间隔以高效写入数据库。

**Connection Pool Management**: 高效管理数据库连接以避免连接耗尽。

---

## Plain Language Explanation

将 JDBC connector 想象成数据库桥梁：

- **JDBC Connector** = 数据库桥梁：Flink 可以读取/写入 PostgreSQL
- **JDBC Sink** = 写入桥梁：将 Flink 结果写入数据库
- **Batch Inserts** = 批量写入：一次写入多行以提高效率

**为什么需要**：MarketLag 将 lag signals 写入 Supabase PostgreSQL。JDBC connector 使 Flink 能够直接将结果写入数据库。

---

## Analogy

如果您了解 **Spark 的 JDBC connector**（您了解），Flink 的类似：
- Spark `df.write.jdbc(...)` = Flink JDBC sink
- 两者都将 DataFrames/Tables 写入数据库

关键区别：Flink JDBC connector 与 Flink SQL 集成，使其更易于使用。

---

## Relationship to Already Learned Topics

- **1.3 Table API/SQL**: JDBC connector uses CREATE TABLE syntax
- **PostgreSQL**: You already know PostgreSQL - Flink connects to it via JDBC
- **Supabase**: MarketLag uses Supabase (managed PostgreSQL)

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// JDBC sink (simplified)
class JdbcTableSink implements TableSink {
    String url;
    String tableName;
    int batchSize;

    void write(TableData data) {
        // Batch insert to database
        List<Row> batch = new ArrayList<>();
        for (Row row : data) {
            batch.add(row);
            if (batch.size() >= batchSize) {
                insertBatch(batch);
                batch.clear();
            }
        }
        if (!batch.isEmpty()) {
            insertBatch(batch);
        }
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSink.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSink.java

---

## JDBC Sink Configuration

### Basic JDBC Sink

```sql
CREATE TABLE lag_signals_sink (
    market VARCHAR(100),
    window TIMESTAMP,
    signal_delta DECIMAL(10, 4),
    price_delta DECIMAL(10, 4),
    lag_flag BOOLEAN,
    confidence DECIMAL(3, 2),
    detected_at TIMESTAMP
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://localhost:5432/postgres',
    'table-name' = 'lag_signals_history',
    'username' = 'postgres',
    'password' = '<password>'
);
```

### Supabase Connection String Format

**Format**:
```
jdbc:postgresql://db.<project-ref>.supabase.co:5432/postgres
```

**Example**:
```
jdbc:postgresql://db.abcdefghijklmnop.supabase.co:5432/postgres
```

**MarketLag Project**: Uses Supabase connection string format.

---

## JDBC Table Connector for Reading/Writing

### Reading from Database

```sql
CREATE TABLE source_table (
    id BIGINT,
    value DOUBLE
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://localhost:5432/postgres',
    'table-name' = 'source_table',
    'username' = 'postgres',
    'password' = '<password>'
);

SELECT * FROM source_table;
```

### Writing to Database

```sql
-- Create sink table
CREATE TABLE sink_table (
    id BIGINT,
    value DOUBLE
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://localhost:5432/postgres',
    'table-name' = 'sink_table',
    'username' = 'postgres',
    'password' = '<password>'
);

-- Write results
INSERT INTO sink_table
SELECT id, value FROM processed_data;
```

---

## Batch Insert Configuration: Batch Size, Flush Interval

### Batch Size Configuration

```sql
CREATE TABLE lag_signals_sink (
    -- schema
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://...',
    'table-name' = 'lag_signals_history',
    'username' = 'postgres',
    'password' = '<password>',
    'sink.buffer-flush.max-rows' = '100'  -- Batch size
);
```

**效果**：Flink 在写入数据库之前缓冲最多 100 行。

### Flush Interval Configuration

```sql
'sink.buffer-flush.interval' = '10s'  -- 每 10 秒刷新一次
```

**效果**：Flink 每 10 秒写入缓冲的行，即使批次未满。

**MarketLag Project**: 使用批量大小 100 和刷新间隔 10s。

---

## Connection Pool Management

### Connection Pool Configuration

```sql
'sink.connection.max-retry-time' = '60s'  -- Max retry time
```

**效果**：Flink 自动管理连接池。在连接失败时重试。

### Best Practices

1. **Batch Inserts**: 使用批量大小以减少数据库负载
2. **Flush Interval**: 平衡延迟与批量大小
3. **Connection Retry**: 配置重试以提高可靠性
4. **Connection Limits**: 注意数据库连接限制

---

## MarketLag Project Example

```sql
CREATE TABLE lag_signals_sink (
    market VARCHAR(100),
    window TIMESTAMP,
    signal_delta DECIMAL(10, 4),
    price_delta DECIMAL(10, 4),
    lag_flag BOOLEAN,
    confidence DECIMAL(3, 2),
    detected_at TIMESTAMP
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://db.xxx.supabase.co:5432/postgres',
    'table-name' = 'lag_signals_history',
    'username' = 'postgres',
    'password' = '<password>',
    'sink.buffer-flush.max-rows' = '100',
    'sink.buffer-flush.interval' = '10s'
);
```

**This is the exact pattern used in MarketLag Job 3**.

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class JdbcConnectorDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        // Create JDBC sink table
        tableEnv.executeSql(
            "CREATE TABLE jdbc_sink (" +
            "  id BIGINT," +
            "  value DOUBLE" +
            ") WITH (" +
            "  'connector' = 'jdbc'," +
            "  'url' = 'jdbc:postgresql://localhost:5432/postgres'," +
            "  'table-name' = 'results'," +
            "  'username' = 'postgres'," +
            "  'password' = 'password'," +
            "  'sink.buffer-flush.max-rows' = '100'," +
            "  'sink.buffer-flush.interval' = '10s'" +
            ")"
        );

        // Write to JDBC
        tableEnv.executeSql(
            "INSERT INTO jdbc_sink " +
            "SELECT id, value FROM source_table"
        );

        env.execute("JDBC Connector Demo");
    }
}
```

---

## Common Mistakes

1. **错误的连接字符串**：
   - ❌ 使用错误的 Supabase 连接字符串格式
   - ✅ 使用：`jdbc:postgresql://db.<project-ref>.supabase.co:5432/postgres`

2. **缺少批量配置**：
   - ❌ 未配置批量大小（低效）
   - ✅ 设置：`sink.buffer-flush.max-rows` 和 `sink.buffer-flush.interval`

3. **连接池耗尽**：
   - ❌ 太多并发写入
   - ✅ 监控连接池，调整并行度

4. **Schema 不匹配**：
   - ❌ 表 schema 与数据库表不匹配
   - ✅ 确保 schema 匹配数据库表结构

5. **未处理错误**：
   - ❌ 作业在数据库错误时失败
   - ✅ 配置重试，使用 dead letter queue（第 9.1 节）

---

## Mind Trigger: When to Think About This

在以下情况下考虑 JDBC connector：
- **将结果写入数据库**：MarketLag 将 lag signals 写入 Supabase
- **配置批量插入**：优化数据库写入性能
- **Supabase 集成**：MarketLag 使用 Supabase PostgreSQL
- **连接管理**：理解连接池有助于避免问题
- **错误处理**：第 5.2 节介绍最佳实践

**在 MarketLag 项目中**：Job 3 通过 JDBC sink 将 lag signals 写入 Supabase PostgreSQL。使用批量大小 100 和刷新间隔 10s。理解 JDBC connector 对于将结果写入数据库至关重要。

---

## Summary

Flink JDBC connector 支持使用 JDBC 从关系数据库读取和写入。JDBC sink 将 Flink 结果写入数据库表。批量插入配置（批量大小、刷新间隔）优化写入性能。连接池管理高效处理数据库连接。MarketLag 使用 JDBC connector 将 lag signals 写入 Supabase PostgreSQL。理解 JDBC connector 对于流应用程序中的数据库集成至关重要。

