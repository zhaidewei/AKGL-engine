# Flink 作业部署工作流

**Learning Point**: 13.1 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 6.3 (Deploying Flink Jobs), understanding of build tools (Maven/Gradle)
**Version**: Flink 2.2.0, Maven/Gradle

---

## Definition (Engineering Language)

**Flink Job Deployment Workflow**: The process of building, packaging, and deploying Flink applications to Confluent Cloud.

**Code Compilation**: Building Flink application code using Maven or Gradle build tools.

**JAR Packaging**: Creating a JAR file (fat JAR) that includes all dependencies for deployment.

**Job Submission**: Submitting the JAR to Confluent Cloud Flink environment via CLI, UI, or API.

**Job Configuration Management**: Managing environment-specific configurations (dev, staging, production).

---

## Plain Language Explanation

将 Flink 作业部署想象成运送包裹：

- **Code Compilation** = 打包物品："构建您的代码"
- **JAR Packaging** = 装箱所有内容："将所有内容放入一个盒子（fat JAR）"
- **Job Submission** = 运送："发送到 Confluent Cloud"
- **Configuration** = 地址标签："dev/prod 的不同地址"

**为什么需要**：MarketLag 需要将 Flink 作业部署到 Confluent Cloud 用于生产。

---

## Analogy

如果您了解 **application deployment**（您了解），Flink deployment 类似：
- Build process = Code compilation（都构建代码）
- Package creation = JAR packaging（都创建可部署工件）
- Deployment = Job submission（都部署到环境）

关键区别：Flink 作业是流应用程序，部署到 Confluent Cloud。

---

## Relationship to Already Learned Topics

- **6.3 Deploying Flink Jobs**：部署到 Confluent Cloud
- **Build Tools**：您已经了解 Maven/Gradle - 用于构建
- **CI/CD**：您已经了解 CI/CD - 可以自动化部署

---

## Code Compilation: Maven/Gradle Build

### Maven Build

**pom.xml**:
```xml
<project>
    <groupId>com.marketlag</groupId>
    <artifactId>marketlag-flink-jobs</artifactId>
    <version>1.0.0</version>

    <properties>
        <flink.version>2.2.0</flink.version>
        <scala.binary.version>2.12</scala.binary.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java-bridge</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <!-- Other dependencies -->
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.8.1</version>
                <configuration>
                    <source>11</source>
                    <target>11</target>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.4</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```

**Build Command**:
```bash
mvn clean package
```

**MarketLag**: Uses Maven to build Flink jobs.

### Gradle Build

**build.gradle**:
```gradle
plugins {
    id 'java'
}

repositories {
    mavenCentral()
}

dependencies {
    implementation "org.apache.flink:flink-streaming-java:2.2.0"
    implementation "org.apache.flink:flink-table-api-java-bridge:2.2.0"
    // Other dependencies
}

jar {
    archiveClassifier = 'all'
    from {
        configurations.runtimeClasspath.collect { it.isDirectory() ? it : zipTree(it) }
    }
}
```

**Build Command**:
```bash
./gradlew build
```

**MarketLag**: Can use Gradle as alternative to Maven.

---

## JAR Packaging: Including Dependencies (Fat JAR)

### Fat JAR Concept

**Problem**: Flink jobs need dependencies (Kafka connector, JDBC connector, etc.). Including them separately is complex.

**Solution**: Create fat JAR (uber JAR) that includes all dependencies.

**Maven Shade Plugin**: Creates fat JAR with all dependencies.

### Maven Shade Plugin Configuration

```xml
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>3.2.4</version>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <artifactSet>
                    <excludes>
                        <exclude>org.apache.flink:force-shading</exclude>
                    </excludes>
                </artifactSet>
                <filters>
                    <filter>
                        <artifact>*:*</artifact>
                        <excludes>
                            <exclude>META-INF/*.SF</exclude>
                            <exclude>META-INF/*.DSA</exclude>
                            <exclude>META-INF/*.RSA</exclude>
                        </excludes>
                    </filter>
                </filters>
            </configuration>
        </execution>
    </executions>
</plugin>
```

**Output**: `target/marketlag-flink-jobs-1.0.0-all.jar`

**MarketLag**: Creates fat JAR for deployment.

---

## Job Submission: CLI, UI, API

### CLI Submission

**Confluent Cloud CLI**:
```bash
confluent flink job create \
    --name job1-rss-signal-aggregation \
    --jar-file target/marketlag-flink-jobs-1.0.0-all.jar \
    --main-class com.marketlag.Job1RSSSignalAggregation \
    --compute-unit 2 \
    --parallelism 1
```

**MarketLag**: Can use CLI for deployment.

### UI Submission

**Confluent Cloud UI**:
1. Navigate to Flink environment
2. Click "Create Job"
3. Upload JAR file
4. Configure job (main class, parallelism, etc.)
5. Submit job

**MarketLag**: Uses UI for initial deployment and updates.

### API Submission

**Confluent Cloud API**:
```bash
curl -X POST https://api.confluent.cloud/flink/v1beta/environments/{env-id}/jobs \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
        "name": "job1-rss-signal-aggregation",
        "jar_file": "base64-encoded-jar",
        "main_class": "com.marketlag.Job1RSSSignalAggregation",
        "compute_unit": 2,
        "parallelism": 1
    }'
```

**MarketLag**: Can use API for automated deployment (CI/CD).

---

## Job Configuration Management: Environment-Specific Configs

### Configuration Files

**dev.properties**:
```properties
kafka.bootstrap.servers=dev-kafka:9092
postgres.jdbc.url=jdbc:postgresql://dev-db:5432/marketlag
checkpoint.interval=60000
```

**prod.properties**:
```properties
kafka.bootstrap.servers=prod-kafka:9092
postgres.jdbc.url=jdbc:postgresql://prod-db:5432/marketlag
checkpoint.interval=300000
```

### Loading Configuration

```java
public class JobConfig {
    public static Properties loadConfig(String env) {
        Properties props = new Properties();
        try (InputStream is = JobConfig.class.getResourceAsStream(env + ".properties")) {
            props.load(is);
        }
        return props;
    }
}

// Usage
Properties config = JobConfig.loadConfig(System.getenv("ENV"));
```

**MarketLag**: Uses environment-specific configurations for dev/prod.

---

## Minimum Viable Code

```bash
# Build JAR
mvn clean package

# Deploy to Confluent Cloud (CLI)
confluent flink job create \
    --name job1-rss-signal-aggregation \
    --jar-file target/marketlag-flink-jobs-1.0.0-all.jar \
    --main-class com.marketlag.Job1RSSSignalAggregation \
    --compute-unit 2 \
    --parallelism 1 \
    --checkpoint-interval 300000

# Or deploy via UI
# 1. Upload JAR in Confluent Cloud UI
# 2. Configure job settings
# 3. Submit job
```

---

## Common Mistakes

1. **不创建 fat JAR**：
   - ❌ 没有依赖项的 JAR（运行时缺少类）
   - ✅ 使用 maven-shade-plugin 或 Gradle fat JAR

2. **错误的 main class**：
   - ❌ Main class 未指定或错误
   - ✅ 在作业提交中指定正确的 main class

3. **不管理配置**：
   - ❌ 硬编码配置（无法为不同环境更改）
   - ✅ 使用特定环境的配置文件

4. **不在本地测试 JAR**：
   - ❌ 部署未测试的 JAR
   - ✅ 在部署前在本地测试 JAR

5. **不对 JAR 进行版本控制**：
   - ❌ 所有版本使用相同的 JAR 名称（无法回滚）
   - ✅ 对 JAR 进行版本控制（例如，`marketlag-jobs-1.0.0.jar`）

---

## Mind Trigger: When to Think About This

在以下情况下考虑 Flink 作业部署：
- **部署到 Confluent Cloud**：MarketLag 部署所有三个作业
- **构建 JARs**：使用 Maven/Gradle 构建 fat JARs
- **作业提交**：通过 CLI、UI 或 API 提交
- **配置管理**：使用特定环境的配置
- **CI/CD**：自动化部署工作流

**在 MarketLag 项目中**：使用 Maven 构建 JARs，创建包含所有依赖项的 fat JARs，通过 UI 或 CLI 提交到 Confluent Cloud。使用特定环境的配置。理解部署工作流对生产运维至关重要。

---

## Summary

Flink 作业部署工作流涉及构建代码（Maven/Gradle）、打包成 fat JAR（包含所有依赖项）以及提交到 Confluent Cloud（CLI、UI 或 API）。管理特定环境的配置。MarketLag 使用此工作流部署所有三个作业。理解部署工作流对生产运维至关重要。

