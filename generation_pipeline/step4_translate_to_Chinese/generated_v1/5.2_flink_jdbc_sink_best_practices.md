# Flink JDBC Sink 最佳实践

**Learning Point**: 5.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 5.1 (JDBC Connector Configuration), understanding of database operations
**Version**: Flink 2.2.0

---

## 工程化定义

**Idempotent Writes**: 可以安全重复执行而不会产生副作用的数据库写入。UPSERT 操作是幂等的，INSERT 操作不是。

**UPSERT vs INSERT**: UPSERT (INSERT ... ON CONFLICT UPDATE) 更新现有行或插入新行。INSERT 仅插入，在重复时失败。

**Error Handling**: 处理数据库写入失败的策略：重试逻辑、死信队列、指数退避。

**Transaction Management**: 确保原子写入（全部或全无）。Flink JDBC sink 支持事务。

**Connection Failure Handling**: 处理数据库连接失败的策略：指数退避重试、连接池。

---

## 通俗解释

将 JDBC sink 最佳实践视为数据库写入指南：

- **Idempotent Writes** = 安全重试："如果写入失败，重试是安全的"
- **UPSERT** = 更新或插入："如果存在则更新，否则插入"
- **Error Handling** = 故障恢复："写入失败时该做什么"
- **Transactions** = 全部或全无："要么写入所有行，要么都不写入"

**为什么需要**: MarketLag 将 lag signals 写入 Supabase。最佳实践确保可靠的写入并优雅地处理故障。

---

## 类比理解

如果您了解 **数据库最佳实践**（您了解），Flink JDBC sink 实践类似：
- Database UPSERT = Flink JDBC UPSERT（两者都更新或插入）
- Database transactions = Flink JDBC transactions（两者都是原子的）
- Database retry = Flink JDBC retry（两者都处理故障）

关键区别：Flink JDBC sink 在流处理上下文中自动处理重试和事务。

---

## 与已学内容的关系

- **5.1 JDBC Connector**: Best practices build on JDBC connector configuration
- **9.1 Dead Letter Queue**: DLQ for failed database writes
- **9.3 Exception Handling**: Error handling in Flink operators
- **Database Operations**: You already know database best practices

---

## 伪代码（基于源码）

Based on Flink 2.2.0 source code:

```java
// JDBC sink with retry (simplified)
class JdbcSink {
    void write(List<Row> batch) {
        int retries = 0;
        while (retries < maxRetries) {
            try {
                executeBatch(batch);
                return;  // Success
            } catch (Exception e) {
                retries++;
                if (retries >= maxRetries) {
                    sendToDLQ(batch);  // Dead letter queue
                } else {
                    Thread.sleep(exponentialBackoff(retries));
                }
            }
        }
    }
}
```

---

## 源码参考

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/sink/JdbcSink.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/sink/JdbcSink.java

---

## Idempotent Writes: UPSERT vs INSERT

### INSERT (Not Idempotent)

```sql
INSERT INTO lag_signals_history (market, window, signal_delta)
VALUES ('market-1', '2024-01-15 14:00:00', 1.5);
```

**Problem**: Fails on duplicate (primary key violation).

**Use Case**: When duplicates are impossible (e.g., unique constraints).

### UPSERT (Idempotent)

```sql
INSERT INTO lag_signals_history (market, window, signal_delta)
VALUES ('market-1', '2024-01-15 14:00:00', 1.5)
ON CONFLICT (market, window)
DO UPDATE SET signal_delta = EXCLUDED.signal_delta;
```

**Benefit**: Safe to retry - updates if exists, inserts if not.

**MarketLag**: Should use UPSERT for idempotent writes.

### Flink JDBC UPSERT Configuration

```sql
CREATE TABLE lag_signals_sink (
    market VARCHAR(100),
    window TIMESTAMP,
    signal_delta DECIMAL(10, 4),
    PRIMARY KEY (market, window) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://...',
    'table-name' = 'lag_signals_history',
    'sink.buffer-flush.max-rows' = '100',
    'sink.buffer-flush.interval' = '10s'
);
```

**Note**: Flink JDBC sink doesn't directly support UPSERT syntax. Use database-specific UPSERT or handle in application logic.

---

## 错误处理：重试逻辑与死信处理

### Retry Logic

Flink JDBC sink supports retry configuration:

```sql
'sink.max-retries' = '3'  -- Max retry attempts
```

**Behavior**: Retries failed writes up to max-retries times.

### Exponential Backoff

Implement exponential backoff in custom error handler:

```java
long backoffDelay = Math.min(1000 * (1L << retryCount), 60000);  // Max 60s
Thread.sleep(backoffDelay);
```

**Benefit**: Reduces load on database during failures.

### Dead Letter Queue

Send failed records to DLQ after max retries:

```java
if (retries >= maxRetries) {
    sendToDLQ(failedRecords);  // Dead letter queue (section 9.1)
}
```

**MarketLag**: Should implement DLQ for failed database writes.

---

## 事务管理

### Automatic Transactions

Flink JDBC sink uses transactions automatically:
- **Batch Writes**: Each batch is a transaction
- **Atomicity**: All rows in batch written or none
- **Consistency**: Database remains consistent

### Transaction Configuration

```sql
'sink.buffer-flush.max-rows' = '100'  -- Batch size (transaction size)
```

**Effect**: Each batch of 100 rows is one transaction.

**MarketLag**: Uses batch size 100, so each batch is one transaction.

---

## Handling Connection Failures: Retry with Exponential Backoff

### Connection Pool Configuration

Flink JDBC sink manages connection pool:
- **Automatic Retry**: Retries on connection failures
- **Connection Pooling**: Reuses connections efficiently
- **Timeout Handling**: Handles connection timeouts

### Exponential Backoff Pattern

```java
int retryCount = 0;
while (retryCount < maxRetries) {
    try {
        writeToDatabase(batch);
        break;  // Success
    } catch (ConnectionException e) {
        retryCount++;
        long delay = Math.min(1000 * (1L << retryCount), 60000);
        Thread.sleep(delay);
    }
}
```

**Benefit**: Reduces load during database outages.

---

## 最小可用代码

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class JdbcSinkBestPracticesDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        // Create JDBC sink with best practices
        tableEnv.executeSql(
            "CREATE TABLE lag_signals_sink (" +
            "  market VARCHAR(100)," +
            "  window TIMESTAMP," +
            "  signal_delta DECIMAL(10, 4)," +
            "  PRIMARY KEY (market, window) NOT ENFORCED" +
            ") WITH (" +
            "  'connector' = 'jdbc'," +
            "  'url' = 'jdbc:postgresql://...'," +
            "  'table-name' = 'lag_signals_history'," +
            "  'username' = 'postgres'," +
            "  'password' = 'password'," +
            "  'sink.buffer-flush.max-rows' = '100'," +
            "  'sink.buffer-flush.interval' = '10s'," +
            "  'sink.max-retries' = '3'" +
            ")"
        );

        // Write with UPSERT (handled in database or application)
        tableEnv.executeSql(
            "INSERT INTO lag_signals_sink " +
            "SELECT market, window, signal_delta FROM processed_data"
        );

        env.execute("JDBC Sink Best Practices Demo");
    }
}
```

---

## 常见错误

1. **不使用 UPSERT**:
   - ❌ 使用 INSERT，在重复时失败
   - ✅ 使用 UPSERT 进行幂等写入

2. **没有错误处理**:
   - ❌ 作业在数据库错误时失败
   - ✅ 配置重试，实现 DLQ

3. **批次大小太大**:
   - ❌ 批次大小 10000（大事务，慢）
   - ✅ 使用合理的批次大小（100-1000）

4. **没有连接重试**:
   - ❌ 作业在连接错误时失败
   - ✅ 配置指数退避的连接重试

5. **不监控故障**:
   - ❌ 不跟踪失败的写入
   - ✅ 监控 DLQ，设置警报

---

## 触发点：什么时候想到这个概念

在以下情况下考虑 JDBC sink 最佳实践：
- **写入数据库**: MarketLag 将 lag signals 写入 Supabase
- **确保可靠性**: 幂等写入、错误处理
- **处理故障**: 重试逻辑、DLQ（第 9.1 节）
- **性能优化**: 批次大小、事务管理
- **生产部署**: 最佳实践对生产至关重要

**在 MarketLag 项目中**: 将 lag signals 写入 Supabase PostgreSQL。应使用 UPSERT 进行幂等写入，配置重试逻辑，并为失败的写入实现 DLQ。理解最佳实践确保可靠的数据库写入。

---

## 小结

Flink JDBC sink 最佳实践包括幂等写入（UPSERT）、错误处理（重试、DLQ）、事务管理（批次写入）和连接故障处理（指数退避）。MarketLag 应使用 UPSERT 进行幂等写入，配置重试逻辑，并实现 DLQ。理解最佳实践确保生产中的可靠数据库写入。

