# RSS Feed 处理

**Learning Point**: 7.1 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 理解 RSS feeds、Python 基础知识、时区处理
**Version**: Python 3.9+, RSS.app API

---

## Definition (Engineering Language)

**RSS Feed Processing**: 获取、解析和处理 RSS (Really Simple Syndication) feeds 以提取结构化数据（title、link、published_at 等）供下游处理的过程。

**RSS Feed Structure**: 基于 XML 的格式，包含带有元数据的 items（title、link、description、published_at、guid）。

**RSS Parsing Libraries**: 用于解析 RSS feeds 的 Python 库：feedparser、rss-parser、BeautifulSoup。

**RSS Item Deduplication**: 避免处理重复 items 的策略：使用 guid（全局唯一标识符）、link 或内容哈希。

**Timezone Handling**: 将 published_at 时间戳从源时区转换为 UTC 以进行一致处理。

---

## Plain Language Explanation

将 RSS feed 处理想象成阅读报纸：

- **RSS Feed** = 报纸：包含带有标题、链接、日期的文章（items）
- **Parsing** = 阅读文章：从每篇文章中提取信息
- **Deduplication** = 跳过重复项："我已经读过这篇文章了"
- **Timezone Conversion** = 标准化时间："将所有时间转换为 UTC"

**Why Needed**: MarketLag 从 RSS.app 获取 RSS feeds 以检测可能影响市场的新闻事件。

---

## Analogy

如果您了解 **web scraping**（您了解），RSS feed 处理是类似的：
- Web scraping = RSS parsing（两者都提取结构化数据）
- HTML parsing = RSS XML parsing（两者都解析结构化格式）
- Deduplication = 两者都需要避免处理重复项

关键区别：RSS 是标准化格式，比 HTML 更容易解析。

---

## Relationship to Already Learned Topics

- **1.4 Time Concepts**: 时区处理转换为 UTC
- **8.1 AWS Lambda**: RSS 处理在 Lambda 函数中运行
- **8.2 EventBridge**: RSS 轮询的定时触发器
- **Python**: 您已经了解 Python - RSS 解析使用 Python

---

## RSS Feed Structure: Items, Title, Link, published_at

### RSS XML Structure

```xml
<rss>
  <channel>
    <item>
      <title>Fed raises interest rates</title>
      <link>https://example.com/article</link>
      <pubDate>Mon, 15 Jan 2024 14:30:00 EST</pubDate>
      <guid>unique-article-id</guid>
      <description>Article content...</description>
    </item>
  </channel>
</rss>
```

### Key Fields

- **title**: 文章标题
- **link**: 文章 URL
- **pubDate**: 发布时间戳（时区感知）
- **guid**: 全局唯一标识符（用于去重）
- **description**: 文章摘要/内容

**MarketLag**: 提取这些字段进行处理。

---

## RSS Parsing Libraries: feedparser (Python), rss-parser

### feedparser (Python - MarketLag 使用此库)

```python
import feedparser

# Parse RSS feed
feed = feedparser.parse('https://rss.app/feeds/v1.1/{feed_id}.json')

# Access items
for entry in feed.entries:
    title = entry.title
    link = entry.link
    published = entry.published  # String, needs parsing
    guid = entry.get('id', entry.link)  # Use guid or link as fallback
```

**MarketLag**: 在 Lambda 函数中使用 feedparser 解析 RSS feeds。

### rss-parser (替代方案)

```python
from rss_parser import Parser

parser = Parser(xml=feed_xml)
feed = parser.parse()

for item in feed.feed:
    title = item.title
    link = item.link
    published = item.pub_date
```

---

## RSS Item Deduplication Strategies: guid, link, hash

### Strategy 1: GUID (Globally Unique Identifier)

```python
seen_guids = set()

for entry in feed.entries:
    guid = entry.get('id', entry.link)
    if guid not in seen_guids:
        seen_guids.add(guid)
        process_item(entry)
```

**Pros**: GUID 专为唯一性设计
**Cons**: 并非所有 feeds 都有 GUID

### Strategy 2: Link

```python
seen_links = set()

for entry in feed.entries:
    link = entry.link
    if link not in seen_links:
        seen_links.add(link)
        process_item(entry)
```

**Pros**: 链接通常是唯一的
**Cons**: 同一篇文章可能有不同的 URLs

### Strategy 3: Content Hash

```python
import hashlib

seen_hashes = set()

for entry in feed.entries:
    content = f"{entry.title}{entry.description}"
    content_hash = hashlib.md5(content.encode()).hexdigest()
    if content_hash not in seen_hashes:
        seen_hashes.add(content_hash)
        process_item(entry)
```

**Pros**: 即使 URLs 不同也能捕获重复内容
**Cons**: 更多计算，可能有误报

**MarketLag**: 使用 guid 或 link 进行去重（存储在 DynamoDB 或 Lambda 的内存中）。

---

## Timezone Handling: Converting published_at to UTC

### Problem

RSS feeds 可能使用不同的时区：
- EST (Eastern Standard Time)
- PST (Pacific Standard Time)
- UTC
- Local timezone

**Need**: 将所有时间转换为 UTC 以进行一致处理。

### Solution

```python
from datetime import datetime
import pytz

def parse_rss_date(date_string):
    """
    Parse RSS date string and convert to UTC.

    Args:
        date_string: RSS pubDate string (e.g., "Mon, 15 Jan 2024 14:30:00 EST")

    Returns:
        datetime: UTC datetime object
    """
    # Parse date string (feedparser handles this)
    dt = feedparser._parse_date(date_string)

    # If timezone-aware, convert to UTC
    if dt.tzinfo:
        utc_dt = dt.astimezone(pytz.UTC)
    else:
        # Assume UTC if no timezone
        utc_dt = dt.replace(tzinfo=pytz.UTC)

    return utc_dt

# Usage
published_str = entry.published
published_utc = parse_rss_date(published_str)
```

**MarketLag**: 在发送到 Kafka 之前将所有 published_at 转换为 UTC。

---

## RSS.app API: `https://rss.app/feeds/v1.1/{feed_id}.json`

### API Endpoint

**Format**: `https://rss.app/feeds/v1.1/{feed_id}.json`

**Example**: `https://rss.app/feeds/v1.1/x6KH5aqpKp2jqVNE.json`

**Method**: GET

**Authentication**: 不需要（公共 feed URL）

**Response**: JSON 格式（不是 XML）

### MarketLag Usage

```python
import requests
import feedparser

feed_id = "x6KH5aqpKp2jqVNE"
url = f"https://rss.app/feeds/v1.1/{feed_id}.json"

response = requests.get(url)
feed_data = response.json()

# Process feed items
for item in feed_data.get('items', []):
    title = item.get('title')
    link = item.get('link')
    published_at = item.get('published_at')  # ISO 8601 format
    # Process item...
```

**MarketLag**: 通过 EventBridge 每 15 分钟轮询一次 RSS.app API（第 8.2 节）。

---

## Minimum Viable Code

```python
import feedparser
from datetime import datetime
import pytz
import hashlib

def process_rss_feed(feed_url):
    """
    Process RSS feed: parse, deduplicate, convert timezone.

    Args:
        feed_url: RSS feed URL

    Returns:
        list: Processed RSS items
    """
    # Parse feed
    feed = feedparser.parse(feed_url)

    seen_guids = set()
    processed_items = []

    for entry in feed.entries:
        # Deduplication: use guid or link
        guid = entry.get('id', entry.link)
        if guid in seen_guids:
            continue
        seen_guids.add(guid)

        # Extract fields
        title = entry.title
        link = entry.link
        published_str = entry.published

        # Parse and convert to UTC
        published_dt = feedparser._parse_date(published_str)
        if published_dt.tzinfo:
            published_utc = published_dt.astimezone(pytz.UTC)
        else:
            published_utc = published_dt.replace(tzinfo=pytz.UTC)

        # Create item
        item = {
            'title': title,
            'link': link,
            'published_at': published_utc.isoformat(),  # ISO 8601
            'guid': guid
        }

        processed_items.append(item)

    return processed_items

# Usage
feed_url = "https://rss.app/feeds/v1.1/x6KH5aqpKp2jqVNE.json"
items = process_rss_feed(feed_url)
```

---

## Common Mistakes

1. **Not handling timezones**:
   - ❌ 使用本地时区，导致不对齐
   - ✅ 始终转换为 UTC（第 1.4 节）

2. **Not deduplicating**:
   - ❌ 多次处理同一篇文章
   - ✅ 使用 guid 或 link 进行去重

3. **Not handling missing fields**:
   - ❌ 假设所有字段都存在
   - ✅ 使用带默认值的 `.get()`：`entry.get('guid', entry.link)`

4. **Not handling API errors**:
   - ❌ 没有处理 RSS.app API 失败的错误处理
   - ✅ 实现重试逻辑、错误处理（第 9.1 节）

5. **Polling too frequently**:
   - ❌ 每分钟轮询（速率限制）
   - ✅ 每 15 分钟轮询一次（MarketLag 模式，第 8.2 节）

---

## Mind Trigger: When to Think About This

在以下情况下考虑 RSS feed 处理：
- **获取新闻数据**: MarketLag 获取 RSS feeds 以进行 lag 检测
- **解析 feeds**: 使用 feedparser 或类似库
- **去重**: 避免处理重复文章
- **时区转换**: 将所有时间戳转换为 UTC（第 1.4 节）
- **Lambda 实现**: 第 8.1 节涵盖 Lambda 函数结构

**在 MarketLag 项目中**: RSS producer（Lambda）每 15 分钟轮询一次 RSS.app API，解析 feeds，对 items 进行去重，将时间戳转换为 UTC，并发送到 Kafka。理解 RSS 处理对于数据获取至关重要。

---

## Summary

RSS feed 处理涉及获取、解析和处理 RSS feeds 以提取结构化数据。RSS feeds 包含带有 title、link、published_at、guid 的 items。使用 feedparser（Python）进行解析。使用 guid 或 link 进行去重。将 published_at 转换为 UTC 以进行一致处理。MarketLag 使用 RSS.app API 获取 feeds。理解 RSS 处理对于新闻数据获取至关重要。
