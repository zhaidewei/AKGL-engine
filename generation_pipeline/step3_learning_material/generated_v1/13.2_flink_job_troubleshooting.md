# Flink Job Troubleshooting

**Learning Point**: 13.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.10 (Checkpoints), 11.1 (Flink Metrics), understanding of debugging
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink Job Troubleshooting**: Techniques and strategies for diagnosing and resolving issues in Flink production jobs.

**Common Issues**: Checkpoint failures, backpressure, out-of-memory (OOM) errors.

**Debugging Techniques**: Using logs, metrics, and Flink UI to identify issues.

**Performance Tuning**: Adjusting parallelism, state backend, and checkpoint interval to optimize performance.

---

## Plain Language Explanation

Think of troubleshooting like diagnosing a car problem:

- **Common Issues** = Common problems: "Checkpoint failures, backpressure, OOM"
- **Debugging** = Diagnostic tools: "Logs, metrics, UI inspection"
- **Performance Tuning** = Tune-up: "Adjust settings for better performance"

**Why Needed**: MarketLag needs to troubleshoot and maintain healthy production jobs.

---

## Analogy

If you know **application troubleshooting** (which you do), Flink troubleshooting is similar:
- Application logs = Flink logs (both help diagnose issues)
- Application metrics = Flink metrics (both indicate problems)
- Performance tuning = Optimization (both improve performance)

Key difference: Flink troubleshooting focuses on streaming-specific issues (backpressure, checkpoints).

---

## Relationship to Already Learned Topics

- **1.10 Checkpoints**: Troubleshooting checkpoint failures
- **11.1 Flink Metrics**: Using metrics for troubleshooting
- **Debugging**: You already know debugging - Flink extends it

---

## Common Issues: Checkpoint Failures, Backpressure, OOM

### Checkpoint Failures

**Symptoms**: Checkpoints fail repeatedly, job may fail.

**Causes**:
- State too large
- Backpressure (slow sink)
- Network issues
- Timeout too short

**Solutions**:
- Increase checkpoint timeout
- Reduce state size (TTL, cleanup)
- Fix backpressure (increase sink parallelism)
- Check network connectivity

**MarketLag**: Monitors checkpoint success rate, alerts on failures.

### Backpressure

**Symptoms**: High latency, low throughput, checkpoint failures.

**Causes**:
- Slow sink (PostgreSQL writes)
- Insufficient parallelism
- Large state operations

**Solutions**:
- Increase sink parallelism
- Optimize sink writes (batch inserts)
- Increase parallelism for slow operators
- Check sink performance

**MarketLag**: Monitors backpressure metrics, adjusts parallelism.

### Out-of-Memory (OOM)

**Symptoms**: Job fails with OOM error, task manager crashes.

**Causes**:
- State too large
- Insufficient memory
- Memory leaks

**Solutions**:
- Increase task manager memory
- Use RocksDB state backend (off-heap)
- Reduce state size (TTL, cleanup)
- Check for memory leaks

**MarketLag**: Uses RocksDB state backend to avoid OOM.

---

## Debugging Techniques: Logs, Metrics, UI Inspection

### Logs

**Location**: Confluent Cloud Flink UI → Job → Logs

**Key Log Messages**:
- Checkpoint failures: "Checkpoint failed"
- Backpressure: "Backpressure detected"
- OOM: "OutOfMemoryError"

**MarketLag**: Reviews logs in Confluent Cloud UI.

### Metrics

**Key Metrics**:
- `numRecordsInPerSecond`: Throughput
- `latency`: End-to-end latency
- `checkpointDuration`: Checkpoint time
- `backpressure`: Backpressure indicator

**MarketLag**: Monitors metrics in Grafana (section 11.2).

### UI Inspection

**Flink UI Features**:
- Job graph visualization
- Operator metrics
- Checkpoint history
- Backpressure visualization

**MarketLag**: Uses Confluent Cloud Flink UI for inspection.

---

## Performance Tuning: Parallelism, State Backend, Checkpoint Interval

### Parallelism Tuning

**Rule of Thumb**: Parallelism = number of CPU cores or Kafka partitions.

**MarketLag**: Uses parallelism = 1 for MVP (low data volume).

**Tuning**:
- Increase parallelism for slow operators
- Match parallelism to Kafka partitions
- Monitor throughput to find optimal parallelism

### State Backend Tuning

**Options**:
- MemoryStateBackend: Fast, limited by memory
- FsStateBackend: Fast, limited by memory
- RocksDBStateBackend: Slower, unlimited state size

**MarketLag**: Uses RocksDB for large state (max_signal_delta).

**Tuning**:
- Use RocksDB for large state
- Configure RocksDB memory settings
- Monitor state size

### Checkpoint Interval Tuning

**Trade-off**: More frequent checkpoints = more overhead, faster recovery.

**MarketLag**: Uses 5-minute checkpoint interval.

**Tuning**:
- Increase interval for low-latency jobs (less overhead)
- Decrease interval for critical jobs (faster recovery)
- Monitor checkpoint duration

---

## Minimum Viable Code

```java
// Example: Adding logging for troubleshooting
public class MyProcessFunction extends KeyedProcessFunction<String, Event, Result> {
    private static final Logger LOG = LoggerFactory.getLogger(MyProcessFunction.class);

    @Override
    public void processElement(Event event, Context ctx, Collector<Result> out) {
        try {
            // Process event
            Result result = processEvent(event);
            out.collect(result);
        } catch (Exception e) {
            // Log error for troubleshooting
            LOG.error("Error processing event: {}", event, e);
            // Send to side output or DLQ
        }
    }
}
```

---

## Common Mistakes

1. **Not monitoring metrics**:
   - ❌ Issues go unnoticed until job fails
   - ✅ Monitor metrics regularly (throughput, latency, checkpoints)

2. **Not checking logs**:
   - ❌ Ignoring log errors
   - ✅ Review logs regularly, set up log alerts

3. **Not tuning performance**:
   - ❌ Accepting poor performance
   - ✅ Tune parallelism, state backend, checkpoint interval

4. **Not handling backpressure**:
   - ❌ Backpressure causes job failures
   - ✅ Monitor backpressure, adjust parallelism

5. **Not testing changes**:
   - ❌ Making changes without testing
   - ✅ Test performance tuning changes in dev environment

---

## Mind Trigger: When to Think About This

Think about Flink job troubleshooting when:
- **Job failures**: MarketLag needs to diagnose and fix failures
- **Performance issues**: Troubleshoot slow jobs, backpressure
- **Checkpoint failures**: Diagnose and fix checkpoint issues
- **Monitoring**: Use metrics and logs for troubleshooting
- **Performance tuning**: Optimize parallelism, state backend, checkpoint interval

**In MarketLag project**: Troubleshoots checkpoint failures, backpressure, and OOM issues. Uses logs, metrics, and UI for debugging. Tunes performance (parallelism, state backend, checkpoint interval). Understanding troubleshooting is essential for maintaining healthy production jobs.

---

## Summary

Flink job troubleshooting involves diagnosing and resolving common issues (checkpoint failures, backpressure, OOM). Use logs, metrics, and UI for debugging. Tune performance (parallelism, state backend, checkpoint interval). MarketLag troubleshoots issues and optimizes performance. Understanding troubleshooting is essential for maintaining healthy production jobs.

