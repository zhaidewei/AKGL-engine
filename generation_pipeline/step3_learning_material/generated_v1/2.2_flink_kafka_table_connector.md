# Flink Kafka Table Connector

**Learning Point**: 2.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.3 (Table API/SQL), 2.1 (Kafka Connector), Kafka knowledge
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink Kafka Table Connector**: A Flink SQL table connector that enables reading from and writing to Kafka topics using SQL syntax. Provides a higher-level abstraction than the DataStream KafkaSource/KafkaSink API.

**CREATE TABLE Syntax**: SQL DDL statement to define a Kafka topic as a Flink table with schema, format, and connector properties.

**Table Format**: Defines how to serialize/deserialize data (JSON, Avro, CSV, etc.). MarketLag project uses JSON format.

**Schema Definition**: Defines the table schema (columns, types) that maps to Kafka message structure.

---

## Plain Language Explanation

Think of Kafka table connector as a SQL view over Kafka:

- **CREATE TABLE** = Define a view: "This Kafka topic is a table with these columns"
- **SELECT FROM** = Read from Kafka: Query the table like any SQL table
- **INSERT INTO** = Write to Kafka: Insert rows into the table (writes to Kafka topic)

**Why It's Better**: Instead of writing Java code with KafkaSource, you write SQL. MarketLag project uses this for all three jobs.

---

## Analogy

If you know **Spark SQL with Kafka** (which you do), Flink's approach is very similar:
- Spark `spark.readStream.format("kafka")` = Flink `CREATE TABLE ... WITH ('connector' = 'kafka')`
- Both let you query Kafka topics as SQL tables

Key difference: Flink SQL is more integrated and provides better streaming semantics.

---

## Relationship to Already Learned Topics

- **1.3 Table API/SQL**: Kafka table connector uses Flink SQL syntax
- **2.1 Kafka Connector**: Table connector uses KafkaSource/KafkaSink under the hood
- **1.4 Time Concepts**: Table definitions include event time and watermarks
- **1.5 Watermarks**: WATERMARK declaration in CREATE TABLE

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// Kafka table connector (simplified)
class KafkaTableSource implements TableSource {
    String topic;
    String format;  // "json", "avro", etc.
    Schema schema;

    DataStream<Row> getDataStream(StreamExecutionEnvironment env) {
        // Convert to KafkaSource
        KafkaSource source = createKafkaSource(topic, format, schema);
        return env.fromSource(source, ...);
    }
}

// CREATE TABLE parsing (simplified)
class TableParser {
    Table parseCreateTable(String sql) {
        // Parse: CREATE TABLE ... WITH ('connector' = 'kafka', ...)
        // Extract: topic, format, schema, properties
        return new KafkaTable(topic, format, schema, properties);
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-connectors/flink-sql-connector-kafka/src/main/java/org/apache/flink/table/descriptors/Kafka.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-connectors/flink-sql-connector-kafka/src/main/java/org/apache/flink/table/descriptors/Kafka.java

**Key Classes**:
- `KafkaTableSource`: Kafka table source implementation
- `KafkaTableSink`: Kafka table sink implementation

---

## Kafka Table Connector Configuration in Flink SQL

### Basic CREATE TABLE Syntax

```sql
CREATE TABLE rss_events (
    title STRING,
    link STRING,
    published_at TIMESTAMP(3),
    source STRING,
    keywords ARRAY<STRING>,
    WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true'
);
```

**MarketLag Project**: Uses this exact pattern for rss_events table.

### Confluent Cloud Configuration

```sql
CREATE TABLE rss_events (
    title STRING,
    link STRING,
    published_at TIMESTAMP(3),
    source STRING,
    keywords ARRAY<STRING>,
    WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'PLAIN',
    'properties.sasl.username' = '<api-key>',
    'properties.sasl.password' = '<api-secret>',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true'
);
```

**MarketLag Project**: Uses Confluent Cloud with SASL_SSL authentication.

---

## Schema Definition: Format (JSON, Avro)

### JSON Format (MarketLag Uses This)

```sql
CREATE TABLE events (
    id BIGINT,
    value DOUBLE,
    timestamp TIMESTAMP(3)
) WITH (
    'connector' = 'kafka',
    'topic' = 'events',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true'  -- Skip malformed JSON
);
```

**JSON Message Format**:
```json
{"id": 1, "value": 10.5, "timestamp": "2024-01-15T14:30:00Z"}
```

### Avro Format (With Schema Registry - Section 2.3)

```sql
CREATE TABLE events (
    id BIGINT,
    value DOUBLE,
    timestamp TIMESTAMP(3)
) WITH (
    'connector' = 'kafka',
    'topic' = 'events',
    'format' = 'avro',
    'avro.schema-registry.url' = 'http://schema-registry:8081'
);
```

---

## Kafka Topic as Flink Table: CREATE TABLE ... WITH (...)

### Required Properties

- **connector**: Must be 'kafka'
- **topic**: Kafka topic name
- **properties.bootstrap.servers**: Kafka broker addresses
- **format**: Data format ('json', 'avro', 'csv', etc.)

### Optional Properties

- **properties.group.id**: Consumer group ID (auto-generated if not set)
- **scan.startup.mode**: 'earliest', 'latest', 'group-offsets', 'timestamp'
- **json.ignore-parse-errors**: Skip malformed JSON (true/false)
- **sink.partitioner**: How to partition output ('fixed', 'round-robin', 'custom')

---

## Reading from Kafka: SELECT FROM kafka_table

### Basic Read

```sql
SELECT * FROM rss_events;
```

### Filtered Read

```sql
SELECT title, published_at, source
FROM rss_events
WHERE source = 'Reuters'
  AND published_at > CURRENT_TIMESTAMP - INTERVAL '1' HOUR;
```

### Windowed Aggregation (Job 1 Pattern)

```sql
SELECT
    market_slug,
    TUMBLE_START(published_at, INTERVAL '1' HOUR) as window_start,
    TUMBLE_END(published_at, INTERVAL '1' HOUR) as window_end,
    COUNT(*) as mention_count,
    SUM(keyword_score) as keyword_score,
    AVG(article_score * source_weight) as source_weighted_signal
FROM rss_events
GROUP BY market_slug, TUMBLE(published_at, INTERVAL '1' HOUR);
```

**MarketLag Job 1**: Uses this pattern to aggregate RSS events hourly.

---

## Writing to Kafka: INSERT INTO kafka_table

### Basic Write

```sql
INSERT INTO output_topic
SELECT market_slug, window_start, signal
FROM processed_events;
```

### Write to Multiple Topics (Not Directly Supported)

Use separate INSERT statements or create multiple sink tables.

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class KafkaTableConnectorDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        // Create Kafka source table
        tableEnv.executeSql(
            "CREATE TABLE rss_events (" +
            "  title STRING," +
            "  published_at TIMESTAMP(3)," +
            "  source STRING," +
            "  WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE" +
            ") WITH (" +
            "  'connector' = 'kafka'," +
            "  'topic' = 'rss.events'," +
            "  'properties.bootstrap.servers' = 'localhost:9092'," +
            "  'format' = 'json'" +
            ")"
        );

        // Create Kafka sink table
        tableEnv.executeSql(
            "CREATE TABLE rss_signals_hourly (" +
            "  market_slug STRING," +
            "  window_start TIMESTAMP(3)," +
            "  signal DOUBLE" +
            ") WITH (" +
            "  'connector' = 'kafka'," +
            "  'topic' = 'rss.signals.hourly'," +
            "  'properties.bootstrap.servers' = 'localhost:9092'," +
            "  'format' = 'json'" +
            ")"
        );

        // Query and write
        tableEnv.executeSql(
            "INSERT INTO rss_signals_hourly " +
            "SELECT " +
            "  'market-1' as market_slug," +
            "  TUMBLE_START(published_at, INTERVAL '1' HOUR) as window_start," +
            "  COUNT(*) as signal " +
            "FROM rss_events " +
            "GROUP BY TUMBLE(published_at, INTERVAL '1' HOUR)"
        );
    }
}
```

---

## MarketLag Project Table Definitions

### RSS Events Table (Job 1 Input)

```sql
CREATE TABLE rss_events (
    title STRING,
    link STRING,
    published_at TIMESTAMP(3),
    source STRING,
    keywords ARRAY<STRING>,
    market_slug STRING,
    keyword_score DOUBLE,
    article_score DOUBLE,
    source_weight DOUBLE,
    WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'PLAIN',
    'properties.sasl.username' = '<api-key>',
    'properties.sasl.password' = '<api-secret>',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true'
);
```

### Polymarket Price Table (Job 3 Input)

```sql
CREATE TABLE polymarket_price_hourly (
    market_slug STRING,
    outcome STRING,
    event_time TIMESTAMP(3),
    price DOUBLE,
    price_delta DOUBLE,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' MINUTE
) WITH (
    'connector' = 'kafka',
    'topic' = 'polymarket.price_hourly',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'PLAIN',
    'properties.sasl.username' = '<api-key>',
    'properties.sasl.password' = '<api-secret>',
    'format' = 'json',
    'json.ignore-parse-errors' = 'true'
);
```

---

## Common Mistakes

1. **Missing WATERMARK declaration**:
   - ❌ CREATE TABLE without WATERMARK for event-time windows
   - ✅ Always declare WATERMARK for event-time processing

2. **Wrong timestamp precision**:
   - ❌ Using TIMESTAMP instead of TIMESTAMP(3)
   - ✅ Use TIMESTAMP(3) for millisecond precision (required for watermarks)

3. **Not configuring authentication**:
   - ❌ Missing SASL_SSL config for Confluent Cloud
   - ✅ Always include security.protocol, sasl.mechanism, sasl.username, sasl.password

4. **JSON parse errors**:
   - ❌ Job fails on malformed JSON
   - ✅ Set 'json.ignore-parse-errors' = 'true' to skip bad records

5. **Schema mismatch**:
   - ❌ Table schema doesn't match JSON structure
   - ✅ Ensure schema matches actual Kafka message format

---

## Mind Trigger: When to Think About This

Think about Kafka table connector when:
- **Writing Flink SQL jobs**: MarketLag uses this for all three jobs
- **Defining table schemas**: CREATE TABLE statements define Kafka topic structure
- **Configuring watermarks**: WATERMARK declaration in CREATE TABLE (section 1.5)
- **Handling formats**: JSON vs Avro (section 2.3 covers Schema Registry)
- **Troubleshooting**: Understanding table connector helps debug read/write issues

**In MarketLag project**: All three jobs use Kafka table connectors. Job 1 reads rss_events, Job 3 reads polymarket.price_hourly. Tables are defined with WATERMARK declarations for event-time processing.

---

## Summary

Flink Kafka table connector enables reading from and writing to Kafka using SQL. CREATE TABLE defines Kafka topics as Flink tables with schema, format, and connector properties. MarketLag uses JSON format with Confluent Cloud SASL_SSL authentication. Tables include WATERMARK declarations for event-time processing. This is the primary pattern used in MarketLag project for all three jobs.

