# Flink JDBC Sink Best Practices

**Learning Point**: 5.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 5.1 (JDBC Connector Configuration), understanding of database operations
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Idempotent Writes**: Database writes that can be safely repeated without side effects. UPSERT operations are idempotent, INSERT operations are not.

**UPSERT vs INSERT**: UPSERT (INSERT ... ON CONFLICT UPDATE) updates existing rows or inserts new ones. INSERT only inserts, fails on duplicates.

**Error Handling**: Strategies for handling database write failures: retry logic, dead letter queues, exponential backoff.

**Transaction Management**: Ensuring atomic writes (all or nothing). Flink JDBC sink supports transactions.

**Connection Failure Handling**: Strategies for handling database connection failures: retry with exponential backoff, connection pooling.

---

## Plain Language Explanation

Think of JDBC sink best practices like database write guidelines:

- **Idempotent Writes** = Safe retries: "If write fails, retry is safe"
- **UPSERT** = Update or insert: "Update if exists, insert if not"
- **Error Handling** = Failure recovery: "What to do when write fails"
- **Transactions** = All or nothing: "Either all rows written or none"

**Why Needed**: MarketLag writes lag signals to Supabase. Best practices ensure reliable writes and handle failures gracefully.

---

## Analogy

If you know **database best practices** (which you do), Flink JDBC sink practices are similar:
- Database UPSERT = Flink JDBC UPSERT (both update or insert)
- Database transactions = Flink JDBC transactions (both atomic)
- Database retry = Flink JDBC retry (both handle failures)

Key difference: Flink JDBC sink handles retries and transactions automatically in streaming context.

---

## Relationship to Already Learned Topics

- **5.1 JDBC Connector**: Best practices build on JDBC connector configuration
- **9.1 Dead Letter Queue**: DLQ for failed database writes
- **9.3 Exception Handling**: Error handling in Flink operators
- **Database Operations**: You already know database best practices

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// JDBC sink with retry (simplified)
class JdbcSink {
    void write(List<Row> batch) {
        int retries = 0;
        while (retries < maxRetries) {
            try {
                executeBatch(batch);
                return;  // Success
            } catch (Exception e) {
                retries++;
                if (retries >= maxRetries) {
                    sendToDLQ(batch);  // Dead letter queue
                } else {
                    Thread.sleep(exponentialBackoff(retries));
                }
            }
        }
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/sink/JdbcSink.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/sink/JdbcSink.java

---

## Idempotent Writes: UPSERT vs INSERT

### INSERT (Not Idempotent)

```sql
INSERT INTO lag_signals_history (market, window, signal_delta)
VALUES ('market-1', '2024-01-15 14:00:00', 1.5);
```

**Problem**: Fails on duplicate (primary key violation).

**Use Case**: When duplicates are impossible (e.g., unique constraints).

### UPSERT (Idempotent)

```sql
INSERT INTO lag_signals_history (market, window, signal_delta)
VALUES ('market-1', '2024-01-15 14:00:00', 1.5)
ON CONFLICT (market, window)
DO UPDATE SET signal_delta = EXCLUDED.signal_delta;
```

**Benefit**: Safe to retry - updates if exists, inserts if not.

**MarketLag**: Should use UPSERT for idempotent writes.

### Flink JDBC UPSERT Configuration

```sql
CREATE TABLE lag_signals_sink (
    market VARCHAR(100),
    window TIMESTAMP,
    signal_delta DECIMAL(10, 4),
    PRIMARY KEY (market, window) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://...',
    'table-name' = 'lag_signals_history',
    'sink.buffer-flush.max-rows' = '100',
    'sink.buffer-flush.interval' = '10s'
);
```

**Note**: Flink JDBC sink doesn't directly support UPSERT syntax. Use database-specific UPSERT or handle in application logic.

---

## Error Handling: Retry Logic, Dead Letter Handling

### Retry Logic

Flink JDBC sink supports retry configuration:

```sql
'sink.max-retries' = '3'  -- Max retry attempts
```

**Behavior**: Retries failed writes up to max-retries times.

### Exponential Backoff

Implement exponential backoff in custom error handler:

```java
long backoffDelay = Math.min(1000 * (1L << retryCount), 60000);  // Max 60s
Thread.sleep(backoffDelay);
```

**Benefit**: Reduces load on database during failures.

### Dead Letter Queue

Send failed records to DLQ after max retries:

```java
if (retries >= maxRetries) {
    sendToDLQ(failedRecords);  // Dead letter queue (section 9.1)
}
```

**MarketLag**: Should implement DLQ for failed database writes.

---

## Transaction Management

### Automatic Transactions

Flink JDBC sink uses transactions automatically:
- **Batch Writes**: Each batch is a transaction
- **Atomicity**: All rows in batch written or none
- **Consistency**: Database remains consistent

### Transaction Configuration

```sql
'sink.buffer-flush.max-rows' = '100'  -- Batch size (transaction size)
```

**Effect**: Each batch of 100 rows is one transaction.

**MarketLag**: Uses batch size 100, so each batch is one transaction.

---

## Handling Connection Failures: Retry with Exponential Backoff

### Connection Pool Configuration

Flink JDBC sink manages connection pool:
- **Automatic Retry**: Retries on connection failures
- **Connection Pooling**: Reuses connections efficiently
- **Timeout Handling**: Handles connection timeouts

### Exponential Backoff Pattern

```java
int retryCount = 0;
while (retryCount < maxRetries) {
    try {
        writeToDatabase(batch);
        break;  // Success
    } catch (ConnectionException e) {
        retryCount++;
        long delay = Math.min(1000 * (1L << retryCount), 60000);
        Thread.sleep(delay);
    }
}
```

**Benefit**: Reduces load during database outages.

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class JdbcSinkBestPracticesDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        // Create JDBC sink with best practices
        tableEnv.executeSql(
            "CREATE TABLE lag_signals_sink (" +
            "  market VARCHAR(100)," +
            "  window TIMESTAMP," +
            "  signal_delta DECIMAL(10, 4)," +
            "  PRIMARY KEY (market, window) NOT ENFORCED" +
            ") WITH (" +
            "  'connector' = 'jdbc'," +
            "  'url' = 'jdbc:postgresql://...'," +
            "  'table-name' = 'lag_signals_history'," +
            "  'username' = 'postgres'," +
            "  'password' = 'password'," +
            "  'sink.buffer-flush.max-rows' = '100'," +
            "  'sink.buffer-flush.interval' = '10s'," +
            "  'sink.max-retries' = '3'" +
            ")"
        );

        // Write with UPSERT (handled in database or application)
        tableEnv.executeSql(
            "INSERT INTO lag_signals_sink " +
            "SELECT market, window, signal_delta FROM processed_data"
        );

        env.execute("JDBC Sink Best Practices Demo");
    }
}
```

---

## Common Mistakes

1. **Not using UPSERT**:
   - ❌ Using INSERT, fails on duplicates
   - ✅ Use UPSERT for idempotent writes

2. **No error handling**:
   - ❌ Job fails on database errors
   - ✅ Configure retry, implement DLQ

3. **Too large batch size**:
   - ❌ Batch size 10000 (large transactions, slow)
   - ✅ Use reasonable batch size (100-1000)

4. **No connection retry**:
   - ❌ Job fails on connection errors
   - ✅ Configure connection retry with exponential backoff

5. **Not monitoring failures**:
   - ❌ Not tracking failed writes
   - ✅ Monitor DLQ, set up alerts

---

## Mind Trigger: When to Think About This

Think about JDBC sink best practices when:
- **Writing to database**: MarketLag writes lag signals to Supabase
- **Ensuring reliability**: Idempotent writes, error handling
- **Handling failures**: Retry logic, DLQ (section 9.1)
- **Performance optimization**: Batch size, transaction management
- **Production deployment**: Best practices essential for production

**In MarketLag project**: Writes lag signals to Supabase PostgreSQL. Should use UPSERT for idempotent writes, configure retry logic, and implement DLQ for failed writes. Understanding best practices ensures reliable database writes.

---

## Summary

Flink JDBC sink best practices include idempotent writes (UPSERT), error handling (retry, DLQ), transaction management (batch writes), and connection failure handling (exponential backoff). MarketLag should use UPSERT for idempotent writes, configure retry logic, and implement DLQ. Understanding best practices ensures reliable database writes in production.

