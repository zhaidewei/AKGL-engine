# Flink Checkpoint Testing

**Learning Point**: 10.3 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.10 (Checkpoints), 10.1 (Local Testing), understanding of fault tolerance
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink Checkpoint Testing**: Testing that Flink checkpoints are created correctly and that state recovery from checkpoints works as expected.

**Testing Checkpoint Creation**: Verifying that checkpoints are successfully created at configured intervals.

**Simulating Failures**: Artificially causing failures (killing tasks, network partitions) to test recovery.

**Verifying State Recovery Correctness**: Ensuring that after recovery, state is correctly restored and processing continues from the correct point.

---

## Plain Language Explanation

Think of checkpoint testing like testing a backup system:

- **Checkpoint Creation** = Backup creation: "Does the backup get created?"
- **Simulating Failures** = Simulating disaster: "What if the system crashes?"
- **State Recovery** = Restore from backup: "Can we restore everything correctly?"

**Why Needed**: MarketLag needs to ensure fault tolerance works - if a job fails, it can recover from checkpoints without data loss.

---

## Analogy

If you know **database backup testing** (which you do), checkpoint testing is similar:
- Database backup = Flink checkpoint (both create snapshots)
- Backup restore = Checkpoint recovery (both restore from snapshot)
- Backup testing = Checkpoint testing (both verify backup/restore works)

Key difference: Flink checkpoints are continuous and automatic.

---

## Relationship to Already Learned Topics

- **1.10 Checkpoints**: Understanding checkpoint mechanism
- **10.1 Local Testing**: Checkpoint testing uses local environment
- **Fault Tolerance**: You already know fault tolerance - this tests it

---

## Testing Checkpoint Creation and Recovery

### Test Checkpoint Creation

```java
@Test
public void testCheckpointCreation() throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();

    // Enable checkpoints
    env.enableCheckpointing(1000);  // 1 second for testing
    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
    env.getCheckpointConfig().setCheckpointStorage("file:///tmp/checkpoints");

    // Create stateful stream
    DataStream<Event> stream = env.fromElements(...)
        .keyBy(Event::getKey)
        .process(new StatefulProcessFunction());

    // Execute
    JobExecutionResult result = env.execute("Checkpoint Test");

    // Verify checkpoint was created
    File checkpointDir = new File("/tmp/checkpoints");
    assertTrue(checkpointDir.exists());
    assertTrue(checkpointDir.listFiles().length > 0);
}
```

**MarketLag**: Tests checkpoint creation with 5-minute interval.

---

## Simulating Failures: Killing Tasks, Network Partitions

### Simulating Task Failure

```java
@Test
public void testTaskFailureRecovery() throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
    env.enableCheckpointing(1000);

    // Create job with state
    DataStream<Event> stream = env.fromElements(...)
        .keyBy(Event::getKey)
        .process(new StatefulProcessFunction());

    // Start job in separate thread
    Thread jobThread = new Thread(() -> {
        try {
            env.execute("Failure Test");
        } catch (Exception e) {
            // Expected when we kill the job
        }
    });
    jobThread.start();

    // Wait for checkpoint
    Thread.sleep(2000);

    // Simulate failure: stop the job
    jobThread.interrupt();

    // Restart and verify recovery
    StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createLocalEnvironment();
    env2.enableCheckpointing(1000);
    // Restore from checkpoint
    // Verify state is correct
}
```

**MarketLag**: Tests recovery after simulated failures.

---

## Verifying State Recovery Correctness

### State Verification

```java
@Test
public void testStateRecovery() throws Exception {
    // First run: create checkpoint with state
    StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createLocalEnvironment();
    env1.enableCheckpointing(1000);
    env1.getCheckpointConfig().setCheckpointStorage("file:///tmp/checkpoints");

    DataStream<Event> stream1 = env1.fromElements(
        new Event("market1", 1.0),
        new Event("market1", 2.0)
    );

    stream1.keyBy(Event::getMarket)
        .process(new StatefulProcessFunction())
        .addSink(collectResults());

    env1.execute("First Run");

    // Second run: restore from checkpoint
    StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createLocalEnvironment();
    env2.enableCheckpointing(1000);
    env2.getCheckpointConfig().setCheckpointStorage("file:///tmp/checkpoints");

    // Restore from checkpoint
    // Verify state is correctly restored
    // Continue processing from checkpoint
}
```

**MarketLag**: Verifies that max_signal_delta state is correctly restored.

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.CheckpointingMode;
import org.junit.Test;
import java.io.File;
import static org.junit.Assert.assertTrue;

public class CheckpointTestingDemo {
    @Test
    public void testCheckpointAndRecovery() throws Exception {
        String checkpointPath = "file:///tmp/test-checkpoints";

        // First run: create checkpoint
        StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createLocalEnvironment();
        env1.enableCheckpointing(1000);
        env1.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        env1.getCheckpointConfig().setCheckpointStorage(checkpointPath);

        DataStream<Event> stream1 = env1.fromElements(
            new Event("market1", 1.0, 1000L),
            new Event("market1", 2.0, 2000L)
        );

        stream1.keyBy(Event::getMarket)
            .process(new StatefulProcessFunction())
            .print();

        env1.execute("Checkpoint Creation");

        // Verify checkpoint exists
        File checkpointDir = new File("/tmp/test-checkpoints");
        assertTrue("Checkpoint directory exists", checkpointDir.exists());

        // Second run: test recovery
        StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createLocalEnvironment();
        env2.enableCheckpointing(1000);
        env2.getCheckpointConfig().setCheckpointStorage(checkpointPath);

        // Restore and verify state
        // (Implementation depends on Flink version)
    }
}
```

---

## Common Mistakes

1. **Not waiting for checkpoint**:
   - ❌ Checking checkpoint immediately (not created yet)
   - ✅ Wait for checkpoint interval to pass

2. **Not verifying state**:
   - ❌ Only checking checkpoint exists, not state correctness
   - ✅ Verify state values are correctly restored

3. **Not cleaning up**:
   - ❌ Test checkpoints persist, interfere with next test
   - ✅ Clean up checkpoint directory after tests

4. **Wrong checkpoint path**:
   - ❌ Using relative path (may not work)
   - ✅ Use absolute path or file:// URI

5. **Not testing recovery**:
   - ❌ Only testing checkpoint creation
   - ✅ Test both creation and recovery

---

## Mind Trigger: When to Think About This

Think about checkpoint testing when:
- **Testing fault tolerance**: MarketLag needs reliable checkpoint recovery
- **Before production**: Verify checkpoints work before deployment
- **State verification**: Verify state is correctly restored
- **Failure scenarios**: Test recovery after failures
- **Checkpoint configuration**: Section 1.10 covers checkpoint configuration

**In MarketLag project**: Tests checkpoint creation and recovery. Verifies that state (max_signal_delta) is correctly restored after failures. Understanding checkpoint testing is essential for fault tolerance validation.

---

## Summary

Flink checkpoint testing verifies checkpoint creation and state recovery. Test checkpoint creation at configured intervals. Simulate failures (task kills, network partitions) to test recovery. Verify state recovery correctness. MarketLag tests checkpoints to ensure fault tolerance. Understanding checkpoint testing is essential for production reliability.

