# Lambda-Kafka Integration

**Learning Point**: 8.3 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 8.1 (AWS Lambda), 2.1 (Kafka Connector), Kafka producer knowledge
**Version**: Python 3.9+, kafka-python, confluent-kafka

---

## Definition (Engineering Language)

**Lambda-Kafka Integration**: Connecting AWS Lambda functions to Kafka (Confluent Cloud) to produce messages. Lambda functions act as Kafka producers.

**Kafka Producer in Lambda**: Using kafka-python or confluent-kafka libraries in Lambda to create Kafka producers and send messages.

**Confluent Cloud Authentication**: SASL_SSL authentication with API key and secret for secure connection to Confluent Cloud.

**Error Handling and Retry Logic**: Handling Kafka producer errors, implementing retry logic for transient failures.

**Dead Letter Queue**: Kafka topics for storing failed records that couldn't be sent to primary topics.

---

## Plain Language Explanation

Think of Lambda-Kafka integration like a delivery service:

- **Lambda** = Delivery person: "Fetch data and deliver to Kafka"
- **Kafka Producer** = Delivery truck: "Vehicle to send messages"
- **Authentication** = ID card: "Prove you're allowed to deliver"
- **Dead Letter Queue** = Failed deliveries box: "Store deliveries that failed"

**Why Needed**: MarketLag Lambda functions send RSS events and Polymarket prices to Kafka topics.

---

## Analogy

If you know **Kafka producers** (which you do), Lambda-Kafka integration is similar:
- Kafka producer = Lambda with Kafka library (both send messages)
- Producer configuration = Lambda environment variables (both configure connection)
- Error handling = Both need retry logic

Key difference: Lambda runs in serverless environment, needs to handle cold starts.

---

## Relationship to Already Learned Topics

- **8.1 AWS Lambda**: Lambda functions contain Kafka producer code
- **2.1 Kafka Connector**: Understanding Kafka producer configuration
- **9.1 Dead Letter Queue**: DLQ for failed Kafka messages
- **Kafka**: You already know Kafka - Lambda integrates with it

---

## Kafka Producer in Lambda: confluent-kafka, kafka-python

### Option 1: confluent-kafka (Recommended)

**Pros**:
- **Official Confluent Library**: Optimized for Confluent Cloud
- **Better Performance**: More efficient
- **Better Error Handling**: More robust

**Cons**:
- **Larger Size**: Larger library (use Lambda layers)

**MarketLag**: Should use confluent-kafka for Confluent Cloud.

### Option 2: kafka-python

**Pros**:
- **Pure Python**: Easier to use
- **Smaller Size**: Smaller library

**Cons**:
- **Less Optimized**: Not optimized for Confluent Cloud
- **Performance**: May be slower

**MarketLag**: Can use kafka-python as alternative.

---

## Confluent Cloud Authentication

### Configuration

```python
from confluent_kafka import Producer

# Confluent Cloud authentication
config = {
    'bootstrap.servers': '<confluent-cloud-endpoint>',
    'security.protocol': 'SASL_SSL',
    'sasl.mechanism': 'PLAIN',
    'sasl.username': '<api-key>',
    'sasl.password': '<api-secret>',
    'acks': 'all',  # Wait for all replicas
    'retries': 3,  # Retry on failure
    'enable.idempotence': True  # Exactly-once semantics
}

producer = Producer(config)
```

**MarketLag**: Uses this configuration for Confluent Cloud authentication.

### Environment Variables

```python
import os

config = {
    'bootstrap.servers': os.environ['KAFKA_ENDPOINT'],
    'sasl.username': os.environ['KAFKA_API_KEY'],
    'sasl.password': os.environ['KAFKA_API_SECRET'],
    # ... other config
}
```

**MarketLag**: Uses environment variables for security (don't hardcode credentials).

---

## Error Handling and Retry Logic in Lambda

### Basic Error Handling

```python
from confluent_kafka import Producer, KafkaError
import json

def send_to_kafka(topic, messages):
    """
    Send messages to Kafka with error handling.

    Args:
        topic: Kafka topic name
        messages: List of messages to send
    """
    producer = Producer(config)

    def delivery_callback(err, msg):
        """Callback for message delivery."""
        if err:
            print(f"Message delivery failed: {err}")
            # Handle error (retry, DLQ, etc.)
        else:
            print(f"Message delivered: {msg.topic()}[{msg.partition()}]")

    # Send messages
    for message in messages:
        producer.produce(
            topic,
            value=json.dumps(message).encode('utf-8'),
            callback=delivery_callback
        )

    # Wait for all messages to be delivered
    producer.flush(timeout=30)
    producer.close()
```

### Retry Logic

```python
import time

def send_with_retry(topic, messages, max_retries=3):
    """
    Send messages with retry logic.
    """
    for attempt in range(max_retries):
        try:
            send_to_kafka(topic, messages)
            return  # Success
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                time.sleep(wait_time)
            else:
                # Max retries reached, send to DLQ
                send_to_dlq(messages)
                raise
```

**MarketLag**: Implements retry logic for transient failures.

---

## Dead Letter Queue: Failed Records Handling

### DLQ Concept

**Purpose**: Store records that failed to be sent to primary topic after max retries.

**Implementation**: Kafka topic (e.g., `dlq.rss.events`, `dlq.polymarket.price_hourly`)

**MarketLag**: Uses DLQ topics for failed RSS and Polymarket records.

### DLQ Implementation

```python
def send_to_dlq(dlq_topic, failed_messages):
    """
    Send failed messages to dead letter queue.

    Args:
        dlq_topic: DLQ topic name
        failed_messages: List of failed messages
    """
    dlq_producer = Producer(config)

    for message in failed_messages:
        dlq_producer.produce(
            dlq_topic,
            value=json.dumps(message).encode('utf-8')
        )

    dlq_producer.flush()
    dlq_producer.close()
```

**MarketLag**: Sends failed records to DLQ topics for later analysis and reprocessing.

---

## Minimum Viable Code

```python
import os
import json
from confluent_kafka import Producer, KafkaError

# Configuration from environment variables
KAFKA_ENDPOINT = os.environ['KAFKA_ENDPOINT']
KAFKA_API_KEY = os.environ['KAFKA_API_KEY']
KAFKA_API_SECRET = os.environ['KAFKA_API_SECRET']
TOPIC = os.environ['KAFKA_TOPIC']
DLQ_TOPIC = os.environ.get('DLQ_TOPIC', f'dlq.{TOPIC}')

def lambda_handler(event, context):
    """
    Lambda handler that sends data to Kafka.
    """
    # Kafka producer configuration
    config = {
        'bootstrap.servers': KAFKA_ENDPOINT,
        'security.protocol': 'SASL_SSL',
        'sasl.mechanism': 'PLAIN',
        'sasl.username': KAFKA_API_KEY,
        'sasl.password': KAFKA_API_SECRET,
        'acks': 'all',
        'retries': 3,
        'enable.idempotence': True
    }

    producer = Producer(config)
    failed_messages = []

    def delivery_callback(err, msg):
        if err:
            print(f"Delivery failed: {err}")
            failed_messages.append(msg.value())
        else:
            print(f"Delivered: {msg.topic()}[{msg.partition()}]")

    # Fetch data
    data = fetch_data()

    # Send to Kafka
    for item in data:
        producer.produce(
            TOPIC,
            value=json.dumps(item).encode('utf-8'),
            callback=delivery_callback
        )

    # Wait for delivery
    producer.flush(timeout=30)
    producer.close()

    # Send failed messages to DLQ
    if failed_messages:
        send_to_dlq(DLQ_TOPIC, failed_messages)

    return {
        'statusCode': 200,
        'body': json.dumps({
            'sent': len(data) - len(failed_messages),
            'failed': len(failed_messages)
        })
    }
```

---

## Common Mistakes

1. **Not closing producer**:
   - ❌ Producer not closed, connections leak
   - ✅ Always call `producer.close()` or use context manager

2. **Not flushing**:
   - ❌ Messages not flushed, may not be sent
   - ✅ Always call `producer.flush()` before closing

3. **Wrong authentication**:
   - ❌ Missing SASL_SSL configuration
   - ✅ Always configure SASL_SSL for Confluent Cloud

4. **No error handling**:
   - ❌ No retry logic, messages lost on failure
   - ✅ Implement retry logic and DLQ

5. **Hardcoding credentials**:
   - ❌ API keys in code (security risk)
   - ✅ Use environment variables for credentials

---

## Mind Trigger: When to Think About This

Think about Lambda-Kafka integration when:
- **Implementing producers**: MarketLag Lambda functions send to Kafka
- **Configuring authentication**: Confluent Cloud requires SASL_SSL
- **Error handling**: Implement retry logic and DLQ (section 9.1)
- **Producer libraries**: Choose confluent-kafka or kafka-python
- **Lambda layers**: Package Kafka libraries in Lambda layers

**In MarketLag project**: Lambda functions use confluent-kafka to send RSS events and Polymarket prices to Kafka. Use SASL_SSL authentication, retry logic, and DLQ for failed messages. Understanding Lambda-Kafka integration is essential for data producer implementation.

---

## Summary

Lambda-Kafka integration enables Lambda functions to produce messages to Kafka. Use confluent-kafka or kafka-python libraries. Confluent Cloud requires SASL_SSL authentication with API key/secret. Implement error handling, retry logic, and dead letter queues for failed messages. MarketLag uses this pattern for RSS and Polymarket producers. Understanding Lambda-Kafka integration is essential for data producer implementation.

