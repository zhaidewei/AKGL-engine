# RSS Feed Processing

**Learning Point**: 7.1 from knowledge_and_skills_needed_v2.md
**Prerequisites**: Understanding of RSS feeds, Python basics, timezone handling
**Version**: Python 3.9+, RSS.app API

---

## Definition (Engineering Language)

**RSS Feed Processing**: The process of fetching, parsing, and processing RSS (Really Simple Syndication) feeds to extract structured data (title, link, published_at, etc.) for downstream processing.

**RSS Feed Structure**: XML-based format containing items with metadata (title, link, description, published_at, guid).

**RSS Parsing Libraries**: Python libraries for parsing RSS feeds: feedparser, rss-parser, BeautifulSoup.

**RSS Item Deduplication**: Strategies to avoid processing duplicate items: using guid (globally unique identifier), link, or content hash.

**Timezone Handling**: Converting published_at timestamps from source timezone to UTC for consistent processing.

---

## Plain Language Explanation

Think of RSS feed processing like reading a newspaper:

- **RSS Feed** = Newspaper: Contains articles (items) with headlines, links, dates
- **Parsing** = Reading articles: Extract information from each article
- **Deduplication** = Skip duplicates: "I already read this article"
- **Timezone Conversion** = Standardize time: "Convert all times to UTC"

**Why Needed**: MarketLag ingests RSS feeds from RSS.app to detect news events that may affect markets.

---

## Analogy

If you know **web scraping** (which you do), RSS feed processing is similar:
- Web scraping = RSS parsing (both extract structured data)
- HTML parsing = RSS XML parsing (both parse structured formats)
- Deduplication = Both need to avoid processing duplicates

Key difference: RSS is standardized format, easier to parse than HTML.

---

## Relationship to Already Learned Topics

- **1.4 Time Concepts**: Timezone handling converts to UTC
- **8.1 AWS Lambda**: RSS processing runs in Lambda functions
- **8.2 EventBridge**: Scheduled triggers for RSS polling
- **Python**: You already know Python - RSS parsing uses Python

---

## RSS Feed Structure: Items, Title, Link, published_at

### RSS XML Structure

```xml
<rss>
  <channel>
    <item>
      <title>Fed raises interest rates</title>
      <link>https://example.com/article</link>
      <pubDate>Mon, 15 Jan 2024 14:30:00 EST</pubDate>
      <guid>unique-article-id</guid>
      <description>Article content...</description>
    </item>
  </channel>
</rss>
```

### Key Fields

- **title**: Article headline
- **link**: Article URL
- **pubDate**: Publication timestamp (timezone-aware)
- **guid**: Globally unique identifier (for deduplication)
- **description**: Article summary/content

**MarketLag**: Extracts these fields for processing.

---

## RSS Parsing Libraries: feedparser (Python), rss-parser

### feedparser (Python - MarketLag Uses This)

```python
import feedparser

# Parse RSS feed
feed = feedparser.parse('https://rss.app/feeds/v1.1/{feed_id}.json')

# Access items
for entry in feed.entries:
    title = entry.title
    link = entry.link
    published = entry.published  # String, needs parsing
    guid = entry.get('id', entry.link)  # Use guid or link as fallback
```

**MarketLag**: Uses feedparser in Lambda function to parse RSS feeds.

### rss-parser (Alternative)

```python
from rss_parser import Parser

parser = Parser(xml=feed_xml)
feed = parser.parse()

for item in feed.feed:
    title = item.title
    link = item.link
    published = item.pub_date
```

---

## RSS Item Deduplication Strategies: guid, link, hash

### Strategy 1: GUID (Globally Unique Identifier)

```python
seen_guids = set()

for entry in feed.entries:
    guid = entry.get('id', entry.link)
    if guid not in seen_guids:
        seen_guids.add(guid)
        process_item(entry)
```

**Pros**: GUID is designed for uniqueness
**Cons**: Not all feeds have GUID

### Strategy 2: Link

```python
seen_links = set()

for entry in feed.entries:
    link = entry.link
    if link not in seen_links:
        seen_links.add(link)
        process_item(entry)
```

**Pros**: Links are usually unique
**Cons**: Same article may have different URLs

### Strategy 3: Content Hash

```python
import hashlib

seen_hashes = set()

for entry in feed.entries:
    content = f"{entry.title}{entry.description}"
    content_hash = hashlib.md5(content.encode()).hexdigest()
    if content_hash not in seen_hashes:
        seen_hashes.add(content_hash)
        process_item(entry)
```

**Pros**: Catches duplicate content even with different URLs
**Cons**: More computation, may have false positives

**MarketLag**: Uses guid or link for deduplication (stored in DynamoDB or in-memory for Lambda).

---

## Timezone Handling: Converting published_at to UTC

### Problem

RSS feeds may use different timezones:
- EST (Eastern Standard Time)
- PST (Pacific Standard Time)
- UTC
- Local timezone

**Need**: Convert all to UTC for consistent processing.

### Solution

```python
from datetime import datetime
import pytz

def parse_rss_date(date_string):
    """
    Parse RSS date string and convert to UTC.

    Args:
        date_string: RSS pubDate string (e.g., "Mon, 15 Jan 2024 14:30:00 EST")

    Returns:
        datetime: UTC datetime object
    """
    # Parse date string (feedparser handles this)
    dt = feedparser._parse_date(date_string)

    # If timezone-aware, convert to UTC
    if dt.tzinfo:
        utc_dt = dt.astimezone(pytz.UTC)
    else:
        # Assume UTC if no timezone
        utc_dt = dt.replace(tzinfo=pytz.UTC)

    return utc_dt

# Usage
published_str = entry.published
published_utc = parse_rss_date(published_str)
```

**MarketLag**: Converts all published_at to UTC before sending to Kafka.

---

## RSS.app API: `https://rss.app/feeds/v1.1/{feed_id}.json`

### API Endpoint

**Format**: `https://rss.app/feeds/v1.1/{feed_id}.json`

**Example**: `https://rss.app/feeds/v1.1/x6KH5aqpKp2jqVNE.json`

**Method**: GET

**Authentication**: None required (public feed URL)

**Response**: JSON format (not XML)

### MarketLag Usage

```python
import requests
import feedparser

feed_id = "x6KH5aqpKp2jqVNE"
url = f"https://rss.app/feeds/v1.1/{feed_id}.json"

response = requests.get(url)
feed_data = response.json()

# Process feed items
for item in feed_data.get('items', []):
    title = item.get('title')
    link = item.get('link')
    published_at = item.get('published_at')  # ISO 8601 format
    # Process item...
```

**MarketLag**: Polls RSS.app API every 15 minutes via EventBridge (section 8.2).

---

## Minimum Viable Code

```python
import feedparser
from datetime import datetime
import pytz
import hashlib

def process_rss_feed(feed_url):
    """
    Process RSS feed: parse, deduplicate, convert timezone.

    Args:
        feed_url: RSS feed URL

    Returns:
        list: Processed RSS items
    """
    # Parse feed
    feed = feedparser.parse(feed_url)

    seen_guids = set()
    processed_items = []

    for entry in feed.entries:
        # Deduplication: use guid or link
        guid = entry.get('id', entry.link)
        if guid in seen_guids:
            continue
        seen_guids.add(guid)

        # Extract fields
        title = entry.title
        link = entry.link
        published_str = entry.published

        # Parse and convert to UTC
        published_dt = feedparser._parse_date(published_str)
        if published_dt.tzinfo:
            published_utc = published_dt.astimezone(pytz.UTC)
        else:
            published_utc = published_dt.replace(tzinfo=pytz.UTC)

        # Create item
        item = {
            'title': title,
            'link': link,
            'published_at': published_utc.isoformat(),  # ISO 8601
            'guid': guid
        }

        processed_items.append(item)

    return processed_items

# Usage
feed_url = "https://rss.app/feeds/v1.1/x6KH5aqpKp2jqVNE.json"
items = process_rss_feed(feed_url)
```

---

## Common Mistakes

1. **Not handling timezones**:
   - ❌ Using local timezone, causing misalignment
   - ✅ Always convert to UTC (section 1.4)

2. **Not deduplicating**:
   - ❌ Processing same article multiple times
   - ✅ Use guid or link for deduplication

3. **Not handling missing fields**:
   - ❌ Assuming all fields exist
   - ✅ Use `.get()` with defaults: `entry.get('guid', entry.link)`

4. **Not handling API errors**:
   - ❌ No error handling for RSS.app API failures
   - ✅ Implement retry logic, error handling (section 9.1)

5. **Polling too frequently**:
   - ❌ Polling every minute (rate limiting)
   - ✅ Poll every 15 minutes (MarketLag pattern, section 8.2)

---

## Mind Trigger: When to Think About This

Think about RSS feed processing when:
- **Ingesting news data**: MarketLag ingests RSS feeds for lag detection
- **Parsing feeds**: Use feedparser or similar library
- **Deduplication**: Avoid processing duplicate articles
- **Timezone conversion**: Convert all timestamps to UTC (section 1.4)
- **Lambda implementation**: Section 8.1 covers Lambda function structure

**In MarketLag project**: RSS producer (Lambda) polls RSS.app API every 15 minutes, parses feeds, deduplicates items, converts timestamps to UTC, and sends to Kafka. Understanding RSS processing is essential for data ingestion.

---

## Summary

RSS feed processing involves fetching, parsing, and processing RSS feeds to extract structured data. RSS feeds contain items with title, link, published_at, guid. Use feedparser (Python) for parsing. Deduplicate using guid or link. Convert published_at to UTC for consistent processing. MarketLag uses RSS.app API to fetch feeds. Understanding RSS processing is essential for news data ingestion.

