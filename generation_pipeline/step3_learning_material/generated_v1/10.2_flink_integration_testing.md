# Flink Integration Testing

**Learning Point**: 10.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 10.1 (Local Testing), understanding of Docker, integration testing
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink Integration Testing**: Testing Flink applications with real external systems (Kafka, PostgreSQL, Schema Registry) in an integrated environment.

**Docker Compose Setup**: Using Docker Compose to orchestrate multiple services (Kafka, PostgreSQL, Schema Registry) for integration testing.

**End-to-End Testing**: Testing complete data pipeline from source (Kafka) through Flink processing to sink (PostgreSQL).

**Test Data Replay**: Using historical data to replay scenarios and verify correctness.

---

## Plain Language Explanation

Think of integration testing like a full dress rehearsal:

- **Docker Compose** = Stage setup: "Set up all systems (Kafka, database) for testing"
- **End-to-End Testing** = Full rehearsal: "Test the entire pipeline from start to finish"
- **Test Data Replay** = Rehearsal script: "Use real data to test"

**Why Needed**: MarketLag needs to test the complete pipeline with real Kafka and PostgreSQL before production.

---

## Analogy

If you know **integration testing** (which you do), Flink integration testing is similar:
- Integration tests = Flink integration tests (both test with real systems)
- Test environment = Docker Compose (both set up test infrastructure)
- End-to-end tests = Full pipeline tests (both test complete flow)

Key difference: Flink integration tests use streaming systems (Kafka) instead of batch systems.

---

## Relationship to Already Learned Topics

- **10.1 Local Testing**: Integration testing extends local testing with real systems
- **2.2 Kafka Table Connector**: Tests with real Kafka
- **5.1 JDBC Connector**: Tests with real PostgreSQL
- **Docker**: You already know Docker - used for test infrastructure

---

## Docker Compose Setup: Kafka, PostgreSQL, Schema Registry

### docker-compose.yml

```yaml
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  postgres:
    image: postgres:14
    environment:
      POSTGRES_DB: testdb
      POSTGRES_USER: testuser
      POSTGRES_PASSWORD: testpass
    ports:
      - "5432:5432"

  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    depends_on:
      - kafka
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
    ports:
      - "8081:8081"
```

**MarketLag**: Uses Docker Compose for integration testing environment.

---

## End-to-End Testing: Source → Flink → Sink

### Test Flow

1. **Setup**: Start Docker Compose services
2. **Produce Test Data**: Send test data to Kafka topics
3. **Run Flink Job**: Execute Flink job locally
4. **Verify Sink**: Check PostgreSQL for results
5. **Cleanup**: Stop services

### Implementation

```java
@Test
public void testEndToEnd() throws Exception {
    // 1. Start Docker Compose (via testcontainers or manual)
    // 2. Produce test data to Kafka
    produceTestDataToKafka("rss.events", testRssEvents);
    produceTestDataToKafka("polymarket.price_hourly", testPrices);

    // 3. Run Flink job
    StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
    // Configure with test Kafka and PostgreSQL
    env.getConfig().setGlobalJobParameters(createTestConfig());
    // Run job
    env.execute("Integration Test");

    // 4. Verify results in PostgreSQL
    List<Result> results = queryPostgreSQL("SELECT * FROM lag_signals_history");
    assertEquals(expectedCount, results.size());
    // Verify specific results
    verifyResults(results);
}
```

**MarketLag**: Tests complete pipeline from Kafka → Flink → PostgreSQL.

---

## Test Data Replay: Using Historical Data

### Replay Pattern

**Concept**: Use real historical data to test with realistic scenarios.

**Implementation**:
1. Store historical data (Kafka messages, API responses)
2. Replay in test environment
3. Verify Flink job produces expected results

```java
@Test
public void testWithHistoricalData() throws Exception {
    // Load historical data
    List<Event> historicalEvents = loadHistoricalData("rss_events_2024-01-15.json");

    // Replay to Kafka
    for (Event event : historicalEvents) {
        produceToKafka("rss.events", event);
    }

    // Run Flink job
    env.execute("Historical Replay Test");

    // Verify against known results
    verifyAgainstExpectedResults();
}
```

**MarketLag**: Can replay historical RSS events and prices for testing.

---

## Minimum Viable Code

```java
import org.testcontainers.containers.KafkaContainer;
import org.testcontainers.containers.PostgreSQLContainer;
import org.testcontainers.junit.jupiter.Testcontainers;

@Testcontainers
public class FlinkIntegrationTest {
    static KafkaContainer kafka = new KafkaContainer();
    static PostgreSQLContainer postgres = new PostgreSQLContainer("postgres:14");

    @BeforeAll
    static void setup() {
        kafka.start();
        postgres.start();
    }

    @Test
    public void testIntegration() throws Exception {
        // Configure Flink with test containers
        String kafkaBootstrap = kafka.getBootstrapServers();
        String jdbcUrl = postgres.getJdbcUrl();

        // Create test data
        List<Event> testEvents = createTestEvents();

        // Produce to Kafka
        produceToKafka(kafkaBootstrap, "rss.events", testEvents);

        // Run Flink job
        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
        // Configure with test containers
        runFlinkJob(env, kafkaBootstrap, jdbcUrl);

        // Verify in PostgreSQL
        List<Result> results = queryPostgreSQL(jdbcUrl, "SELECT * FROM results");
        assertResults(results);
    }
}
```

---

## Common Mistakes

1. **Not using testcontainers**:
   - ❌ Manual Docker setup (tedious, error-prone)
   - ✅ Use testcontainers for automated setup

2. **Not cleaning up**:
   - ❌ Test data persists between tests
   - ✅ Clean up test data after each test

3. **Not waiting for processing**:
   - ❌ Checking results before Flink finishes
   - ✅ Wait for Flink job to complete before verifying

4. **Using production configs**:
   - ❌ Accidentally connecting to production systems
   - ✅ Use test-specific configurations

5. **Not testing error scenarios**:
   - ❌ Only testing happy path
   - ✅ Test error scenarios (missing data, malformed data)

---

## Mind Trigger: When to Think About This

Think about Flink integration testing when:
- **Testing complete pipeline**: MarketLag tests end-to-end pipeline
- **Before deployment**: Integration tests validate before production
- **Debugging issues**: Integration tests help reproduce issues
- **Data replay**: Test with historical data
- **Local testing**: Section 10.1 covers local testing, integration extends it

**In MarketLag project**: Tests complete pipeline with Docker Compose (Kafka, PostgreSQL). Tests end-to-end from Kafka → Flink → PostgreSQL. Can replay historical data for testing. Understanding integration testing is essential for validating complete pipeline.

---

## Summary

Flink integration testing tests Flink applications with real external systems. Use Docker Compose to set up Kafka, PostgreSQL, Schema Registry. Test end-to-end pipeline from source to sink. Replay historical data for realistic testing. MarketLag uses integration testing to validate complete pipeline before deployment. Understanding integration testing is essential for production readiness.

