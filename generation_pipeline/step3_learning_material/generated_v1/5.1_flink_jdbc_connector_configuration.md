# Flink JDBC Connector Configuration

**Learning Point**: 5.1 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.3 (Table API/SQL), understanding of JDBC/PostgreSQL
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink JDBC Connector**: A Flink table connector that enables reading from and writing to relational databases using JDBC. Supports PostgreSQL, MySQL, and other JDBC-compatible databases.

**JDBC Sink**: Writing Flink results to a database table via JDBC. Supports batch inserts for performance.

**JDBC Table Connector**: Using CREATE TABLE syntax to define database tables as Flink tables, enabling SQL queries against databases.

**Batch Insert Configuration**: Configuring batch size and flush interval for efficient database writes.

**Connection Pool Management**: Managing database connections efficiently to avoid connection exhaustion.

---

## Plain Language Explanation

Think of JDBC connector like a database bridge:

- **JDBC Connector** = Database bridge: Flink can read/write to PostgreSQL
- **JDBC Sink** = Writing bridge: Write Flink results to database
- **Batch Inserts** = Bulk writes: Write multiple rows at once for efficiency

**Why Needed**: MarketLag writes lag signals to Supabase PostgreSQL. JDBC connector enables Flink to write results directly to database.

---

## Analogy

If you know **Spark's JDBC connector** (which you do), Flink's is similar:
- Spark `df.write.jdbc(...)` = Flink JDBC sink
- Both write DataFrames/Tables to databases

Key difference: Flink JDBC connector is integrated with Flink SQL, making it easier to use.

---

## Relationship to Already Learned Topics

- **1.3 Table API/SQL**: JDBC connector uses CREATE TABLE syntax
- **PostgreSQL**: You already know PostgreSQL - Flink connects to it via JDBC
- **Supabase**: MarketLag uses Supabase (managed PostgreSQL)

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// JDBC sink (simplified)
class JdbcTableSink implements TableSink {
    String url;
    String tableName;
    int batchSize;

    void write(TableData data) {
        // Batch insert to database
        List<Row> batch = new ArrayList<>();
        for (Row row : data) {
            batch.add(row);
            if (batch.size() >= batchSize) {
                insertBatch(batch);
                batch.clear();
            }
        }
        if (!batch.isEmpty()) {
            insertBatch(batch);
        }
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSink.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSink.java

---

## JDBC Sink Configuration

### Basic JDBC Sink

```sql
CREATE TABLE lag_signals_sink (
    market VARCHAR(100),
    window TIMESTAMP,
    signal_delta DECIMAL(10, 4),
    price_delta DECIMAL(10, 4),
    lag_flag BOOLEAN,
    confidence DECIMAL(3, 2),
    detected_at TIMESTAMP
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://localhost:5432/postgres',
    'table-name' = 'lag_signals_history',
    'username' = 'postgres',
    'password' = '<password>'
);
```

### Supabase Connection String Format

**Format**:
```
jdbc:postgresql://db.<project-ref>.supabase.co:5432/postgres
```

**Example**:
```
jdbc:postgresql://db.abcdefghijklmnop.supabase.co:5432/postgres
```

**MarketLag Project**: Uses Supabase connection string format.

---

## JDBC Table Connector for Reading/Writing

### Reading from Database

```sql
CREATE TABLE source_table (
    id BIGINT,
    value DOUBLE
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://localhost:5432/postgres',
    'table-name' = 'source_table',
    'username' = 'postgres',
    'password' = '<password>'
);

SELECT * FROM source_table;
```

### Writing to Database

```sql
-- Create sink table
CREATE TABLE sink_table (
    id BIGINT,
    value DOUBLE
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://localhost:5432/postgres',
    'table-name' = 'sink_table',
    'username' = 'postgres',
    'password' = '<password>'
);

-- Write results
INSERT INTO sink_table
SELECT id, value FROM processed_data;
```

---

## Batch Insert Configuration: Batch Size, Flush Interval

### Batch Size Configuration

```sql
CREATE TABLE lag_signals_sink (
    -- schema
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://...',
    'table-name' = 'lag_signals_history',
    'username' = 'postgres',
    'password' = '<password>',
    'sink.buffer-flush.max-rows' = '100'  -- Batch size
);
```

**Effect**: Flink buffers up to 100 rows before writing to database.

### Flush Interval Configuration

```sql
'sink.buffer-flush.interval' = '10s'  -- Flush every 10 seconds
```

**Effect**: Flink writes buffered rows every 10 seconds, even if batch not full.

**MarketLag Project**: Uses batch size 100 and flush interval 10s.

---

## Connection Pool Management

### Connection Pool Configuration

```sql
'sink.connection.max-retry-time' = '60s'  -- Max retry time
```

**Effect**: Flink manages connection pool automatically. Retries on connection failures.

### Best Practices

1. **Batch Inserts**: Use batch size to reduce database load
2. **Flush Interval**: Balance latency vs batch size
3. **Connection Retry**: Configure retry for reliability
4. **Connection Limits**: Be aware of database connection limits

---

## MarketLag Project Example

```sql
CREATE TABLE lag_signals_sink (
    market VARCHAR(100),
    window TIMESTAMP,
    signal_delta DECIMAL(10, 4),
    price_delta DECIMAL(10, 4),
    lag_flag BOOLEAN,
    confidence DECIMAL(3, 2),
    detected_at TIMESTAMP
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://db.xxx.supabase.co:5432/postgres',
    'table-name' = 'lag_signals_history',
    'username' = 'postgres',
    'password' = '<password>',
    'sink.buffer-flush.max-rows' = '100',
    'sink.buffer-flush.interval' = '10s'
);
```

**This is the exact pattern used in MarketLag Job 3**.

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class JdbcConnectorDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        // Create JDBC sink table
        tableEnv.executeSql(
            "CREATE TABLE jdbc_sink (" +
            "  id BIGINT," +
            "  value DOUBLE" +
            ") WITH (" +
            "  'connector' = 'jdbc'," +
            "  'url' = 'jdbc:postgresql://localhost:5432/postgres'," +
            "  'table-name' = 'results'," +
            "  'username' = 'postgres'," +
            "  'password' = 'password'," +
            "  'sink.buffer-flush.max-rows' = '100'," +
            "  'sink.buffer-flush.interval' = '10s'" +
            ")"
        );

        // Write to JDBC
        tableEnv.executeSql(
            "INSERT INTO jdbc_sink " +
            "SELECT id, value FROM source_table"
        );

        env.execute("JDBC Connector Demo");
    }
}
```

---

## Common Mistakes

1. **Wrong connection string**:
   - ❌ Using wrong Supabase connection string format
   - ✅ Use: `jdbc:postgresql://db.<project-ref>.supabase.co:5432/postgres`

2. **Missing batch configuration**:
   - ❌ Not configuring batch size (inefficient)
   - ✅ Set: `sink.buffer-flush.max-rows` and `sink.buffer-flush.interval`

3. **Connection pool exhaustion**:
   - ❌ Too many concurrent writes
   - ✅ Monitor connection pool, adjust parallelism

4. **Schema mismatch**:
   - ❌ Table schema doesn't match database table
   - ✅ Ensure schema matches database table structure

5. **Not handling errors**:
   - ❌ Job fails on database errors
   - ✅ Configure retry, use dead letter queue (section 9.1)

---

## Mind Trigger: When to Think About This

Think about JDBC connector when:
- **Writing results to database**: MarketLag writes lag signals to Supabase
- **Configuring batch inserts**: Optimize database write performance
- **Supabase integration**: MarketLag uses Supabase PostgreSQL
- **Connection management**: Understanding connection pools helps avoid issues
- **Error handling**: Section 5.2 covers best practices

**In MarketLag project**: Job 3 writes lag signals to Supabase PostgreSQL via JDBC sink. Uses batch size 100 and flush interval 10s. Understanding JDBC connector is essential for writing results to database.

---

## Summary

Flink JDBC connector enables reading from and writing to relational databases using JDBC. JDBC sink writes Flink results to database tables. Batch insert configuration (batch size, flush interval) optimizes write performance. Connection pool management handles database connections efficiently. MarketLag uses JDBC connector to write lag signals to Supabase PostgreSQL. Understanding JDBC connector is essential for database integration in streaming applications.

