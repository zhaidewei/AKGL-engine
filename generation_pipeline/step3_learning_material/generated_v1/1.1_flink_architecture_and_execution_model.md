# Flink Architecture and Execution Model

**Learning Point**: 1.1 from knowledge_and_skills_needed_v2.md
**Prerequisites**: Understanding of distributed systems, Kafka basics (you already know this from your experience)
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink Cluster Architecture**: A distributed system consisting of a JobManager (coordinator) and one or more TaskManagers (workers). The JobManager manages job lifecycle, scheduling, and checkpoint coordination. TaskManagers execute tasks in parallel slots, where each slot can run one parallel instance of an operator.

**Execution Model**: Flink uses a dataflow graph model where operators are connected in a directed acyclic graph (DAG). The graph is partitioned into parallel subtasks, each running in a slot on a TaskManager. Data flows through the graph via data streams between operators.

---

## Plain Language Explanation

Think of Flink like a restaurant kitchen:

- **JobManager** = Head Chef: Coordinates the entire operation, decides what tasks go where, monitors progress, and handles emergencies (failures)
- **TaskManager** = Kitchen Stations: Multiple workstations (slots) where actual cooking (data processing) happens
- **Slots** = Individual cooking stations: Each can handle one dish (one parallel task) at a time
- **Job** = A complete recipe: The entire data processing pipeline from source to sink

When you submit a job, the JobManager breaks it down into tasks and assigns them to available slots across TaskManagers. If a TaskManager fails, the JobManager reassigns its tasks to other TaskManagers.

---

## Analogy

If you're familiar with **Apache Spark** (which you know), Flink is similar but optimized for streaming:
- Spark's Driver = Flink's JobManager
- Spark's Executors = Flink's TaskManagers
- Spark's Cores = Flink's Slots

The key difference: Spark processes data in batches (micro-batches in streaming), while Flink processes records one-by-one in true streaming fashion.

---

## Relationship to Already Learned Topics

- **Kafka**: Flink reads from Kafka topics (you know Kafka from your experience). Kafka partitions map to Flink parallelism - each Kafka partition can be processed by one Flink subtask.
- **Distributed Systems**: Similar to your experience with Spark clusters - JobManager coordinates like Spark Driver, TaskManagers execute like Spark Executors.
- **Event-Driven Architecture**: Flink is event-driven - each record triggers processing immediately, unlike batch systems that wait for data to accumulate.

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code structure:

```java
// JobManager (simplified)
class JobManager {
    JobGraph jobGraph;  // The dataflow graph
    List<TaskManager> taskManagers;  // Available workers

    void submitJob(JobGraph graph) {
        // 1. Parse job graph into execution graph
        ExecutionGraph execGraph = createExecutionGraph(graph);

        // 2. Schedule tasks to TaskManagers
        for (ExecutionVertex vertex : execGraph.getVertices()) {
            TaskManager tm = selectTaskManager();
            Slot slot = tm.allocateSlot();
            scheduleTask(vertex, slot);
        }

        // 3. Deploy tasks
        deployTasks();
    }
}

// TaskManager (simplified)
class TaskManager {
    List<Slot> slots;  // Available processing slots
    Map<ExecutionAttemptID, Task> runningTasks;

    void deployTask(TaskDeploymentDescriptor descriptor) {
        Slot slot = allocateSlot();
        Task task = createTask(descriptor);
        slot.setTask(task);
        task.start();
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java

**Key Classes**:
- `JobManagerRunner`: Main entry point for JobManager
- `TaskManagerRunner`: Main entry point for TaskManager
- `ExecutionGraph`: Represents the execution plan

---

## Execution Environments

Flink provides three execution environments (from project design v5):

1. **LocalEnvironment**: Runs Flink in a single JVM (for testing)
   ```java
   StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
   // Automatically detects local mode
   ```

2. **RemoteEnvironment**: Connects to a remote Flink cluster
   ```java
   StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(
       "jobmanager-host", 1234
   );
   ```

3. **Confluent Cloud Environment**: Managed Flink (used in MarketLag project)
   - No code changes needed - just deploy JAR/SQL to Confluent Cloud
   - Environment is pre-configured with RocksDB state backend

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;

public class FlinkArchitectureDemo {
    public static void main(String[] args) throws Exception {
        // Creates execution environment (auto-detects local vs cluster)
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // Set parallelism (number of parallel subtasks per operator)
        env.setParallelism(2);  // Each operator will have 2 parallel instances

        // Create a simple data stream
        DataStream<String> stream = env.fromElements("Hello", "Flink", "World");

        // Transform data (creates operator in the DAG)
        DataStream<String> upper = stream.map(s -> s.toUpperCase());

        // Print (sink operator)
        upper.print();

        // Execute the job
        // JobManager will:
        // 1. Create execution graph
        // 2. Schedule tasks to TaskManager slots
        // 3. Deploy and run
        env.execute("Flink Architecture Demo");
    }
}
```

**Run locally**:
```bash
# Compile (assuming Maven project)
mvn clean package

# Run
java -cp target/your-jar.jar FlinkArchitectureDemo
```

---

## Parallelism and Task Distribution

**Parallelism** = Number of parallel instances of an operator

Example:
- Source operator with parallelism=4 → 4 subtasks, each reading from different Kafka partitions
- Map operator with parallelism=4 → 4 subtasks processing in parallel
- Sink operator with parallelism=2 → 2 subtasks writing in parallel

**Task Distribution**:
- JobManager tries to distribute tasks evenly across TaskManagers
- If you have 4 TaskManagers with 2 slots each = 8 total slots
- A job with parallelism=8 will use all slots
- A job with parallelism=4 will use 4 slots (one per TaskManager ideally)

---

## Common Mistakes

1. **Confusing parallelism with Kafka partitions**:
   - ❌ Setting parallelism higher than Kafka partitions (wasteful)
   - ✅ Match parallelism to Kafka partition count for optimal throughput

2. **Not understanding slot sharing**:
   - ❌ Thinking each operator needs its own slot
   - ✅ Multiple operators can share a slot (operator chaining)

3. **Ignoring TaskManager resources**:
   - ❌ Not monitoring TaskManager memory/CPU
   - ✅ Monitor TaskManager metrics to avoid OOM

4. **Local vs Cluster confusion**:
   - ❌ Using LocalEnvironment in production
   - ✅ Use RemoteEnvironment or Confluent Cloud for production

---

## Mind Trigger: When to Think About This

Think about Flink architecture when:
- **Deploying a job**: Understanding how JobManager schedules tasks helps debug deployment issues
- **Performance tuning**: Knowing slot distribution helps optimize parallelism settings
- **Troubleshooting failures**: JobManager logs show scheduling decisions; TaskManager logs show execution issues
- **Scaling**: Adding TaskManagers increases available slots, allowing higher parallelism
- **Resource planning**: Each slot consumes memory/CPU - plan TaskManager resources accordingly

**In MarketLag project**: Confluent Cloud manages JobManager and TaskManagers automatically, but you still need to set parallelism=2 (as per project design) to match your 2 CFU allocation.

---

## Summary

Flink's architecture separates coordination (JobManager) from execution (TaskManagers). Jobs are broken into parallel tasks distributed across slots. Understanding this helps with deployment, performance tuning, and troubleshooting. The MarketLag project uses Confluent Cloud's managed Flink, which handles infrastructure but you still configure parallelism and monitor execution.

