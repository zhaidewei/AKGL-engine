# Flink Time Concepts and Timezone Handling

**Learning Point**: 1.4 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.3 (Table API/SQL), understanding of timestamps and timezones (you already know this from data engineering)
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Processing Time**: The wall-clock time when an event is processed by Flink operators. Determined by the system clock of the machine processing the event.

**Event Time**: The timestamp when an event actually occurred in the real world, extracted from the event data itself. Independent of processing time.

**Ingestion Time**: A hybrid approach - timestamp assigned when Flink source operator receives the event, before any processing.

**UTC Standardization**: Converting all timestamps to Coordinated Universal Time (UTC) to avoid timezone ambiguity. Critical for distributed systems processing data from multiple timezones.

**Event Time Alignment**: Ensuring event timestamps are aligned to specific boundaries (e.g., hour boundaries) for consistent windowing across distributed processing.

---

## Plain Language Explanation

Think of time in Flink like a package delivery system:

- **Processing Time** = When the delivery truck scans the package (when Flink processes it)
- **Event Time** = When the package was actually shipped (when the event happened in real world)
- **Ingestion Time** = When the package arrived at the sorting facility (when Flink received it)

**Why Event Time Matters**: If a package was shipped yesterday but processed today, you want to group it with yesterday's packages, not today's. Event time ensures correct chronological ordering even when processing is delayed.

**UTC Standardization**: Like using a single timezone for all warehouses - avoids confusion when packages come from different timezones. Everyone uses UTC, then converts to local time only for display.

---

## Analogy

If you know **Spark Structured Streaming** (which you do), Flink's time concepts are similar:
- Spark's processing time = Flink processing time
- Spark's event time = Flink event time
- Spark's watermark = Flink watermark (covered in section 1.5)

The key difference: Flink's event-time processing is more mature and lower-latency than Spark's micro-batch approach.

---

## Relationship to Already Learned Topics

- **1.3 Table API/SQL**: Time attributes are declared in CREATE TABLE statements
- **1.5 Watermarks**: Event time requires watermarks to handle late data
- **1.6 Windows**: Windows use event time to assign events to correct time windows
- **Timezone Handling**: You already know timezone handling from your data engineering experience - Flink applies the same principles

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// Time extraction (simplified)
class TimestampExtractor {
    long extractEventTime(Record record) {
        // Extract timestamp from record field
        return record.getField("event_time");
    }

    long getProcessingTime() {
        // System wall-clock time
        return System.currentTimeMillis();
    }

    long getIngestionTime() {
        // Assigned when source receives record
        return System.currentTimeMillis();  // At ingestion moment
    }
}

// Time alignment (simplified)
class TimeAligner {
    long alignToHour(long timestamp) {
        // Align to UTC hour boundary
        return (timestamp / 3600000) * 3600000;  // Round down to hour
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/AssignerWithPeriodicWatermarks.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/AssignerWithPeriodicWatermarks.java

**Key Classes**:
- `TimestampAssigner`: Assigns timestamps to events
- `WatermarkGenerator`: Generates watermarks (covered in section 1.5)

---

## Processing Time vs Event Time vs Ingestion Time

### Processing Time
- **Definition**: System clock when operator processes event
- **Pros**: Simple, no timestamp extraction needed, low latency
- **Cons**: Not deterministic, results vary with processing speed
- **Use Case**: When exact event time doesn't matter (e.g., simple aggregations)

```java
// Processing time (automatic)
DataStream<Event> stream = env.fromElements(...);
stream.map(...);  // Uses processing time automatically
```

### Event Time
- **Definition**: Timestamp from event data itself
- **Pros**: Deterministic, handles out-of-order data correctly
- **Cons**: Requires timestamp extraction, needs watermarks
- **Use Case**: When chronological correctness matters (MarketLag project uses this)

```java
// Event time (explicit)
stream.assignTimestampsAndWatermarks(
    WatermarkStrategy
        .forBoundedOutOfOrderness(Duration.ofMinutes(5))
        .withTimestampAssigner((event, timestamp) -> event.getPublishedAt())
);
```

### Ingestion Time
- **Definition**: Timestamp assigned when source receives event
- **Pros**: Automatic, handles some out-of-order
- **Cons**: Less accurate than event time
- **Use Case**: When event time unavailable but need better than processing time

```java
// Ingestion time
stream.assignTimestampsAndWatermarks(
    WatermarkStrategy.forMonotonousTimestamps()
        .withTimestampAssigner((event, timestamp) -> System.currentTimeMillis())
);
```

---

## UTC Standardization

### Why UTC?

1. **Distributed Systems**: Multiple machines in different timezones
2. **Data Sources**: RSS feeds, Polymarket API may use different timezones
3. **Consistency**: Single source of truth for timestamps
4. **Window Alignment**: Windows align to UTC boundaries (e.g., UTC hour 0, 1, 2...)

### Timezone Conversion (Local → UTC)

```java
// Example: Convert local timezone to UTC
String localTime = "2024-01-15 14:30:00 EST";  // Eastern Standard Time
ZonedDateTime local = ZonedDateTime.parse(localTime,
    DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss z"));
ZonedDateTime utc = local.withZoneSameInstant(ZoneId.of("UTC"));
long utcTimestamp = utc.toInstant().toEpochMilli();
```

### ISO 8601 Format

Standard format for timestamps: `2024-01-15T14:30:00Z` (Z = UTC)

```java
// Parse ISO 8601
Instant instant = Instant.parse("2024-01-15T14:30:00Z");
long timestamp = instant.toEpochMilli();
```

---

## Event Time Extraction from Records

### From JSON (MarketLag Project Pattern)

```json
{
  "title": "Fed raises rates",
  "published_at": "2024-01-15T14:30:00Z",
  "source": "Reuters"
}
```

```java
// Extract event time from JSON field
public class EventTimeExtractor implements TimestampAssigner<JsonNode> {
    @Override
    public long extractTimestamp(JsonNode element, long recordTimestamp) {
        String publishedAt = element.get("published_at").asText();
        return Instant.parse(publishedAt).toEpochMilli();
    }
}
```

### In Flink SQL

```sql
CREATE TABLE rss_events (
    title STRING,
    published_at TIMESTAMP(3),  -- Event time column
    source STRING,
    WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE
) WITH (...);
```

---

## Event Time Alignment

### Hour Alignment

Events must align to UTC hour boundaries for consistent windowing:

```java
// Align timestamp to UTC hour boundary
public long alignToHour(long timestamp) {
    // Convert to UTC
    Instant instant = Instant.ofEpochMilli(timestamp);
    ZonedDateTime utc = instant.atZone(ZoneId.of("UTC"));

    // Round down to hour
    ZonedDateTime aligned = utc.withMinute(0).withSecond(0).withNano(0);
    return aligned.toInstant().toEpochMilli();
}
```

### Window Alignment

In MarketLag project, windows align to UTC hours:
- Window 1: 2024-01-15 00:00:00 UTC to 01:00:00 UTC
- Window 2: 2024-01-15 01:00:00 UTC to 02:00:00 UTC
- etc.

This ensures all events for the same hour are grouped together, regardless of source timezone.

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;
import org.apache.flink.streaming.api.watermark.Watermark;
import java.time.Instant;
import java.time.ZoneId;
import java.time.ZonedDateTime;

public class TimeConceptsDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // Create events with timestamps
        DataStream<Event> events = env.fromElements(
            new Event("event1", "2024-01-15T14:30:00Z"),
            new Event("event2", "2024-01-15T14:45:00Z"),
            new Event("event3", "2024-01-15T15:00:00Z")
        );

        // Assign event time and watermarks
        DataStream<Event> withTimestamps = events.assignTimestampsAndWatermarks(
            WatermarkStrategy
                .<Event>forBoundedOutOfOrderness(Duration.ofMinutes(5))
                .withTimestampAssigner((event, timestamp) ->
                    Instant.parse(event.getTimestamp()).toEpochMilli()
                )
        );

        // Align to UTC hour
        DataStream<Event> aligned = withTimestamps.map(event -> {
            long utcTimestamp = Instant.parse(event.getTimestamp()).toEpochMilli();
            ZonedDateTime utc = Instant.ofEpochMilli(utcTimestamp)
                .atZone(ZoneId.of("UTC"));
            ZonedDateTime aligned = utc.withMinute(0).withSecond(0).withNano(0);
            event.setAlignedTimestamp(aligned.toInstant().toEpochMilli());
            return event;
        });

        aligned.print();
        env.execute("Time Concepts Demo");
    }
}
```

---

## Timestamp Assignment Strategies

### 1. From Record Field (Most Common)
```java
.withTimestampAssigner((event, timestamp) -> event.getEventTime())
```

### 2. From Metadata (Kafka)
```java
.withTimestampAssigner(
    KafkaTimestampAssigner.create()  // Uses Kafka record timestamp
)
```

### 3. Custom Extraction
```java
.withTimestampAssigner((event, timestamp) -> {
    // Custom logic to extract timestamp
    String timeStr = event.getField("time");
    return parseTimestamp(timeStr);
})
```

---

## Common Mistakes

1. **Mixing time types**:
   - ❌ Using processing time for event-time windows
   - ✅ Always use event time for windows that need chronological correctness

2. **Timezone confusion**:
   - ❌ Not converting to UTC, mixing timezones
   - ✅ Always convert to UTC at ingestion, store as UTC, convert to local only for display

3. **Timestamp precision**:
   - ❌ Using TIMESTAMP instead of TIMESTAMP(3) in SQL
   - ✅ Use TIMESTAMP(3) for millisecond precision (required for watermarks)

4. **Not aligning to boundaries**:
   - ❌ Events at 14:30 and 14:45 in different windows
   - ✅ Align all timestamps to hour boundaries for consistent windowing

5. **ISO 8601 parsing errors**:
   - ❌ Not handling timezone indicators (Z, +05:00)
   - ✅ Always parse with timezone awareness, convert to UTC

---

## Mind Trigger: When to Think About This

Think about Flink time concepts when:
- **Setting up event-time processing**: MarketLag project uses event time for all windows
- **Handling timezone data**: RSS feeds and Polymarket API may use different timezones
- **Configuring watermarks**: Section 1.5 requires event time to be set up first
- **Window alignment**: Section 1.6 windows need aligned timestamps
- **Debugging window issues**: Misaligned timestamps cause events in wrong windows

**In MarketLag project**: All timestamps are standardized to UTC, aligned to hour boundaries. RSS events and Polymarket prices both use UTC timestamps for consistent windowing.

---

## Summary

Flink supports three time concepts: processing time (system clock), event time (from data), and ingestion time (at source). MarketLag uses event time for chronological correctness. All timestamps are standardized to UTC and aligned to hour boundaries for consistent windowing. Understanding time concepts is prerequisite for watermarks (section 1.5) and windows (section 1.6).

