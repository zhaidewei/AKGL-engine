# Flink Table API and SQL

**Learning Point**: 1.3 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.2 (DataStream API), SQL knowledge (you already have this)
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink Table API**: A unified relational API for batch and streaming processing. Tables are the core abstraction, representing structured data with schema. The Table API provides a fluent DSL for table operations.

**Flink SQL**: SQL interface built on top of the Table API. Flink SQL extends standard SQL with streaming semantics (windows, watermarks, temporal functions).

**Relationship**: Flink SQL statements are parsed and converted to Table API operations, which are then compiled to DataStream/DataSet operations.

---

## Plain Language Explanation

Think of Flink Table API/SQL as a SQL database for streaming data:

- **Table** = A database table, but instead of static rows, it's a continuous stream of rows
- **Table API** = Programmatic way to query tables (like using a query builder library)
- **Flink SQL** = Writing SQL queries (like you do with PostgreSQL, but for streams)

The key difference from traditional SQL: tables are **unbounded** (infinite streams), so queries run continuously and produce results as new data arrives.

---

## Analogy

If you know **Spark SQL** (which you do from Databricks experience), Flink SQL is very similar:
- Spark DataFrame = Flink Table
- Spark SQL queries = Flink SQL queries
- Spark Structured Streaming = Flink SQL streaming

Main difference: Flink SQL is optimized for low-latency streaming, while Spark SQL focuses on micro-batch processing.

---

## Relationship to Already Learned Topics

- **1.2 DataStream API**: Table API/SQL compiles to DataStream operations under the hood
- **SQL**: Uses standard SQL syntax you already know (SELECT, FROM, WHERE, JOIN, GROUP BY)
- **Kafka**: Tables can be created from Kafka topics (see section 2.2)
- **PostgreSQL/Supabase**: Similar SQL syntax, but Flink SQL adds streaming extensions

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// Table (simplified)
class Table {
    TableEnvironment environment;
    TableSchema schema;
    RelNode logicalPlan;  // SQL logical plan

    Table select(String fields) {
        // Parse SQL and create new Table
        return environment.sqlQuery("SELECT " + fields + " FROM this");
    }
}

// TableEnvironment (simplified)
class StreamTableEnvironment {
    StreamExecutionEnvironment streamEnv;

    void createTemporaryView(String name, DataStream<?> stream) {
        // Convert DataStream to Table
        Table table = streamToTable(stream);
        registerTable(name, table);
    }

    TableResult executeSql(String sql) {
        // 1. Parse SQL
        SqlNode sqlNode = parser.parse(sql);

        // 2. Convert to logical plan
        RelNode relNode = sqlToRelConverter.convert(sqlNode);

        // 3. Optimize
        RelNode optimized = optimizer.optimize(relNode);

        // 4. Convert to DataStream
        DataStream<?> stream = relToDataStreamConverter.convert(optimized);

        // 5. Execute
        return streamEnv.execute();
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/Table.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/Table.java

**Key Classes**:
- `Table`: Main Table API class
- `StreamTableEnvironment`: Environment for streaming Table API/SQL
- `TableResult`: Result of executing a table query

---

## When to Use Table API vs DataStream API

### Use Table API/SQL when:
- ✅ You're comfortable with SQL
- ✅ Logic is mostly relational (joins, aggregations, filters)
- ✅ You want rapid development (SQL is concise)
- ✅ Team has SQL expertise but limited Java/Scala knowledge

### Use DataStream API when:
- ✅ You need fine-grained control (custom ProcessFunctions)
- ✅ Complex event processing (CEP patterns)
- ✅ Low-level state management
- ✅ Performance-critical custom operators

**MarketLag Project**: Uses Flink SQL for Jobs 1, 2, and 3 because the logic is primarily relational (window aggregations, joins).

---

## Creating Tables from DataStreams

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.table.api.Table;

StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

// Create DataStream
DataStream<Event> events = env.fromElements(
    new Event("user1", "click", 1000L),
    new Event("user2", "view", 2000L)
);

// Convert to Table
Table eventTable = tableEnv.fromDataStream(events);
// Or register as temporary view
tableEnv.createTemporaryView("events", events);
```

---

## Flink SQL Syntax

Flink SQL extends standard SQL with streaming concepts:

### Basic Query
```sql
SELECT user_id, COUNT(*) as click_count
FROM events
WHERE event_type = 'click'
GROUP BY user_id;
```

### Window Aggregation (Streaming-Specific)
```sql
SELECT
    user_id,
    TUMBLE_START(event_time, INTERVAL '1' HOUR) as window_start,
    COUNT(*) as click_count
FROM events
GROUP BY user_id, TUMBLE(event_time, INTERVAL '1' HOUR);
```

### Creating Tables from Kafka (see section 2.2 for details)
```sql
CREATE TABLE rss_events (
    title STRING,
    published_at TIMESTAMP(3),
    source STRING,
    WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json'
);
```

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class FlinkSQLDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        // Create a simple DataStream
        DataStream<String> words = env.fromElements(
            "hello", "world", "flink", "streaming", "hello", "flink"
        );

        // Register as temporary view
        tableEnv.createTemporaryView("words", words, $("word"));

        // Execute SQL query
        Table result = tableEnv.sqlQuery(
            "SELECT word, COUNT(*) as cnt " +
            "FROM words " +
            "GROUP BY word " +
            "ORDER BY cnt DESC"
        );

        // Convert back to DataStream and print
        tableEnv.toDataStream(result).print();

        env.execute("Flink SQL Demo");
    }
}
```

**Output**:
```
hello: 2
flink: 2
world: 1
streaming: 1
```

---

## Registering Tables and Views

### Temporary View (Session-scoped)
```java
tableEnv.createTemporaryView("my_table", dataStream);
// Can be used in SQL queries within this session
```

### Temporary Table (Session-scoped, can be modified)
```java
tableEnv.createTemporaryTable("my_table", tableDescriptor);
```

### Catalog Table (Persistent, stored in catalog)
```java
// Register in catalog (e.g., Hive, JDBC catalog)
tableEnv.createTable("catalog.db.my_table", tableDescriptor);
```

**MarketLag Project**: Uses temporary views created from Kafka table connectors (see section 2.2).

---

## Table API vs SQL Comparison

### Table API (Programmatic)
```java
Table result = tableEnv.from("events")
    .select($("user_id"), $("event_type"))
    .where($("event_type").isEqual("click"))
    .groupBy($("user_id"))
    .select($("user_id"), $("event_type").count().as("cnt"));
```

### Flink SQL (Declarative)
```sql
SELECT user_id, COUNT(*) as cnt
FROM events
WHERE event_type = 'click'
GROUP BY user_id;
```

**Both compile to the same execution plan**. SQL is more concise; Table API gives more compile-time type safety.

---

## Common Mistakes

1. **Mixing Table API and DataStream without conversion**:
   - ❌ Using DataStream methods on Table
   - ✅ Convert: `tableEnv.toDataStream(table)` or `tableEnv.fromDataStream(stream)`

2. **Forgetting WATERMARK declaration**:
   - ❌ Creating table without watermark for event-time windows
   - ✅ Always declare watermark: `WATERMARK FOR time_col AS time_col - INTERVAL '5' MINUTE`

3. **Not understanding table vs view**:
   - ❌ Creating table when view is sufficient
   - ✅ Use temporary view for read-only, table for writable sources

4. **SQL syntax errors with streaming**:
   - ❌ Using standard SQL without understanding streaming semantics
   - ✅ Learn Flink SQL extensions: windows, watermarks, temporal joins

5. **Type conversion issues**:
   - ❌ Assuming automatic type conversion works everywhere
   - ✅ Explicitly cast types: `CAST(col AS TIMESTAMP(3))`

---

## Mind Trigger: When to Think About This

Think about Flink Table API/SQL when:
- **Writing Flink jobs**: MarketLag project uses SQL for all three jobs
- **Creating Kafka table connectors**: Section 2.2 shows CREATE TABLE syntax
- **Window aggregations**: Section 3.1 covers TUMBLE window functions
- **Joining streams**: Section 3.3 covers equi-joins in SQL
- **Performance issues**: SQL queries compile to DataStream - understand the execution plan

**In MarketLag project**: All three jobs (RSS aggregation, price processing, lag detection) use Flink SQL. Understanding SQL is critical for this project.

---

## Summary

Flink Table API and SQL provide a relational interface for streaming data. Tables represent unbounded streams with schema. Flink SQL extends standard SQL with streaming concepts (windows, watermarks). The MarketLag project uses Flink SQL as the primary interface, making SQL knowledge essential. Table API/SQL compiles to DataStream operations, so understanding both helps with optimization and debugging.

