# Schema Registry Integration

**Learning Point**: 2.3 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 2.2 (Kafka Table Connector), understanding of schema evolution
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Schema Registry**: A centralized service for managing and versioning schemas for Kafka messages. Enables schema evolution and compatibility checking.

**Schema Evolution**: The process of updating schemas over time while maintaining backward/forward compatibility.

**Schema Compatibility**: Rules that determine if a new schema version is compatible with existing versions (BACKWARD, FORWARD, FULL).

**Avro Format with Schema Registry**: Using Avro serialization with schemas stored in Schema Registry for type safety and evolution.

**JSON Schema with Schema Registry**: Using JSON Schema format with schemas stored in Schema Registry.

---

## Plain Language Explanation

Think of Schema Registry like a version control system for data structures:

- **Schema Registry** = Version control: Stores all versions of your data structure
- **Schema Evolution** = Updating structure: Adding new fields without breaking old code
- **Compatibility** = Safety rules: Ensures new and old code can work together

**Why Needed**: As MarketLag project evolves, you might add new fields to RSS events or price data. Schema Registry ensures old and new consumers can coexist.

---

## Analogy

If you know **database migrations** (which you do), schema evolution is similar:
- Database migration = Schema evolution (adding columns)
- Backward compatibility = Old code works with new data
- Forward compatibility = New code works with old data

Key difference: Schema Registry handles this automatically for Kafka messages.

---

## Relationship to Already Learned Topics

- **2.2 Kafka Table Connector**: Schema Registry integrates with Kafka table connector
- **Kafka**: Schema Registry is a Confluent Cloud service alongside Kafka
- **Data Quality**: Schema validation ensures data quality (section 9.2)

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 and Schema Registry integration:

```java
// Schema Registry integration (simplified)
class SchemaRegistryDeserializer {
    SchemaRegistryClient client;
    String registryUrl;

    Object deserialize(byte[] data, String topic) {
        // 1. Get schema ID from message
        int schemaId = extractSchemaId(data);

        // 2. Fetch schema from registry
        Schema schema = client.getSchemaById(schemaId);

        // 3. Deserialize using schema
        return deserializeWithSchema(data, schema);
    }
}
```

---

## Source Code References

**Schema Registry**: Confluent Schema Registry (not in Flink source, but integrated)

**Flink Integration**: `generation_pipeline/step0_context/source_codes/flink/flink-formats/flink-format-avro/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentRegistryAvroDeserializationSchema.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-formats/flink-format-avro/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentRegistryAvroDeserializationSchema.java

---

## Schema Registry Concept and Purpose

### Why Schema Registry?

1. **Type Safety**: Ensures messages match expected structure
2. **Schema Evolution**: Add fields without breaking consumers
3. **Versioning**: Track schema changes over time
4. **Compatibility Checking**: Automatically validate schema compatibility

### How It Works

1. **Producer**: Registers schema, gets schema ID, embeds ID in message
2. **Consumer**: Reads schema ID from message, fetches schema from registry, deserializes

---

## Confluent Schema Registry Integration with Flink

### Avro Format with Schema Registry

```sql
CREATE TABLE rss_events (
    title STRING,
    published_at TIMESTAMP(3),
    source STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'format' = 'avro-confluent',
    'avro-confluent.schema-registry.url' = 'https://<schema-registry-endpoint>',
    'avro-confluent.schema-registry.subject' = 'rss-events-value',
    'properties.schema.registry.url' = 'https://<schema-registry-endpoint>',
    'properties.basic.auth.credentials.source' = 'USER_INFO',
    'properties.basic.auth.user.info' = '<api-key>:<api-secret>'
);
```

**MarketLag Project**: Can use Avro with Schema Registry for better type safety and evolution.

### JSON Schema with Schema Registry

```sql
CREATE TABLE rss_events (
    title STRING,
    published_at TIMESTAMP(3),
    source STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'format' = 'json',
    'json.schema.registry.url' = 'https://<schema-registry-endpoint>',
    'json.schema.registry.subject' = 'rss-events-value'
);
```

---

## Schema Evolution and Compatibility

### Compatibility Modes

1. **BACKWARD**: New schema can read data written with old schema
   - Old consumers can read new data
   - Use when: Adding optional fields

2. **FORWARD**: Old schema can read data written with new schema
   - New consumers can read old data
   - Use when: Removing optional fields

3. **FULL**: Both backward and forward compatible
   - Any version can read any version
   - Use when: Adding/removing optional fields

### Schema Evolution Example

**Version 1**:
```json
{
  "title": "string",
  "published_at": "timestamp"
}
```

**Version 2** (BACKWARD compatible - adding optional field):
```json
{
  "title": "string",
  "published_at": "timestamp",
  "source": "string"  // New optional field
}
```

Old consumers can still read Version 2 messages (ignore new field). New consumers can read both versions.

---

## Configuration: Adding Schema Registry URL and Credentials

### Confluent Cloud Schema Registry

```sql
CREATE TABLE rss_events (
    -- schema definition
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = '<confluent-cloud-endpoint>',
    'format' = 'avro-confluent',
    -- Schema Registry configuration
    'avro-confluent.schema-registry.url' = 'https://<schema-registry-endpoint>',
    'avro-confluent.schema-registry.subject' = 'rss-events-value',
    'properties.schema.registry.url' = 'https://<schema-registry-endpoint>',
    'properties.basic.auth.credentials.source' = 'USER_INFO',
    'properties.basic.auth.user.info' = '<api-key>:<api-secret>',
    -- Kafka authentication (still needed)
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'PLAIN',
    'properties.sasl.username' = '<api-key>',
    'properties.sasl.password' = '<api-secret>'
);
```

**MarketLag Project**: Uses Confluent Cloud Schema Registry for schema management.

---

## Minimum Viable Code

```java
// Using Avro with Schema Registry in DataStream API
import org.apache.flink.formats.avro.registry.confluent.ConfluentRegistryAvroDeserializationSchema;

KafkaSource<GenericRecord> source = KafkaSource.<GenericRecord>builder()
    .setBootstrapServers("localhost:9092")
    .setTopics("rss.events")
    .setStartingOffsets(OffsetsInitializer.latest())
    .setValueOnlyDeserializer(
        ConfluentRegistryAvroDeserializationSchema.forGeneric(
            "rss-events-value",
            "http://schema-registry:8081"
        )
    )
    .build();
```

---

## Common Mistakes

1. **Not configuring Schema Registry URL**:
   - ❌ Missing schema-registry.url in table definition
   - ✅ Always include Schema Registry URL and credentials

2. **Wrong subject name**:
   - ❌ Using topic name instead of subject name
   - ✅ Subject name is usually `<topic-name>-value` or `<topic-name>-key`

3. **Compatibility mode conflicts**:
   - ❌ Registering incompatible schema
   - ✅ Understand compatibility modes, test schema evolution

4. **Missing authentication**:
   - ❌ Schema Registry requires authentication in Confluent Cloud
   - ✅ Include basic.auth.credentials.source and basic.auth.user.info

5. **Not handling schema evolution**:
   - ❌ Breaking changes without compatibility
   - ✅ Use compatible evolution (add optional fields, don't remove required fields)

---

## Mind Trigger: When to Think About This

Think about Schema Registry when:
- **Using Avro format**: Avro benefits most from Schema Registry
- **Schema evolution**: Need to add/change fields over time
- **Type safety**: Want to ensure message structure matches expectations
- **Confluent Cloud**: Schema Registry is included in Confluent Cloud
- **Data quality**: Schema validation ensures data quality (section 9.2)

**In MarketLag project**: Confluent Cloud includes Schema Registry. While project currently uses JSON format, Schema Registry can be added for better type safety and schema evolution as the project grows.

---

## Summary

Schema Registry manages and versions schemas for Kafka messages, enabling schema evolution and compatibility checking. Confluent Cloud includes Schema Registry. Flink integrates with Schema Registry for Avro and JSON Schema formats. Understanding Schema Registry helps with schema evolution, type safety, and data quality as the MarketLag project grows.

