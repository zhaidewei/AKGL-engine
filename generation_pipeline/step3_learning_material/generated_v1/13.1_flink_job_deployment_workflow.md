# Flink Job Deployment Workflow

**Learning Point**: 13.1 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 6.3 (Deploying Flink Jobs), understanding of build tools (Maven/Gradle)
**Version**: Flink 2.2.0, Maven/Gradle

---

## Definition (Engineering Language)

**Flink Job Deployment Workflow**: The process of building, packaging, and deploying Flink applications to Confluent Cloud.

**Code Compilation**: Building Flink application code using Maven or Gradle build tools.

**JAR Packaging**: Creating a JAR file (fat JAR) that includes all dependencies for deployment.

**Job Submission**: Submitting the JAR to Confluent Cloud Flink environment via CLI, UI, or API.

**Job Configuration Management**: Managing environment-specific configurations (dev, staging, production).

---

## Plain Language Explanation

Think of Flink job deployment like shipping a package:

- **Code Compilation** = Packing items: "Build your code"
- **JAR Packaging** = Boxing everything: "Put everything in one box (fat JAR)"
- **Job Submission** = Shipping: "Send to Confluent Cloud"
- **Configuration** = Address label: "Different addresses for dev/prod"

**Why Needed**: MarketLag needs to deploy Flink jobs to Confluent Cloud for production.

---

## Analogy

If you know **application deployment** (which you do), Flink deployment is similar:
- Build process = Code compilation (both build code)
- Package creation = JAR packaging (both create deployable artifact)
- Deployment = Job submission (both deploy to environment)

Key difference: Flink jobs are streaming applications, deployed to Confluent Cloud.

---

## Relationship to Already Learned Topics

- **6.3 Deploying Flink Jobs**: Deployment to Confluent Cloud
- **Build Tools**: You already know Maven/Gradle - used for building
- **CI/CD**: You already know CI/CD - can automate deployment

---

## Code Compilation: Maven/Gradle Build

### Maven Build

**pom.xml**:
```xml
<project>
    <groupId>com.marketlag</groupId>
    <artifactId>marketlag-flink-jobs</artifactId>
    <version>1.0.0</version>

    <properties>
        <flink.version>2.2.0</flink.version>
        <scala.binary.version>2.12</scala.binary.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java-bridge</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <!-- Other dependencies -->
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.8.1</version>
                <configuration>
                    <source>11</source>
                    <target>11</target>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.4</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```

**Build Command**:
```bash
mvn clean package
```

**MarketLag**: Uses Maven to build Flink jobs.

### Gradle Build

**build.gradle**:
```gradle
plugins {
    id 'java'
}

repositories {
    mavenCentral()
}

dependencies {
    implementation "org.apache.flink:flink-streaming-java:2.2.0"
    implementation "org.apache.flink:flink-table-api-java-bridge:2.2.0"
    // Other dependencies
}

jar {
    archiveClassifier = 'all'
    from {
        configurations.runtimeClasspath.collect { it.isDirectory() ? it : zipTree(it) }
    }
}
```

**Build Command**:
```bash
./gradlew build
```

**MarketLag**: Can use Gradle as alternative to Maven.

---

## JAR Packaging: Including Dependencies (Fat JAR)

### Fat JAR Concept

**Problem**: Flink jobs need dependencies (Kafka connector, JDBC connector, etc.). Including them separately is complex.

**Solution**: Create fat JAR (uber JAR) that includes all dependencies.

**Maven Shade Plugin**: Creates fat JAR with all dependencies.

### Maven Shade Plugin Configuration

```xml
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>3.2.4</version>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <artifactSet>
                    <excludes>
                        <exclude>org.apache.flink:force-shading</exclude>
                    </excludes>
                </artifactSet>
                <filters>
                    <filter>
                        <artifact>*:*</artifact>
                        <excludes>
                            <exclude>META-INF/*.SF</exclude>
                            <exclude>META-INF/*.DSA</exclude>
                            <exclude>META-INF/*.RSA</exclude>
                        </excludes>
                    </filter>
                </filters>
            </configuration>
        </execution>
    </executions>
</plugin>
```

**Output**: `target/marketlag-flink-jobs-1.0.0-all.jar`

**MarketLag**: Creates fat JAR for deployment.

---

## Job Submission: CLI, UI, API

### CLI Submission

**Confluent Cloud CLI**:
```bash
confluent flink job create \
    --name job1-rss-signal-aggregation \
    --jar-file target/marketlag-flink-jobs-1.0.0-all.jar \
    --main-class com.marketlag.Job1RSSSignalAggregation \
    --compute-unit 2 \
    --parallelism 1
```

**MarketLag**: Can use CLI for deployment.

### UI Submission

**Confluent Cloud UI**:
1. Navigate to Flink environment
2. Click "Create Job"
3. Upload JAR file
4. Configure job (main class, parallelism, etc.)
5. Submit job

**MarketLag**: Uses UI for initial deployment and updates.

### API Submission

**Confluent Cloud API**:
```bash
curl -X POST https://api.confluent.cloud/flink/v1beta/environments/{env-id}/jobs \
    -H "Authorization: Bearer $TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
        "name": "job1-rss-signal-aggregation",
        "jar_file": "base64-encoded-jar",
        "main_class": "com.marketlag.Job1RSSSignalAggregation",
        "compute_unit": 2,
        "parallelism": 1
    }'
```

**MarketLag**: Can use API for automated deployment (CI/CD).

---

## Job Configuration Management: Environment-Specific Configs

### Configuration Files

**dev.properties**:
```properties
kafka.bootstrap.servers=dev-kafka:9092
postgres.jdbc.url=jdbc:postgresql://dev-db:5432/marketlag
checkpoint.interval=60000
```

**prod.properties**:
```properties
kafka.bootstrap.servers=prod-kafka:9092
postgres.jdbc.url=jdbc:postgresql://prod-db:5432/marketlag
checkpoint.interval=300000
```

### Loading Configuration

```java
public class JobConfig {
    public static Properties loadConfig(String env) {
        Properties props = new Properties();
        try (InputStream is = JobConfig.class.getResourceAsStream(env + ".properties")) {
            props.load(is);
        }
        return props;
    }
}

// Usage
Properties config = JobConfig.loadConfig(System.getenv("ENV"));
```

**MarketLag**: Uses environment-specific configurations for dev/prod.

---

## Minimum Viable Code

```bash
# Build JAR
mvn clean package

# Deploy to Confluent Cloud (CLI)
confluent flink job create \
    --name job1-rss-signal-aggregation \
    --jar-file target/marketlag-flink-jobs-1.0.0-all.jar \
    --main-class com.marketlag.Job1RSSSignalAggregation \
    --compute-unit 2 \
    --parallelism 1 \
    --checkpoint-interval 300000

# Or deploy via UI
# 1. Upload JAR in Confluent Cloud UI
# 2. Configure job settings
# 3. Submit job
```

---

## Common Mistakes

1. **Not creating fat JAR**:
   - ❌ JAR without dependencies (missing classes at runtime)
   - ✅ Use maven-shade-plugin or Gradle fat JAR

2. **Wrong main class**:
   - ❌ Main class not specified or wrong
   - ✅ Specify correct main class in job submission

3. **Not managing configurations**:
   - ❌ Hardcoding configs (can't change for different environments)
   - ✅ Use environment-specific configuration files

4. **Not testing JAR locally**:
   - ❌ Deploying untested JAR
   - ✅ Test JAR locally before deployment

5. **Not versioning JARs**:
   - ❌ Same JAR name for all versions (can't rollback)
   - ✅ Version JARs (e.g., `marketlag-jobs-1.0.0.jar`)

---

## Mind Trigger: When to Think About This

Think about Flink job deployment when:
- **Deploying to Confluent Cloud**: MarketLag deploys all three jobs
- **Building JARs**: Use Maven/Gradle to build fat JARs
- **Job submission**: Submit via CLI, UI, or API
- **Configuration management**: Use environment-specific configs
- **CI/CD**: Automate deployment workflow

**In MarketLag project**: Builds JARs with Maven, creates fat JARs with all dependencies, submits to Confluent Cloud via UI or CLI. Uses environment-specific configurations. Understanding deployment workflow is essential for production operations.

---

## Summary

Flink job deployment workflow involves building code (Maven/Gradle), packaging into fat JAR (with all dependencies), and submitting to Confluent Cloud (CLI, UI, or API). Manage environment-specific configurations. MarketLag uses this workflow to deploy all three jobs. Understanding deployment workflow is essential for production operations.

