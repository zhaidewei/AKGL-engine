# Flink Checkpoints

**Learning Point**: 1.10 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.8 (State Backend), understanding of fault tolerance
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Checkpoint**: A consistent snapshot of the entire Flink job state at a specific point in time. Checkpoints include operator state, keyed state, and source positions. Checkpoints enable Flink to recover from failures by restoring state to the last successful checkpoint.

**Checkpoint Interval**: The time between checkpoints. Shorter intervals provide faster recovery but higher overhead.

**Checkpoint Mode**: EXACTLY_ONCE (default) ensures each record is processed exactly once after recovery. AT_LEAST_ONCE allows duplicates but is faster.

**Checkpoint Storage**: Where checkpoint data is persisted (filesystem, S3, etc.). **Confluent Cloud uses S3-compatible storage**.

**Savepoint**: A manually triggered checkpoint used for planned stops, version upgrades, or state migration. Unlike checkpoints, savepoints survive job cancellation.

---

## Plain Language Explanation

Think of checkpoints like saving your game progress:

- **Checkpoint** = Save game: Snapshot of everything (state, position) at this moment
- **Checkpoint Interval** = How often to save: Every 5 minutes (MarketLag uses this)
- **Recovery** = Load game: Restore from last save if something goes wrong
- **Savepoint** = Manual save: You decide when to save (for upgrades, migrations)

**Why Needed**: If a TaskManager crashes, Flink can restore from the last checkpoint and continue processing from where it left off, ensuring no data loss.

---

## Analogy

If you know **database backups** (which you do), checkpoints are similar:
- Database backup = Flink checkpoint (snapshot of state)
- Backup frequency = Checkpoint interval (how often to backup)
- Restore from backup = Flink recovery (restore from checkpoint)

Key difference: Flink checkpoints are incremental and lightweight, taken automatically in the background.

---

## Relationship to Already Learned Topics

- **1.8 State Backend**: Checkpoints save state from state backend (RocksDB in MarketLag)
- **1.7 State Types**: All state types (ValueState, MapState) are included in checkpoints
- **Fault Tolerance**: Checkpoints enable fault tolerance - core Flink feature
- **S3 Storage**: You know S3 from AWS - Confluent Cloud uses S3 for checkpoint storage

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// Checkpoint (simplified)
class Checkpoint {
    long checkpointId;
    long timestamp;
    Map<OperatorID, OperatorState> operatorStates;
    Map<SourceID, SourcePosition> sourcePositions;

    void save() {
        // 1. Trigger checkpoint at all operators
        // 2. Each operator saves its state
        // 3. Save source positions (Kafka offsets)
        // 4. Write to checkpoint storage (S3)
    }
}

// Checkpoint coordinator (simplified)
class CheckpointCoordinator {
    void triggerCheckpoint(long interval) {
        while (jobRunning) {
            Thread.sleep(interval);
            Checkpoint checkpoint = createCheckpoint();
            checkpoint.save();
        }
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java

**Key Classes**:
- `CheckpointCoordinator`: Coordinates checkpoint creation
- `CompletedCheckpoint`: Represents a completed checkpoint
- `CheckpointStorage`: Interface for checkpoint storage

---

## Checkpoint Concept: Consistent Snapshots

### What Gets Checkpointed

1. **Operator State**: State from all operators (ValueState, MapState, etc.)
2. **Source Positions**: Kafka offsets, file positions, etc.
3. **Metadata**: Checkpoint ID, timestamp, job graph

### Consistency Guarantee

Checkpoints are **consistent** - all operators save state at the same logical point:
- All operators see the same set of input records
- State reflects processing up to that point
- No partial updates in checkpoint

**Example**: If checkpoint is taken after processing 1000 records, all operators' state reflects exactly those 1000 records.

---

## Checkpoint Configuration

### Checkpoint Interval

```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Enable checkpoints
env.enableCheckpointing(300000);  // 5 minutes (300,000 ms) - MarketLag uses this
```

**MarketLag Project**: Uses 5-minute checkpoint interval (300,000 milliseconds).

### Checkpoint Mode

```java
// EXACTLY_ONCE (default) - each record processed exactly once
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

// AT_LEAST_ONCE - faster, but may process records multiple times
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.AT_LEAST_ONCE);
```

**MarketLag Project**: Uses EXACTLY_ONCE mode for correctness.

### Checkpoint Timeout

```java
// How long to wait for checkpoint to complete
env.getCheckpointConfig().setCheckpointTimeout(600000);  // 10 minutes
```

If checkpoint doesn't complete within timeout, it's aborted.

---

## Checkpoint Storage: Filesystem, S3-Compatible

### Filesystem Storage (Local/Development)

```java
env.getCheckpointConfig().setCheckpointStorage("file:///path/to/checkpoints");
```

### S3-Compatible Storage (Confluent Cloud)

```java
// Confluent Cloud automatically configures S3 checkpoint storage
env.getCheckpointConfig().setCheckpointStorage("s3://bucket-name/checkpoints");
```

**Confluent Cloud**: Automatically uses S3-compatible storage for checkpoints. No manual configuration needed.

### Checkpoint Storage Configuration

```java
CheckpointStorage checkpointStorage = new FileSystemCheckpointStorage("s3://bucket/checkpoints");
env.getCheckpointConfig().setCheckpointStorage(checkpointStorage);
```

---

## Checkpoint Recovery: How Flink Restores from Checkpoints

### Recovery Process

1. **Job Restart**: Job is restarted (after failure or manual restart)
2. **Find Latest Checkpoint**: Flink finds the most recent successful checkpoint
3. **Restore State**: All operators restore their state from checkpoint
4. **Restore Source Positions**: Sources resume from saved positions (Kafka offsets)
5. **Continue Processing**: Job continues from checkpoint position

### Automatic Recovery

Flink automatically recovers from the latest checkpoint on:
- TaskManager failure
- JobManager failure (with high availability)
- Manual job restart

**No data loss**: Processing continues exactly from where it left off.

---

## Savepoints vs Checkpoints

### Checkpoints
- **Trigger**: Automatic (based on interval)
- **Purpose**: Fault tolerance, fast recovery
- **Retention**: Automatically cleaned up (keep last N)
- **Survival**: Lost when job is cancelled
- **Use Case**: Production fault tolerance

### Savepoints
- **Trigger**: Manual (via CLI or API)
- **Purpose**: Planned stops, version upgrades, state migration
- **Retention**: Manual cleanup
- **Survival**: Survive job cancellation
- **Use Case**: Version upgrades, A/B testing, state migration

### Creating Savepoints

```bash
# Create savepoint
flink savepoint <job-id> <savepoint-path>

# Stop job with savepoint
flink cancel -s <savepoint-path> <job-id>

# Restore from savepoint
flink run -s <savepoint-path> <jar-file>
```

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.CheckpointingMode;
import org.apache.flink.core.fs.Path;

public class CheckpointDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // Enable checkpoints with 5-minute interval (MarketLag pattern)
        env.enableCheckpointing(300000);  // 300,000 ms = 5 minutes

        // Configure checkpoint mode
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

        // Set checkpoint timeout
        env.getCheckpointConfig().setCheckpointTimeout(600000);  // 10 minutes

        // Configure checkpoint storage (S3 in Confluent Cloud)
        env.getCheckpointConfig().setCheckpointStorage("s3://my-bucket/checkpoints");

        // Keep only last 3 checkpoints
        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);
        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);

        // Your job logic
        DataStream<Event> stream = env.fromElements(...);
        stream.process(...);

        env.execute("Checkpoint Demo");
    }
}
```

---

## Checkpoint Configuration in Confluent Cloud

Confluent Cloud automatically configures checkpoints:

- **Checkpoint Interval**: Configurable (MarketLag uses 5 minutes)
- **Checkpoint Mode**: EXACTLY_ONCE
- **Checkpoint Storage**: S3-compatible (automatic)
- **Checkpoint Retention**: Automatic cleanup

**MarketLag Configuration**:
- Checkpoint interval: 300,000 ms (5 minutes)
- Checkpoint mode: EXACTLY_ONCE
- Storage: S3 (automatic in Confluent Cloud)

---

## Common Mistakes

1. **Checkpoint interval too short**:
   - ❌ 1 second interval (high overhead, impacts performance)
   - ✅ Balance recovery time vs overhead (MarketLag uses 5 minutes)

2. **Checkpoint timeout too short**:
   - ❌ 1 minute timeout (checkpoints fail with large state)
   - ✅ Set timeout based on state size (10+ minutes for large state)

3. **Not configuring checkpoint storage**:
   - ❌ Using default (may not persist across restarts)
   - ✅ Configure S3 or filesystem storage (Confluent Cloud does this)

4. **Checkpoints failing silently**:
   - ❌ Not monitoring checkpoint success rate
   - ✅ Monitor checkpoint metrics, set up alerts

5. **Not understanding savepoints**:
   - ❌ Using checkpoints for version upgrades
   - ✅ Use savepoints for planned stops, upgrades

---

## Mind Trigger: When to Think About This

Think about checkpoints when:
- **Configuring fault tolerance**: MarketLag uses 5-minute checkpoints
- **Deploying to production**: Checkpoints are essential for production reliability
- **Troubleshooting failures**: Check checkpoint success rate, investigate failures
- **Planning upgrades**: Use savepoints for version upgrades
- **State size planning**: Large state affects checkpoint duration

**In MarketLag project**: Uses 5-minute checkpoints with EXACTLY_ONCE mode. Checkpoints stored in S3 (automatic in Confluent Cloud). State (MapState for max_signal_delta) is included in checkpoints, enabling recovery without data loss.

---

## Summary

Checkpoints are consistent snapshots of Flink job state, enabling fault tolerance. MarketLag uses 5-minute checkpoint interval with EXACTLY_ONCE mode. Checkpoints are stored in S3 in Confluent Cloud. Flink automatically recovers from the latest checkpoint on failures. Savepoints are manual checkpoints for planned stops and upgrades. Understanding checkpoints is critical for production deployments and fault tolerance.

