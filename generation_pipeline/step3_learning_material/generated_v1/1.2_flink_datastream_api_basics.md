# Flink DataStream API Basics

**Learning Point**: 1.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.1 (Flink Architecture), Java basics, understanding of functional programming
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**DataStream API**: Flink's core API for building streaming applications. A `DataStream<T>` represents an unbounded stream of records of type `T`. Operators transform DataStreams into new DataStreams, forming a directed acyclic graph (DAG) of operations.

**Key Concepts**:
- **Source**: Creates a DataStream from external systems (Kafka, files, collections)
- **Transformation**: Operators (map, filter, flatMap, keyBy) that transform one DataStream into another
- **Sink**: Writes DataStream results to external systems
- **Operator Chaining**: Flink optimizes by chaining multiple operators into a single task to reduce serialization overhead

---

## Plain Language Explanation

The DataStream API is like an assembly line:

- **Source** = Raw materials conveyor belt (data coming in)
- **Transformations** = Workstations that modify the materials (map, filter, etc.)
- **Sink** = Finished products conveyor belt (data going out)
- **Operator Chaining** = Combining workstations to reduce handoff time

Each transformation creates a new "conveyor belt" (DataStream) that flows to the next workstation. Flink automatically optimizes by chaining workstations together when possible.

---

## Analogy

If you know **Spark's RDD API** (which you do), DataStream is similar:
- Spark RDD = Flink DataStream (both represent distributed data)
- Spark transformations (map, filter) = Flink transformations (same names, similar concepts)
- Spark actions (collect, count) = Flink sinks (write results)

Key difference: RDDs are batch-oriented, DataStreams are continuous and unbounded.

---

## Relationship to Already Learned Topics

- **1.1 Flink Architecture**: DataStream operators become tasks scheduled on TaskManager slots
- **Kafka**: DataStream sources read from Kafka topics (you'll see this in section 2.1)
- **Functional Programming**: Transformations use lambda functions (similar to Spark, which you know)
- **Event-Driven Architecture**: Each record in a DataStream triggers immediate processing

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// DataStream (simplified)
class DataStream<T> {
    StreamExecutionEnvironment environment;
    StreamTransformation<T> transformation;

    <R> DataStream<R> map(MapFunction<T, R> mapper) {
        // Creates a new transformation node in the DAG
        return new DataStream<>(
            environment,
            new OneInputTransformation<>(this.transformation, mapper)
        );
    }

    DataStream<T> filter(FilterFunction<T> filter) {
        return new DataStream<>(
            environment,
            new OneInputTransformation<>(this.transformation, filter)
        );
    }
}

// Operator chaining (simplified)
class StreamGraph {
    void optimize() {
        // Chain operators that can be chained
        // Conditions: same parallelism, no shuffle between them
        chainOperators();
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java

**Key Classes**:
- `DataStream`: Main API class
- `StreamExecutionEnvironment`: Entry point for creating DataStreams
- `MapFunction`, `FilterFunction`: Transformation functions

---

## Core Transformations

### 1. map() - One-to-One Transformation

Transforms each record into exactly one output record.

```java
DataStream<String> words = ...;
DataStream<String> upper = words.map(s -> s.toUpperCase());
// Input: ["hello", "world"]
// Output: ["HELLO", "WORLD"]
```

### 2. filter() - Record Filtering

Keeps only records that pass a condition.

```java
DataStream<Integer> numbers = ...;
DataStream<Integer> evens = numbers.filter(n -> n % 2 == 0);
// Input: [1, 2, 3, 4, 5]
// Output: [2, 4]
```

### 3. flatMap() - One-to-Many Transformation

Transforms each record into zero, one, or multiple output records.

```java
DataStream<String> sentences = ...;
DataStream<String> words = sentences.flatMap(s ->
    Arrays.asList(s.split(" ")).iterator()
);
// Input: ["hello world", "flink streaming"]
// Output: ["hello", "world", "flink", "streaming"]
```

### 4. keyBy() - Keyed Streams

Partitions stream by a key, enabling keyed state and keyed operations.

```java
DataStream<Event> events = ...;
KeyedStream<Event, String> keyed = events.keyBy(e -> e.getUserId());
// Partitions stream by userId
// All events with same userId go to same subtask
```

---

## Minimum Viable Code

```java
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;

public class DataStreamAPIDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();
        env.setParallelism(2);

        @SuppressWarnings("deprecation")
        DataStream<String> sentences = env.fromElements(
            "Apache Flink is a streaming framework",
            "Flink processes data in real-time",
            "Streaming is different from batch processing"
        );

        DataStream<String> words = sentences.flatMap(
            (String sentence, Collector<String> out) -> {
                for (String word : sentence.split("\\s+")) {
                    out.collect(word);
                }
            }
        ).returns(String.class);

        // TRANSFORMATION 2: Filter words longer than 4 characters
        DataStream<String> longWords = words.filter(word -> word.length() > 4);

        // TRANSFORMATION 3: Convert to uppercase
        DataStream<String> upper = longWords.map((MapFunction<String, String>) String::toUpperCase);

        // TRANSFORMATION 4: Key by first character
        KeyedStream<String, Character> keyed = upper.keyBy(word -> word.charAt(0));

        // SINK: Print results
        keyed.print();

        env.execute("DataStream API Demo");
    }
}
```

**Output** (example):
```
2> APACHE
1> FLINK
2> STREAMING
1> FRAMEWORK
2> PROCESSES
1> REAL-TIME
2> STREAMING
1> DIFFERENT
2> BATCH
2> PROCESSING
```

---

## Operator Chaining

Flink automatically chains operators to optimize performance:

```java
stream
    .map(...)      // Operator 1
    .filter(...)   // Operator 2
    .map(...)      // Operator 3
    .print();      // Operator 4
```

**Without chaining**: 4 separate tasks, 3 network shuffles
**With chaining**: 1 task, 0 network shuffles (all in same JVM)

**Chaining Conditions**:
- Same parallelism
- No shuffle between operators (no keyBy, rebalance, etc.)
- Same slot sharing group

**Disable chaining** (if needed):
```java
stream.map(...).disableChaining();  // Starts new chain
```

---

## DataStream Creation from Sources

### From Collections
```java
DataStream<String> stream = env.fromElements("a", "b", "c");
DataStream<Integer> numbers = env.fromCollection(Arrays.asList(1, 2, 3));
```

### From Files
```java
DataStream<String> lines = env.readTextFile("path/to/file.txt");
```

### From Kafka (see section 2.1 for details)
```java
// Will be covered in Flink-Kafka Integration section
DataStream<String> kafkaStream = env.addSource(new FlinkKafkaConsumer<>(...));
```

---

## Common Mistakes

1. **Forgetting to call execute()**:
   - ❌ Creating DataStream but not calling `env.execute()`
   - ✅ Always call `env.execute("job name")` to start execution

2. **Not understanding keyBy semantics**:
   - ❌ Using keyBy without understanding it causes a shuffle
   - ✅ keyBy repartitions data - use only when needed for stateful operations

3. **Ignoring parallelism**:
   - ❌ Not setting parallelism, using default (CPU cores)
   - ✅ Set parallelism explicitly: `env.setParallelism(4)` or per-operator

4. **Operator chaining confusion**:
   - ❌ Not understanding why operators are chained
   - ✅ Understand chaining reduces overhead but may complicate debugging

5. **Type safety issues**:
   - ❌ Using raw types, losing type information
   - ✅ Use generic types: `DataStream<MyEvent>` not `DataStream`

---

## Mind Trigger: When to Think About This

Think about DataStream API when:
- **Building streaming pipelines**: Every Flink streaming job uses DataStream API
- **Transforming data**: map, filter, flatMap are your primary tools
- **Partitioning data**: keyBy is needed before stateful operations (windows, state)
- **Performance optimization**: Understanding operator chaining helps optimize jobs
- **Debugging**: Knowing the DAG structure helps trace data flow issues

**In MarketLag project**: While the project primarily uses Flink SQL (section 1.3), understanding DataStream API helps when you need custom ProcessFunctions (section 4.1) or when debugging SQL execution plans.

---

## Summary

The DataStream API is Flink's core streaming API. Sources create streams, transformations (map, filter, flatMap, keyBy) modify them, and sinks write results. Flink optimizes by chaining operators. Understanding these basics is essential even when using Flink SQL, as SQL compiles to DataStream operations under the hood.

