# Kafka Partitioning and Flink Parallelism

**Learning Point**: 2.4 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.1 (Flink Architecture), 2.1 (Kafka Connector), Kafka partitioning knowledge
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Kafka Partition Key**: The key used to determine which Kafka partition a message is written to. Messages with the same key go to the same partition.

**Flink keyBy**: Flink operator that partitions the stream by a key. Events with the same key are processed by the same subtask.

**Partition Assignment**: How Kafka partitions are assigned to Flink subtasks. One Flink subtask can read from multiple partitions, but one partition is read by only one subtask.

**Parallelism**: Number of parallel subtasks per operator. Should match or be a factor of Kafka partition count for optimal distribution.

---

## Plain Language Explanation

Think of Kafka partitions and Flink parallelism like a restaurant:

- **Kafka Partitions** = Serving stations: Each station handles a subset of orders
- **Partition Key** = Order routing: Orders for same customer go to same station
- **Flink Parallelism** = Workers: Number of workers processing orders
- **keyBy** = Worker assignment: Same customer's orders handled by same worker

**Why It Matters**: MarketLag uses `market_slug` as partition key for `rss.events`, ensuring all events for the same market are processed together.

---

## Analogy

If you know **Spark's partitioning** (which you do), Flink's approach is similar:
- Spark partitionBy = Flink keyBy (both partition by key)
- Spark partitions = Flink subtasks (both process in parallel)
- Kafka partitions map to processing partitions

Key difference: Flink's keyBy ensures all events with same key go to same subtask, enabling keyed state.

---

## Relationship to Already Learned Topics

- **1.1 Flink Architecture**: Parallelism determines number of subtasks
- **2.1 Kafka Connector**: KafkaSource reads from partitions
- **1.7 State Types**: Keyed state requires keyBy (same key = same state)
- **Kafka**: You already know Kafka partitioning - Flink respects it

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// Partition assignment (simplified)
class KafkaSourceFunction {
    void assignPartitions(List<KafkaPartition> partitions, int parallelism) {
        // Assign partitions to subtasks
        for (int i = 0; i < partitions.size(); i++) {
            int subtaskIndex = i % parallelism;
            assignPartitionToSubtask(partitions.get(i), subtaskIndex);
        }
    }
}

// keyBy partitioning (simplified)
class KeyByOperator {
    int selectChannel(Record record, int parallelism) {
        Object key = extractKey(record);
        int hash = key.hashCode();
        return Math.abs(hash) % parallelism;  // Route to subtask
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/split/KafkaPartitionSplit.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/split/KafkaPartitionSplit.java

---

## Kafka Partition Key and Flink keyBy Relationship

### Kafka Partition Key

Kafka uses partition key to determine partition:
- Same key → Same partition
- Different keys → Distributed across partitions

**MarketLag Pattern**: `market_slug` as partition key for `rss.events`
- All RSS events for "fed-hike-january" go to same Kafka partition
- Ensures ordering per market

### Flink keyBy

Flink keyBy partitions stream by key:
- Same key → Same subtask
- Different keys → Distributed across subtasks

**Relationship**: If Kafka partition key = Flink keyBy key, then:
- Events from same Kafka partition go to same Flink subtask
- Maintains ordering and enables keyed state

---

## Partition Assignment Strategies

### Automatic Assignment (Default)

Flink automatically assigns Kafka partitions to subtasks:
- Partition 0 → Subtask 0
- Partition 1 → Subtask 1
- Partition 2 → Subtask 0 (if parallelism=2, wraps around)
- etc.

**Optimal**: Parallelism should equal or be a factor of partition count.

### Manual Assignment (Advanced)

Can manually assign partitions, but usually not needed.

---

## Parallelism and Partition Distribution

### Optimal Parallelism

**Rule of Thumb**: Set parallelism equal to Kafka partition count (or a factor).

**Example**:
- Kafka topic has 4 partitions
- Flink parallelism = 4 → Each subtask reads 1 partition (optimal)
- Flink parallelism = 2 → Each subtask reads 2 partitions (acceptable)
- Flink parallelism = 8 → Some subtasks idle (wasteful)

**MarketLag Project**: Uses parallelism=2 (for 2 CFU). Kafka topics should have 2 or 4 partitions for optimal distribution.

### Partition Distribution Example

**4 Partitions, Parallelism=2**:
- Subtask 0: Reads partitions 0, 2
- Subtask 1: Reads partitions 1, 3

**4 Partitions, Parallelism=4**:
- Subtask 0: Reads partition 0
- Subtask 1: Reads partition 1
- Subtask 2: Reads partition 2
- Subtask 3: Reads partition 3

---

## Project Pattern: MarketLag Partitioning

### RSS Events Topic

**Partition Key**: `market_slug`
- All events for same market → Same partition
- Ensures ordering per market
- Enables efficient keyed processing

**Kafka Producer (Lambda)**:
```python
producer.send('rss.events',
              key=market_slug,  # Partition key
              value=json.dumps(event))
```

**Flink Processing**:
```sql
-- In Flink SQL, keyBy happens automatically in GROUP BY
SELECT market_slug, ...
FROM rss_events
GROUP BY market_slug, TUMBLE(...)
-- Events with same market_slug processed together
```

### Polymarket Price Topic

**Partition Key**: `market_slug|outcome` (e.g., "fed-hike-january|YES")
- Prices for same market+outcome → Same partition
- Enables efficient joins with RSS signals

**Kafka Producer (Lambda)**:
```python
partition_key = f"{market_slug}|{outcome}"
producer.send('polymarket.price_hourly',
              key=partition_key,
              value=json.dumps(price_event))
```

---

## Minimum Viable Code

```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Set parallelism (should match or be factor of Kafka partition count)
env.setParallelism(4);  // If Kafka topic has 4 partitions

// Kafka source automatically distributes partitions
KafkaSource<String> source = KafkaSource.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setTopics("rss.events")  // Has 4 partitions
    .setGroupId("flink-consumer")
    .setStartingOffsets(OffsetsInitializer.latest())
    .setValueOnlyDeserializer(new SimpleStringSchema())
    .build();

DataStream<String> stream = env.fromSource(source, ...);

// keyBy ensures same key goes to same subtask
stream.keyBy(event -> extractMarketSlug(event))
    .process(...);  // Keyed state accessible per key
```

---

## Common Mistakes

1. **Parallelism not matching partitions**:
   - ❌ Parallelism=8, partitions=4 (wasteful, some subtasks idle)
   - ✅ Set parallelism equal to or factor of partition count

2. **Partition key mismatch**:
   - ❌ Kafka partition key ≠ Flink keyBy key (causes reshuffle)
   - ✅ Use same key for both (MarketLag uses market_slug)

3. **Too many partitions**:
   - ❌ 100 partitions for small data volume (overhead)
   - ✅ Use 2-4 partitions for MVP (MarketLag pattern)

4. **Not understanding keyBy**:
   - ❌ Using keyBy without understanding it causes shuffle
   - ✅ keyBy repartitions data - use only when needed for stateful operations

5. **Partition skew**:
   - ❌ One partition has much more data than others
   - ✅ Choose partition key that distributes data evenly (market_slug is good)

---

## Mind Trigger: When to Think About This

Think about Kafka partitioning and Flink parallelism when:
- **Setting up Kafka topics**: Choose partition count and partition key
- **Configuring Flink parallelism**: Should match partition count
- **Designing data flow**: MarketLag uses market_slug as partition key
- **Performance tuning**: Optimal parallelism improves throughput
- **Stateful processing**: keyBy needed for keyed state (section 1.7)

**In MarketLag project**: Uses `market_slug` as partition key for `rss.events` and `market_slug|outcome` for `polymarket.price_hourly`. Parallelism=2 matches 2 CFU allocation. Understanding partitioning ensures efficient data distribution and keyed state access.

---

## Summary

Kafka partition key determines which partition a message goes to. Flink keyBy partitions stream by key, ensuring same key goes to same subtask. Parallelism should match or be a factor of Kafka partition count. MarketLag uses `market_slug` as partition key for RSS events and `market_slug|outcome` for prices. Understanding partitioning and parallelism is essential for performance and correctness in stateful stream processing.

