# Alerting Configuration

**Learning Point**: 11.3 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 11.1 (Flink Metrics), 11.2 (Grafana Dashboard), understanding of alerting
**Version**: Grafana

---

## Definition (Engineering Language)

**Alerting Configuration**: Setting up alert rules in Grafana to notify when system issues or anomalies occur.

**Grafana Alert Rules**: Threshold-based alert rules that evaluate metrics and fire when conditions are met.

**Alert Notification Channels**: Channels for sending alerts (email, Slack, PagerDuty).

**Alert Evaluation**: Grafana evaluates alert rules at configured intervals and fires when thresholds are exceeded.

**Project Alerts**: MarketLag-specific alerts including lag signal count = 0 (unexpected), Flink job failure, and high consumer lag.

---

## Plain Language Explanation

Think of alerting like a smoke alarm:

- **Alert Rules** = Alarm settings: "Alert if smoke detected"
- **Notification Channels** = Alarm sound: "Send alert to email, Slack"
- **Thresholds** = Sensitivity: "Alert if value exceeds threshold"

**Why Needed**: MarketLag needs alerts to notify when system issues occur or anomalies are detected.

---

## Analogy

If you know **monitoring alerts** (which you do), Grafana alerting is similar:
- Monitoring alerts = Grafana alerts (both notify on issues)
- Alert rules = Alert conditions (both define when to alert)
- Notification channels = Alert destinations (both send to email, Slack)

Key difference: Grafana alerts are integrated with dashboards and metrics.

---

## Relationship to Already Learned Topics

- **11.1 Flink Metrics**: Alerts based on Flink metrics
- **11.2 Grafana Dashboard**: Alerts configured in Grafana
- **Monitoring**: You already know alerting - Grafana extends it

---

## Grafana Alert Rules: Threshold-Based Alerts

### Alert Rule Structure

**Components**:
1. **Query**: Metric query (e.g., lag signal count)
2. **Condition**: Threshold condition (e.g., count = 0)
3. **Evaluation**: How often to evaluate (e.g., every 5 minutes)
4. **Notification**: Where to send alert

### Example Alert Rule

**Alert**: Lag Signal Count = 0 (Unexpected)

**Query**:
```sql
SELECT COUNT(*) as lag_count
FROM lag_signals_history
WHERE lag_flag = true
  AND detected_at >= NOW() - INTERVAL '1 hour'
```

**Condition**: `lag_count = 0` for 1 hour

**Evaluation**: Every 5 minutes

**Notification**: Email, Slack

**MarketLag**: Uses this alert to detect when no lag signals are detected (may indicate system issue).

---

## Alert Notification Channels: Email, Slack

### Email Channel

**Configuration**:
- **Type**: Email
- **Recipients**: team@example.com
- **Subject**: MarketLag Alert: {{ .GroupLabels.alertname }}

**MarketLag**: Sends alerts to team email.

### Slack Channel

**Configuration**:
- **Type**: Slack
- **Webhook URL**: Slack webhook URL
- **Channel**: #marketlag-alerts
- **Message**: Custom message with alert details

**MarketLag**: Sends alerts to Slack channel for real-time notification.

---

## Alert Evaluation and Firing

### Evaluation Process

1. **Query Execution**: Execute metric query
2. **Condition Check**: Check if condition is met
3. **State Management**: Track alert state (OK, Pending, Firing)
4. **Notification**: Send notification when state changes to Firing

### Alert States

- **OK**: Condition not met
- **Pending**: Condition met, waiting for duration
- **Firing**: Condition met for duration, alert fired

**MarketLag**: Alerts fire when conditions are met for configured duration.

---

## Project Alerts: Lag Signal Count = 0, Flink Job Failure, High Consumer Lag

### Alert 1: Lag Signal Count = 0 (Unexpected)

**Purpose**: Detect when no lag signals are detected (may indicate system issue).

**Query**:
```sql
SELECT COUNT(*) as lag_count
FROM lag_signals_history
WHERE lag_flag = true
  AND detected_at >= NOW() - INTERVAL '1 hour'
```

**Condition**: `lag_count = 0` for 1 hour

**Why**: If system is working, should detect some lag signals. Zero may indicate:
- RSS feed not working
- Polymarket API not working
- Flink job not processing

**MarketLag**: Uses this alert to detect system issues.

### Alert 2: Flink Job Failure

**Purpose**: Detect when Flink job fails or stops.

**Query**: Flink job status metric (from Prometheus or Confluent Cloud)

**Condition**: Job status != RUNNING

**Why**: Job failures need immediate attention.

**MarketLag**: Alerts on Flink job failures.

### Alert 3: High Consumer Lag

**Purpose**: Detect when Kafka consumer lag is high (indicates backpressure).

**Query**: Kafka consumer lag metric

**Condition**: Consumer lag > 1000 records for 5 minutes

**Why**: High consumer lag indicates Flink can't keep up with Kafka.

**MarketLag**: Alerts on high consumer lag to detect backpressure.

---

## Minimum Viable Code

```yaml
# Grafana Alert Rule Configuration
apiVersion: 1

alert:
  - uid: lag-signal-zero
    name: Lag Signal Count = 0 (Unexpected)
    condition: A
    data:
      - refId: A
        datasourceUid: postgres
        model:
          query: |
            SELECT COUNT(*) as lag_count
            FROM lag_signals_history
            WHERE lag_flag = true
              AND detected_at >= NOW() - INTERVAL '1 hour'
          format: table
    noDataState: Alerting
    execErrState: Alerting
    for: 1h
    annotations:
      summary: No lag signals detected in the last hour
    labels:
      severity: warning
    notifications:
      - uid: email-channel
      - uid: slack-channel
```

---

## Common Mistakes

1. **Alert fatigue**:
   - ❌ Too many alerts, team ignores them
   - ✅ Set appropriate thresholds, only alert on important issues

2. **Not setting evaluation duration**:
   - ❌ Alert fires immediately on transient spikes
   - ✅ Set evaluation duration (e.g., 5 minutes) to avoid false positives

3. **Not testing alerts**:
   - ❌ Alerts configured but never tested
   - ✅ Test alerts to ensure they work

4. **Not setting notification channels**:
   - ❌ Alerts fire but no one notified
   - ✅ Configure notification channels (email, Slack)

5. **Not reviewing alert effectiveness**:
   - ❌ Alerts fire but no action taken
   - ✅ Review alerts regularly, adjust thresholds

---

## Mind Trigger: When to Think About This

Think about alerting configuration when:
- **Setting up monitoring**: MarketLag needs alerts for system issues
- **Detecting anomalies**: Alert on unexpected conditions (zero lag signals)
- **Job failures**: Alert on Flink job failures
- **Performance issues**: Alert on high consumer lag
- **Grafana dashboards**: Section 11.2 covers dashboards, alerts extend them

**In MarketLag project**: Configures alerts for lag signal count = 0 (unexpected), Flink job failure, and high consumer lag. Sends alerts to email and Slack. Understanding alerting configuration is essential for operational monitoring.

---

## Summary

Alerting configuration enables notification when system issues or anomalies occur. Configure Grafana alert rules with threshold-based conditions. Set up notification channels (email, Slack). MarketLag alerts on lag signal count = 0 (unexpected), Flink job failure, and high consumer lag. Understanding alerting configuration is essential for operational monitoring and incident response.

