# Data Validation in Streaming

**Learning Point**: 9.2 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 2.3 (Schema Registry), understanding of data quality
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Data Validation in Streaming**: Checking data quality and correctness as records flow through the stream processing pipeline. Ensures only valid data is processed.

**Schema Validation**: Verifying that records match expected schema structure. Uses Schema Registry for schema validation.

**Data Range Validation**: Checking that values are within expected ranges (e.g., price ∈ [0, 1] for prediction markets).

**Timestamp Validation**: Verifying timestamps are reasonable (not future, not too old).

**Missing Field Handling**: Strategies for handling missing or null fields: default values, skipping records, or error handling.

---

## Plain Language Explanation

Think of data validation like quality control:

- **Schema Validation** = Structure check: "Does this record have all required fields?"
- **Range Validation** = Value check: "Is price between 0 and 1?"
- **Timestamp Validation** = Time check: "Is timestamp reasonable?"
- **Missing Fields** = Completeness check: "Are all fields present?"

**Why Needed**: MarketLag needs to ensure data quality before processing. Invalid data can cause incorrect lag detection.

---

## Analogy

If you know **data quality frameworks** (which you do), streaming validation is similar:
- Data quality checks = Streaming validation (both check data quality)
- ETL validation = Streaming validation (both validate before processing)
- Data profiling = Validation rules (both define what's valid)

Key difference: Streaming validation happens in real-time as data flows.

---

## Relationship to Already Learned Topics

- **2.3 Schema Registry**: Schema validation uses Schema Registry
- **9.1 Dead Letter Queue**: Invalid records sent to DLQ
- **9.3 Exception Handling**: Validation failures handled gracefully
- **Data Quality**: You already know data quality - this applies to streaming

---

## Schema Validation: Using Schema Registry

### Schema Registry Validation

**How It Works**:
1. Schema Registry stores expected schema
2. Records validated against schema
3. Invalid records rejected or sent to DLQ

**MarketLag**: Can use Schema Registry for schema validation (section 2.3).

### Flink SQL Schema Validation

```sql
CREATE TABLE rss_events (
    title STRING,
    published_at TIMESTAMP(3),
    source STRING
) WITH (
    'connector' = 'kafka',
    'format' = 'json',
    'json.ignore-parse-errors' = 'false'  -- Reject invalid JSON
);
```

**MarketLag**: Uses `json.ignore-parse-errors` = 'true' to skip malformed JSON, but could validate schema.

---

## Data Range Validation: price ∈ [0,1]

### Validation Rule

**Rule**: Price must be between 0 and 1 (prediction market prices).

**Implementation**:

```sql
-- In Flink SQL
SELECT *
FROM polymarket_price_hourly
WHERE price >= 0 AND price <= 1
  AND price IS NOT NULL
```

**MarketLag**: Validates price range in Flink SQL queries.

### Lambda Validation

```python
def validate_price(price):
    """
    Validate price is in valid range.

    Args:
        price: Price value

    Returns:
        bool: True if valid
    """
    if price is None:
        return False
    return 0.0 <= float(price) <= 1.0

# Usage
if not validate_price(price):
    send_to_dlq(record)  # Invalid price, send to DLQ
    return
```

**MarketLag**: Can validate in Lambda before sending to Kafka.

---

## Timestamp Validation: Not Future, Not Too Old

### Validation Rules

1. **Not Future**: Timestamp should not be in the future
2. **Not Too Old**: Timestamp should not be too old (e.g., > 7 days)

### Implementation

```python
from datetime import datetime, timedelta
import pytz

def validate_timestamp(timestamp_str, max_age_days=7):
    """
    Validate timestamp is reasonable.

    Args:
        timestamp_str: ISO 8601 timestamp string
        max_age_days: Maximum age in days

    Returns:
        bool: True if valid
    """
    try:
        timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        now = datetime.now(pytz.UTC)

        # Not future
        if timestamp > now:
            return False

        # Not too old
        max_age = timedelta(days=max_age_days)
        if timestamp < now - max_age:
            return False

        return True
    except Exception:
        return False
```

**MarketLag**: Validates timestamps in Lambda or Flink.

---

## Missing Field Handling: Default Values, Skipping Records

### Strategy 1: Default Values

```sql
-- Use COALESCE for default values
SELECT
    market_slug,
    COALESCE(price, 0.0) as price,  -- Default to 0.0 if null
    COALESCE(source, 'unknown') as source
FROM rss_events
```

**Use Case**: When missing fields can be safely defaulted.

### Strategy 2: Skipping Records

```sql
-- Filter out records with missing required fields
SELECT *
FROM rss_events
WHERE market_slug IS NOT NULL
  AND published_at IS NOT NULL
  AND price IS NOT NULL
```

**Use Case**: When missing fields make record unusable.

**MarketLag**: Filters out records with missing required fields.

### Strategy 3: Error Handling

```python
# In Lambda or Flink
if required_field is None:
    send_to_dlq(record)  # Send to DLQ for analysis
    return
```

**Use Case**: When missing fields indicate data quality issues.

---

## Minimum Viable Code

```python
# Data validation in Lambda
def validate_record(record):
    """
    Validate record before sending to Kafka.

    Args:
        record: Record to validate

    Returns:
        tuple: (is_valid, errors)
    """
    errors = []

    # Schema validation: required fields
    if 'market_slug' not in record:
        errors.append("Missing market_slug")
    if 'price' not in record:
        errors.append("Missing price")

    # Range validation
    if 'price' in record:
        price = record['price']
        if price is not None and (price < 0 or price > 1):
            errors.append(f"Price out of range: {price}")

    # Timestamp validation
    if 'published_at' in record:
        if not validate_timestamp(record['published_at']):
            errors.append("Invalid timestamp")

    return len(errors) == 0, errors

# Usage
is_valid, errors = validate_record(record)
if not is_valid:
    print(f"Validation failed: {errors}")
    send_to_dlq(record)
else:
    send_to_kafka(record)
```

---

## Common Mistakes

1. **Not validating data**:
   - ❌ Processing invalid data, causing downstream errors
   - ✅ Always validate data before processing

2. **Too strict validation**:
   - ❌ Rejecting valid data due to overly strict rules
   - ✅ Balance validation strictness with data loss

3. **Not handling missing fields**:
   - ❌ Assuming all fields exist, causing errors
   - ✅ Always check for missing fields, use defaults or skip

4. **Not validating timestamps**:
   - ❌ Accepting future timestamps, causing window issues
   - ✅ Validate timestamps are reasonable

5. **Not sending invalid data to DLQ**:
   - ❌ Silently dropping invalid data
   - ✅ Send to DLQ for analysis and potential reprocessing

---

## Mind Trigger: When to Think About This

Think about data validation when:
- **Ingesting data**: Validate data at ingestion (Lambda)
- **Processing data**: Validate in Flink before processing
- **Schema evolution**: Section 2.3 covers schema validation
- **Error handling**: Section 9.1 covers DLQ for invalid data
- **Data quality**: Ensuring data quality throughout pipeline

**In MarketLag project**: Validates data at multiple points: Lambda (before Kafka), Flink (before processing). Validates price range [0,1], timestamps, and required fields. Invalid data sent to DLQ. Understanding data validation is essential for data quality.

---

## Summary

Data validation in streaming checks data quality as records flow through the pipeline. Schema validation uses Schema Registry. Range validation checks values (e.g., price ∈ [0,1]). Timestamp validation ensures reasonable timestamps. Missing fields handled with defaults, skipping, or error handling. MarketLag validates data at multiple points. Understanding data validation is essential for data quality in streaming applications.

