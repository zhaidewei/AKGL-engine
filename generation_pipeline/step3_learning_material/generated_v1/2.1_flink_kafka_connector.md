# Flink Kafka Connector

**Learning Point**: 2.1 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.2 (DataStream API), Kafka knowledge (you already have this)
**Version**: Flink 2.2.0, Kafka 3.7.2

---

## Definition (Engineering Language)

**Flink Kafka Connector**: Libraries that enable Flink to read from and write to Kafka topics. Provides source and sink implementations for Kafka integration.

**FlinkKafkaConsumer** (deprecated): Legacy Kafka source connector. Replaced by KafkaSource in Flink 1.12+.

**KafkaSource** (new): Modern Kafka source connector with improved performance and features. Uses Kafka's new consumer API.

**FlinkKafkaProducer** (deprecated): Legacy Kafka sink connector. Replaced by KafkaSink in Flink 1.12+.

**KafkaSink** (new): Modern Kafka sink connector with improved reliability and features.

**Consumer Group Management**: Flink manages Kafka consumer groups automatically, handling partition assignment and offset commits.

---

## Plain Language Explanation

Think of Flink Kafka connector as a bridge between Flink and Kafka:

- **KafkaSource** = Reading bridge: Flink reads data from Kafka topics
- **KafkaSink** = Writing bridge: Flink writes data to Kafka topics
- **Consumer Group** = Reading team: Flink workers coordinate to read from different Kafka partitions

**Why Needed**: MarketLag project flows all data through Kafka - RSS events and Polymarket prices are in Kafka topics, Flink jobs read from and write to Kafka.

---

## Analogy

If you know **Spark's Kafka connector** (which you do from Spark experience), Flink's connector is similar:
- Spark readStream from Kafka = Flink KafkaSource
- Spark writeStream to Kafka = Flink KafkaSink
- Consumer groups work the same way

Key difference: Flink uses newer Kafka consumer API, providing better performance and exactly-once semantics.

---

## Relationship to Already Learned Topics

- **1.2 DataStream API**: KafkaSource creates DataStreams
- **Kafka**: You already know Kafka from your experience - Flink integrates with it
- **2.2 Kafka Table Connector**: Higher-level SQL interface (uses KafkaSource under the hood)
- **Consumer Groups**: Similar to Kafka consumer groups you already know

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// KafkaSource (simplified)
class KafkaSource<T> implements SourceFunction<T> {
    List<String> topics;
    KafkaConsumerConfig config;

    void run(SourceContext<T> ctx) {
        KafkaConsumer consumer = createConsumer();
        consumer.subscribe(topics);

        while (running) {
            ConsumerRecords records = consumer.poll();
            for (ConsumerRecord record : records) {
                T value = deserialize(record.value());
                ctx.collectWithTimestamp(value, record.timestamp());
            }

            // Commit offsets (for checkpointing)
            commitOffsets();
        }
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSource.java

**Key Classes**:
- `KafkaSource`: Modern Kafka source
- `KafkaSink`: Modern Kafka sink
- `FlinkKafkaConsumer`: Legacy source (deprecated)
- `FlinkKafkaProducer`: Legacy sink (deprecated)

---

## Kafka Consumer Configuration in Flink

### Basic KafkaSource Setup

```java
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.api.common.serialization.SimpleStringSchema;

KafkaSource<String> source = KafkaSource.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setTopics("rss.events")
    .setGroupId("flink-consumer-group")
    .setStartingOffsets(OffsetsInitializer.latest())
    .setValueOnlyDeserializer(new SimpleStringSchema())
    .build();

DataStream<String> stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), "Kafka Source");
```

### Confluent Cloud Authentication

```java
Properties props = new Properties();
props.setProperty("bootstrap.servers", "<confluent-cloud-endpoint>");
props.setProperty("security.protocol", "SASL_SSL");
props.setProperty("sasl.mechanism", "PLAIN");
props.setProperty("sasl.jaas.config",
    "org.apache.kafka.common.security.plain.PlainLoginModule required " +
    "username='<api-key>' password='<api-secret>';");

KafkaSource<String> source = KafkaSource.<String>builder()
    .setBootstrapServers(props.getProperty("bootstrap.servers"))
    .setKafkaConsumerConfig(props)
    .setTopics("rss.events")
    .setGroupId("flink-consumer-group")
    .setStartingOffsets(OffsetsInitializer.latest())
    .setValueOnlyDeserializer(new SimpleStringSchema())
    .build();
```

**MarketLag Project**: Uses Confluent Cloud with SASL_SSL authentication.

---

## Kafka Producer Configuration in Flink

### Basic KafkaSink Setup

```java
import org.apache.flink.connector.kafka.sink.KafkaSink;
import org.apache.flink.api.common.serialization.SimpleStringSchema;

KafkaSink<String> sink = KafkaSink.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setRecordSerializer(KafkaRecordSerializationSchema.builder()
        .setTopic("output.topic")
        .setValueSerializationSchema(new SimpleStringSchema())
        .build())
    .setKafkaProducerConfig(producerProps)
    .build();

stream.sinkTo(sink);
```

### Exactly-Once Semantics

```java
// Enable exactly-once (requires idempotent producer)
Properties producerProps = new Properties();
producerProps.setProperty("acks", "all");
producerProps.setProperty("retries", "3");
producerProps.setProperty("enable.idempotence", "true");

KafkaSink<String> sink = KafkaSink.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setRecordSerializer(...)
    .setKafkaProducerConfig(producerProps)
    .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)  // Requires idempotence
    .build();
```

---

## Kafka Source: FlinkKafkaConsumer (Deprecated) vs KafkaSource (New)

### FlinkKafkaConsumer (Deprecated - Don't Use)

```java
// OLD WAY - Don't use
FlinkKafkaConsumer<String> consumer = new FlinkKafkaConsumer<>(
    "topic",
    new SimpleStringSchema(),
    properties
);
```

**Why Deprecated**: Uses old Kafka consumer API, less efficient, fewer features.

### KafkaSource (New - Use This)

```java
// NEW WAY - Use this
KafkaSource<String> source = KafkaSource.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setTopics("topic")
    .setGroupId("group")
    .setStartingOffsets(OffsetsInitializer.latest())
    .setValueOnlyDeserializer(new SimpleStringSchema())
    .build();
```

**Advantages**: Better performance, exactly-once semantics, improved offset management.

**MarketLag Project**: Should use KafkaSource (though project primarily uses Flink SQL which uses KafkaSource internally).

---

## Kafka Sink: FlinkKafkaProducer (Deprecated) vs KafkaSink (New)

### FlinkKafkaProducer (Deprecated - Don't Use)

```java
// OLD WAY - Don't use
FlinkKafkaProducer<String> producer = new FlinkKafkaProducer<>(
    "topic",
    new SimpleStringSchema(),
    properties
);
```

### KafkaSink (New - Use This)

```java
// NEW WAY - Use this
KafkaSink<String> sink = KafkaSink.<String>builder()
    .setBootstrapServers("localhost:9092")
    .setRecordSerializer(KafkaRecordSerializationSchema.builder()
        .setTopic("topic")
        .setValueSerializationSchema(new SimpleStringSchema())
        .build())
    .build();
```

---

## Consumer Group Management and Offset Handling

### Automatic Offset Management

Flink automatically manages Kafka offsets:
- **Checkpoint Integration**: Offsets are included in checkpoints
- **Automatic Commits**: Offsets committed on checkpoint completion
- **Recovery**: On restart, Flink resumes from last checkpointed offsets

### Starting Offsets

```java
// Start from latest (skip old messages)
.setStartingOffsets(OffsetsInitializer.latest())

// Start from earliest (read all messages)
.setStartingOffsets(OffsetsInitializer.earliest())

// Start from specific offsets
.setStartingOffsets(OffsetsInitializer.offsets(Map<TopicPartition, Long>))
```

**MarketLag Project**: Typically starts from latest for production (process new events only).

---

## Minimum Viable Code

```java
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.sink.KafkaSink;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;

public class KafkaConnectorDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // Kafka Source
        KafkaSource<String> source = KafkaSource.<String>builder()
            .setBootstrapServers("localhost:9092")
            .setTopics("input.topic")
            .setGroupId("flink-consumer")
            .setStartingOffsets(OffsetsInitializer.latest())
            .setValueOnlyDeserializer(new SimpleStringSchema())
            .build();

        DataStream<String> stream = env.fromSource(
            source,
            WatermarkStrategy.noWatermarks(),
            "Kafka Source"
        );

        // Process
        DataStream<String> processed = stream.map(s -> s.toUpperCase());

        // Kafka Sink
        KafkaSink<String> sink = KafkaSink.<String>builder()
            .setBootstrapServers("localhost:9092")
            .setRecordSerializer(KafkaRecordSerializationSchema.builder()
                .setTopic("output.topic")
                .setValueSerializationSchema(new SimpleStringSchema())
                .build())
            .build();

        processed.sinkTo(sink);

        env.execute("Kafka Connector Demo");
    }
}
```

---

## Common Mistakes

1. **Using deprecated connectors**:
   - ❌ Using FlinkKafkaConsumer/FlinkKafkaProducer
   - ✅ Use KafkaSource/KafkaSink (new API)

2. **Not configuring authentication**:
   - ❌ Missing SASL_SSL config for Confluent Cloud
   - ✅ Always configure security.protocol, sasl.mechanism, sasl.jaas.config

3. **Wrong starting offsets**:
   - ❌ Using earliest in production (processes all historical data)
   - ✅ Use latest for production (process new events only)

4. **Not handling deserialization errors**:
   - ❌ Job fails on malformed messages
   - ✅ Use error handling, dead letter queues (section 9.1)

5. **Consumer group conflicts**:
   - ❌ Multiple jobs using same consumer group
   - ✅ Use unique consumer group per job

---

## Mind Trigger: When to Think About This

Think about Flink Kafka connector when:
- **Reading from Kafka**: MarketLag reads RSS events and Polymarket prices from Kafka
- **Writing to Kafka**: Flink jobs may write intermediate results to Kafka
- **Configuring authentication**: Confluent Cloud requires SASL_SSL
- **Offset management**: Understanding how Flink manages offsets helps with recovery
- **Using Flink SQL**: Section 2.2 covers higher-level SQL interface (uses KafkaSource internally)

**In MarketLag project**: All data flows through Kafka. Flink jobs read from Kafka topics (rss.events, polymarket.price_hourly) and write results. While the project uses Flink SQL (section 2.2), understanding the underlying connector helps with troubleshooting and optimization.

---

## Summary

Flink provides KafkaSource (new) and KafkaSink (new) for Kafka integration. Legacy FlinkKafkaConsumer/FlinkKafkaProducer are deprecated. Flink automatically manages consumer groups and offsets, integrating with checkpoints. Confluent Cloud requires SASL_SSL authentication. Understanding the connector is essential even when using Flink SQL (section 2.2), as SQL uses the connector under the hood.

