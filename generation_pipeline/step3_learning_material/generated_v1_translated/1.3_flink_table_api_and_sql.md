# Flink Table API and SQL

**Learning Point**: 1.3 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.2 (DataStream API), SQL 知识（您已具备）
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Flink Table API**: 用于批处理和流处理的统一关系型 API。Table 是核心抽象，表示具有模式的结构化数据。Table API 为表操作提供流畅的 DSL。

**Flink SQL**: 构建在 Table API 之上的 SQL 接口。Flink SQL 扩展了标准 SQL，增加了流处理语义（窗口、watermark、时间函数）。

**Relationship**: Flink SQL 语句被解析并转换为 Table API 操作，然后编译为 DataStream/DataSet 操作。

---

## Plain Language Explanation

将 Flink Table API/SQL 视为流数据的 SQL 数据库：

- **Table** = 数据库表，但不是静态行，而是连续的行流
- **Table API** = 以编程方式查询表（类似于使用查询构建器库）
- **Flink SQL** = 编写 SQL 查询（就像您使用 PostgreSQL 一样，但用于流）

与传统 SQL 的关键区别：表是**无界的**（无限流），因此查询持续运行并在新数据到达时产生结果。

---

## Analogy

如果您了解 **Spark SQL**（您从 Databricks 经验中了解），Flink SQL 非常相似：
- Spark DataFrame = Flink Table
- Spark SQL 查询 = Flink SQL 查询
- Spark Structured Streaming = Flink SQL 流处理

主要区别：Flink SQL 针对低延迟流处理进行了优化，而 Spark SQL 专注于微批处理。

---

## Relationship to Already Learned Topics

- **1.2 DataStream API**: Table API/SQL 在底层编译为 DataStream 操作
- **SQL**: 使用您已经了解的标准 SQL 语法（SELECT、FROM、WHERE、JOIN、GROUP BY）
- **Kafka**: 可以从 Kafka topic 创建表（参见第 2.2 节）
- **PostgreSQL/Supabase**: 类似的 SQL 语法，但 Flink SQL 添加了流处理扩展

---

## Pseudocode (From Source Code)

基于 Flink 2.2.0 源代码：

```java
// Table (simplified)
class Table {
    TableEnvironment environment;
    TableSchema schema;
    RelNode logicalPlan;  // SQL logical plan

    Table select(String fields) {
        // Parse SQL and create new Table
        return environment.sqlQuery("SELECT " + fields + " FROM this");
    }
}

// TableEnvironment (simplified)
class StreamTableEnvironment {
    StreamExecutionEnvironment streamEnv;

    void createTemporaryView(String name, DataStream<?> stream) {
        // Convert DataStream to Table
        Table table = streamToTable(stream);
        registerTable(name, table);
    }

    TableResult executeSql(String sql) {
        // 1. Parse SQL
        SqlNode sqlNode = parser.parse(sql);

        // 2. Convert to logical plan
        RelNode relNode = sqlToRelConverter.convert(sqlNode);

        // 3. Optimize
        RelNode optimized = optimizer.optimize(relNode);

        // 4. Convert to DataStream
        DataStream<?> stream = relToDataStreamConverter.convert(optimized);

        // 5. Execute
        return streamEnv.execute();
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/Table.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/Table.java

**Key Classes**:
- `Table`: 主 Table API 类
- `StreamTableEnvironment`: 流处理 Table API/SQL 的环境
- `TableResult`: 执行表查询的结果

---

## When to Use Table API vs DataStream API

### 使用 Table API/SQL 的情况：
- ✅ 您熟悉 SQL
- ✅ 逻辑主要是关系型的（连接、聚合、过滤）
- ✅ 您希望快速开发（SQL 简洁）
- ✅ 团队有 SQL 专业知识但 Java/Scala 知识有限

### 使用 DataStream API 的情况：
- ✅ 您需要细粒度控制（自定义 ProcessFunction）
- ✅ 复杂事件处理（CEP 模式）
- ✅ 低级状态管理
- ✅ 性能关键的自定义算子

**MarketLag Project**: 作业 1、2 和 3 使用 Flink SQL，因为逻辑主要是关系型的（窗口聚合、连接）。

---

## Creating Tables from DataStreams

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.table.api.Table;

StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

// Create DataStream
DataStream<Event> events = env.fromElements(
    new Event("user1", "click", 1000L),
    new Event("user2", "view", 2000L)
);

// Convert to Table
Table eventTable = tableEnv.fromDataStream(events);
// Or register as temporary view
tableEnv.createTemporaryView("events", events);
```

---

## Flink SQL Syntax

Flink SQL extends standard SQL with streaming concepts:

### Basic Query
```sql
SELECT user_id, COUNT(*) as click_count
FROM events
WHERE event_type = 'click'
GROUP BY user_id;
```

### Window Aggregation (Streaming-Specific)
```sql
SELECT
    user_id,
    TUMBLE_START(event_time, INTERVAL '1' HOUR) as window_start,
    COUNT(*) as click_count
FROM events
GROUP BY user_id, TUMBLE(event_time, INTERVAL '1' HOUR);
```

### Creating Tables from Kafka (see section 2.2 for details)
```sql
CREATE TABLE rss_events (
    title STRING,
    published_at TIMESTAMP(3),
    source STRING,
    WATERMARK FOR published_at AS published_at - INTERVAL '5' MINUTE
) WITH (
    'connector' = 'kafka',
    'topic' = 'rss.events',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json'
);
```

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class FlinkSQLDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        // Create a simple DataStream
        DataStream<String> words = env.fromElements(
            "hello", "world", "flink", "streaming", "hello", "flink"
        );

        // Register as temporary view
        tableEnv.createTemporaryView("words", words, $("word"));

        // Execute SQL query
        Table result = tableEnv.sqlQuery(
            "SELECT word, COUNT(*) as cnt " +
            "FROM words " +
            "GROUP BY word " +
            "ORDER BY cnt DESC"
        );

        // Convert back to DataStream and print
        tableEnv.toDataStream(result).print();

        env.execute("Flink SQL Demo");
    }
}
```

**Output**:
```
hello: 2
flink: 2
world: 1
streaming: 1
```

---

## Registering Tables and Views

### Temporary View (Session-scoped)
```java
tableEnv.createTemporaryView("my_table", dataStream);
// Can be used in SQL queries within this session
```

### Temporary Table (Session-scoped, can be modified)
```java
tableEnv.createTemporaryTable("my_table", tableDescriptor);
```

### Catalog Table (Persistent, stored in catalog)
```java
// Register in catalog (e.g., Hive, JDBC catalog)
tableEnv.createTable("catalog.db.my_table", tableDescriptor);
```

**MarketLag Project**: 使用从 Kafka 表连接器创建的临时视图（参见第 2.2 节）。

---

## Table API vs SQL Comparison

### Table API (Programmatic)
```java
Table result = tableEnv.from("events")
    .select($("user_id"), $("event_type"))
    .where($("event_type").isEqual("click"))
    .groupBy($("user_id"))
    .select($("user_id"), $("event_type").count().as("cnt"));
```

### Flink SQL (Declarative)
```sql
SELECT user_id, COUNT(*) as cnt
FROM events
WHERE event_type = 'click'
GROUP BY user_id;
```

**两者编译为相同的执行计划**。SQL 更简洁；Table API 提供更多编译时类型安全。

---

## Common Mistakes

1. **混合使用 Table API 和 DataStream 而不转换**:
   - ❌ 在 Table 上使用 DataStream 方法
   - ✅ 转换：`tableEnv.toDataStream(table)` 或 `tableEnv.fromDataStream(stream)`

2. **忘记 WATERMARK 声明**:
   - ❌ 为事件时间窗口创建表时没有 watermark
   - ✅ 始终声明 watermark：`WATERMARK FOR time_col AS time_col - INTERVAL '5' MINUTE`

3. **不理解表与视图的区别**:
   - ❌ 在视图足够时创建表
   - ✅ 只读使用临时视图，可写源使用表

4. **流处理 SQL 语法错误**:
   - ❌ 在不理解流处理语义的情况下使用标准 SQL
   - ✅ 学习 Flink SQL 扩展：窗口、watermark、时间连接

5. **类型转换问题**:
   - ❌ 假设自动类型转换在任何地方都有效
   - ✅ 显式转换类型：`CAST(col AS TIMESTAMP(3))`

---

## Mind Trigger: When to Think About This

在以下情况下考虑 Flink Table API/SQL：
- **编写 Flink 作业**: MarketLag 项目的所有三个作业都使用 SQL
- **创建 Kafka 表连接器**: 第 2.2 节展示了 CREATE TABLE 语法
- **窗口聚合**: 第 3.1 节涵盖 TUMBLE 窗口函数
- **连接流**: 第 3.3 节涵盖 SQL 中的等值连接
- **性能问题**: SQL 查询编译为 DataStream - 理解执行计划

**在 MarketLag 项目中**: 所有三个作业（RSS 聚合、价格处理、延迟检测）都使用 Flink SQL。理解 SQL 对这个项目至关重要。

---

## Summary

Flink Table API 和 SQL 为流数据提供关系型接口。表表示具有模式的无界流。Flink SQL 扩展了标准 SQL，增加了流处理概念（窗口、watermark）。MarketLag 项目使用 Flink SQL 作为主要接口，使 SQL 知识变得至关重要。Table API/SQL 编译为 DataStream 操作，因此理解两者有助于优化和调试。

