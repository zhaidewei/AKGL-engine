# Alerting Configuration

**Learning Point**: 11.3 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 11.1 (Flink Metrics), 11.2 (Grafana Dashboard), understanding of alerting
**Version**: Grafana

---

## Definition (Engineering Language)

**Alerting Configuration**: Setting up alert rules in Grafana to notify when system issues or anomalies occur.

**Grafana Alert Rules**: Threshold-based alert rules that evaluate metrics and fire when conditions are met.

**Alert Notification Channels**: Channels for sending alerts (email, Slack, PagerDuty).

**Alert Evaluation**: Grafana evaluates alert rules at configured intervals and fires when thresholds are exceeded.

**Project Alerts**: MarketLag-specific alerts including lag signal count = 0 (unexpected), Flink job failure, and high consumer lag.

---

## Plain Language Explanation

将告警想象成烟雾报警器：

- **Alert Rules** = 报警设置："如果检测到烟雾则报警"
- **Notification Channels** = 报警声音："将告警发送到 email、Slack"
- **Thresholds** = 灵敏度："如果值超过阈值则报警"

**为什么需要**：MarketLag 需要告警以在系统问题发生或检测到异常时通知。

---

## Analogy

如果您了解 **monitoring alerts**（您了解），Grafana alerting 类似：
- Monitoring alerts = Grafana alerts（都在出现问题时通知）
- Alert rules = Alert conditions（都定义何时告警）
- Notification channels = Alert destinations（都发送到 email、Slack）

关键区别：Grafana alerts 与 dashboards 和 metrics 集成。

---

## Relationship to Already Learned Topics

- **11.1 Flink Metrics**：基于 Flink metrics 的告警
- **11.2 Grafana Dashboard**：在 Grafana 中配置告警
- **Monitoring**：您已经了解告警 - Grafana 扩展了它

---

## Grafana Alert Rules: Threshold-Based Alerts

### Alert Rule Structure

**Components**:
1. **Query**: Metric query (e.g., lag signal count)
2. **Condition**: Threshold condition (e.g., count = 0)
3. **Evaluation**: How often to evaluate (e.g., every 5 minutes)
4. **Notification**: Where to send alert

### Example Alert Rule

**Alert**: Lag Signal Count = 0 (Unexpected)

**Query**:
```sql
SELECT COUNT(*) as lag_count
FROM lag_signals_history
WHERE lag_flag = true
  AND detected_at >= NOW() - INTERVAL '1 hour'
```

**Condition**: `lag_count = 0` for 1 hour

**Evaluation**: Every 5 minutes

**Notification**: Email, Slack

**MarketLag**：使用此告警检测何时未检测到 lag signals（可能表示系统问题）。

---

## Alert Notification Channels: Email, Slack

### Email Channel

**Configuration**:
- **Type**: Email
- **Recipients**: team@example.com
- **Subject**: MarketLag Alert: {{ .GroupLabels.alertname }}

**MarketLag**：将告警发送到团队 email。

### Slack Channel

**Configuration**:
- **Type**: Slack
- **Webhook URL**: Slack webhook URL
- **Channel**: #marketlag-alerts
- **Message**: Custom message with alert details

**MarketLag**：将告警发送到 Slack channel 以进行实时通知。

---

## Alert Evaluation and Firing

### Evaluation Process

1. **Query Execution**: Execute metric query
2. **Condition Check**: Check if condition is met
3. **State Management**: Track alert state (OK, Pending, Firing)
4. **Notification**: Send notification when state changes to Firing

### Alert States

- **OK**: Condition not met
- **Pending**: Condition met, waiting for duration
- **Firing**: Condition met for duration, alert fired

**MarketLag**：当条件满足配置的持续时间时触发告警。

---

## Project Alerts: Lag Signal Count = 0, Flink Job Failure, High Consumer Lag

### Alert 1: Lag Signal Count = 0 (Unexpected)

**目的**：检测何时未检测到 lag signals（可能表示系统问题）。

**Query**:
```sql
SELECT COUNT(*) as lag_count
FROM lag_signals_history
WHERE lag_flag = true
  AND detected_at >= NOW() - INTERVAL '1 hour'
```

**Condition**: `lag_count = 0` for 1 hour

**原因**：如果系统正常工作，应该检测到一些 lag signals。零可能表示：
- RSS feed 不工作
- Polymarket API 不工作
- Flink 作业未处理

**MarketLag**：使用此告警检测系统问题。

### Alert 2: Flink Job Failure

**目的**：检测 Flink 作业何时失败或停止。

**查询**：Flink 作业状态 metric（来自 Prometheus 或 Confluent Cloud）

**条件**：Job status != RUNNING

**原因**：作业失败需要立即关注。

**MarketLag**：对 Flink 作业失败进行告警。

### Alert 3: High Consumer Lag

**目的**：检测 Kafka consumer lag 何时较高（表示存在反压）。

**查询**：Kafka consumer lag metric

**条件**：Consumer lag > 1000 条记录持续 5 分钟

**原因**：高 consumer lag 表示 Flink 无法跟上 Kafka。

**MarketLag**：对高 consumer lag 进行告警以检测反压。

---

## Minimum Viable Code

```yaml
# Grafana Alert Rule Configuration
apiVersion: 1

alert:
  - uid: lag-signal-zero
    name: Lag Signal Count = 0 (Unexpected)
    condition: A
    data:
      - refId: A
        datasourceUid: postgres
        model:
          query: |
            SELECT COUNT(*) as lag_count
            FROM lag_signals_history
            WHERE lag_flag = true
              AND detected_at >= NOW() - INTERVAL '1 hour'
          format: table
    noDataState: Alerting
    execErrState: Alerting
    for: 1h
    annotations:
      summary: No lag signals detected in the last hour
    labels:
      severity: warning
    notifications:
      - uid: email-channel
      - uid: slack-channel
```

---

## Common Mistakes

1. **告警疲劳**：
   - ❌ 告警过多，团队忽略它们
   - ✅ 设置适当的阈值，仅对重要问题告警

2. **不设置评估持续时间**：
   - ❌ 在瞬态峰值时立即触发告警
   - ✅ 设置评估持续时间（例如，5 分钟）以避免误报

3. **不测试告警**：
   - ❌ 配置了告警但从未测试
   - ✅ 测试告警以确保它们工作

4. **不设置通知渠道**：
   - ❌ 告警触发但没有人收到通知
   - ✅ 配置通知渠道（email、Slack）

5. **不审查告警有效性**：
   - ❌ 告警触发但没有采取行动
   - ✅ 定期审查告警，调整阈值

---

## Mind Trigger: When to Think About This

在以下情况下考虑告警配置：
- **设置监控**：MarketLag 需要针对系统问题的告警
- **检测异常**：对意外条件（零 lag signals）告警
- **作业失败**：对 Flink 作业失败告警
- **性能问题**：对高 consumer lag 告警
- **Grafana dashboards**：第 11.2 节涵盖 dashboards，告警扩展了它们

**在 MarketLag 项目中**：配置 lag signal count = 0（意外）、Flink 作业失败和高 consumer lag 的告警。将告警发送到 email 和 Slack。理解告警配置对运维监控至关重要。

---

## Summary

告警配置支持在系统问题或异常发生时进行通知。配置具有基于阈值条件的 Grafana alert rules。设置通知渠道（email、Slack）。MarketLag 对 lag signal count = 0（意外）、Flink 作业失败和高 consumer lag 进行告警。理解告警配置对运维监控和事件响应至关重要。

