# Deploying Flink Jobs to Confluent Cloud

**Learning Point**: 6.3 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 6.1 (Confluent Cloud Overview), 6.2 (Flink Environment), understanding of deployment
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Job Submission Methods**: 将 Flink 作业部署到 Confluent Cloud 的方法：UI（Web 界面）、CLI（命令行）和 API（REST API）。

**Job Configuration**: Flink 作业的设置：并行度、检查点间隔、检查点模式等。

**Job Monitoring**: 通过 UI 仪表板、指标和日志观察作业健康状况和性能。

**Job Update**: 更新已部署的作业：版本控制、滚动更新、配置更改。

**Confluent Cloud UI**: 用于管理 Flink environments 和 jobs 的 Web 界面。

**Confluent CLI**: 用于管理 Confluent Cloud 资源的命令行工具。

**Confluent REST API**: 用于管理 Confluent Cloud 资源的编程接口。

---

## Plain Language Explanation

将部署 Flink 作业想象成部署应用程序：

- **UI** = Web 仪表板：单击按钮部署作业
- **CLI** = 命令行：输入命令部署作业
- **API** = 编程方式：编写脚本部署作业

**为什么需要**：MarketLag 作业需要部署到 Confluent Cloud 进行生产。理解部署方法有助于选择正确的方法。

---

## Analogy

如果您了解 **AWS Lambda 部署**（您了解），Confluent Cloud 部署类似：
- AWS Console = Confluent Cloud UI（两者都是 Web 界面）
- AWS CLI = Confluent CLI（两者都是命令行工具）
- AWS API = Confluent REST API（两者都是编程接口）

关键区别：Confluent Cloud 管理 Flink 基础设施，您只需部署作业。

---

## Relationship to Already Learned Topics

- **6.1 Confluent Cloud Overview**: Understanding Confluent Cloud architecture
- **6.2 Flink Environment**: Flink environment must be created before deploying jobs
- **1.10 Checkpoints**: Checkpoint configuration is part of job deployment
- **Deployment**: You already know deployment patterns from AWS experience

---

## Pseudocode (From Confluent Cloud)

Based on Confluent Cloud deployment process:

```bash
# CLI deployment (simplified)
confluent flink job create \
    --name "marketlag-job-1" \
    --sql-file "job1.sql" \
    --parallelism 2 \
    --checkpoint-interval 300000
```

---

## Source Code References

**Confluent Cloud Documentation**: Confluent Cloud UI, CLI, and API documentation (not in Flink source).

**MarketLag Project**: Deployment configuration in project design.

---

## Job Submission Methods

### UI: Upload JAR or SQL Script

**Steps**:
1. Log in to Confluent Cloud UI
2. Navigate to Flink environment
3. Click "Create Job"
4. Upload SQL script or JAR file
5. Configure job settings (parallelism, checkpoint interval)
6. Submit job

**Use Case**: One-off deployments, manual deployments, testing.

**MarketLag Project**: Can use UI for initial deployment and testing.

### CLI: Using `confluent flink job create` Command

**Installation**:
```bash
# Install Confluent CLI
curl -sL --http1.1 https://cnfl.io/cli | sh -s -- -b /usr/local/bin
```

**Authentication**:
```bash
confluent login
```

**Create Job**:
```bash
confluent flink job create \
    --name "marketlag-job-1" \
    --sql-file "job1.sql" \
    --parallelism 2 \
    --checkpoint-interval 300000 \
    --checkpoint-mode EXACTLY_ONCE
```

**Use Case**: Automated deployments, CI/CD pipelines, scripting.

**MarketLag Project**: Can use CLI for automated deployments.

### API: REST API for Programmatic Deployment

**Example**:
```bash
curl -X POST "https://api.confluent.cloud/flink/v1/environments/{env-id}/jobs" \
    -H "Authorization: Basic <credentials>" \
    -H "Content-Type: application/json" \
    -d '{
        "name": "marketlag-job-1",
        "sql": "...",
        "parallelism": 2,
        "checkpoint_interval": 300000
    }'
```

**Use Case**: Custom deployment tools, integration with other systems.

**MarketLag Project**: Can use API for custom deployment automation.

---

## Job Configuration

### Parallelism: 2 (for MVP)

```bash
--parallelism 2
```

**MarketLag Project**: Uses parallelism=2 to match 2 CFU allocation.

**Why**: 2 CFU provides 2 parallel subtasks. Setting parallelism=2 ensures optimal resource usage.

### Checkpoint Interval: 300000 (5 minutes)

```bash
--checkpoint-interval 300000
```

**MarketLag Project**: Uses 5-minute checkpoint interval (300,000 milliseconds).

**Why**: Balance between recovery time and checkpoint overhead.

### Checkpoint Mode: EXACTLY_ONCE

```bash
--checkpoint-mode EXACTLY_ONCE
```

**MarketLag Project**: Uses EXACTLY_ONCE for correctness.

**Why**: Ensures each record is processed exactly once, even after failures.

---

## Job Monitoring: UI Dashboard, Metrics, Logs

### UI Dashboard

Confluent Cloud UI provides:
- **Job Status**: Running, stopped, failed
- **Metrics**: Throughput, latency, checkpoint success rate
- **Logs**: Job execution logs
- **Graph**: Job graph visualization

**Use Case**: Real-time monitoring, debugging, troubleshooting.

### Metrics

Key metrics to monitor:
- **Throughput**: Records processed per second
- **Latency**: End-to-end latency
- **Checkpoint Success Rate**: Percentage of successful checkpoints
- **Consumer Lag**: Kafka consumer lag

**MarketLag Project**: Monitor these metrics for all three jobs.

### Logs

Access logs through:
- **UI**: View logs in Confluent Cloud UI
- **CLI**: `confluent flink job logs <job-id>`
- **API**: REST API endpoint for logs

**Use Case**: Debugging, troubleshooting, audit trail.

---

## Job Update: Versioning, Rolling Updates

### Versioning

Confluent Cloud supports job versioning:
- **Version History**: Track job versions
- **Rollback**: Roll back to previous version
- **Comparison**: Compare versions

**Use Case**: Safe deployments, rollback on issues.

### Rolling Updates

Confluent Cloud performs rolling updates:
- **Zero Downtime**: Updates without stopping job
- **Gradual Rollout**: Updates subtasks gradually
- **Automatic Rollback**: Roll back on errors

**Use Case**: Production updates without downtime.

---

## Minimum Viable Code

### SQL Job File (job1.sql)

```sql
-- MarketLag Job 1: RSS Signal Aggregation
CREATE TABLE rss_events (
    -- schema
) WITH (
    'connector' = 'kafka',
    -- configuration
);

CREATE TABLE rss_signals_hourly (
    -- schema
) WITH (
    'connector' = 'kafka',
    -- configuration
);

INSERT INTO rss_signals_hourly
SELECT
    market_slug,
    TUMBLE_START(published_at, INTERVAL '1' HOUR) as window_start,
    COUNT(*) as mention_count,
    SUM(keyword_score) as keyword_score,
    AVG(article_score * source_weight) as source_weighted_signal
FROM rss_events
GROUP BY market_slug, TUMBLE(published_at, INTERVAL '1' HOUR);
```

### CLI Deployment

```bash
confluent flink job create \
    --name "marketlag-job-1" \
    --sql-file "job1.sql" \
    --parallelism 2 \
    --checkpoint-interval 300000 \
    --checkpoint-mode EXACTLY_ONCE
```

---

## Common Mistakes

1. **错误的并行度**：
   - ❌ 设置并行度高于 CFU 分配
   - ✅ 使并行度与 CFU 匹配（2 CFU = 并行度 2）

2. **缺少检查点配置**：
   - ❌ 未配置检查点间隔
   - ✅ 始终配置：checkpoint-interval、checkpoint-mode

3. **错误的 SQL 文件**：
   - ❌ 部署错误的 SQL 文件
   - ✅ 在部署前验证 SQL 文件

4. **部署后未监控**：
   - ❌ 部署后忘记监控
   - ✅ 部署后监控指标和日志

5. **未在本地测试**：
   - ❌ 未在本地测试就部署
   - ✅ 先在本地测试（第 10.1 节）

---

## Mind Trigger: When to Think About This

在以下情况下考虑部署 Flink 作业：
- **部署到生产环境**：MarketLag 作业需要部署到 Confluent Cloud
- **选择部署方法**：根据用例选择 UI、CLI 或 API
- **配置作业**：设置并行度、检查点间隔、检查点模式
- **监控作业**：第 6.4 节详细介绍监控
- **更新作业**：理解版本控制和滚动更新

**在 MarketLag 项目中**：所有三个作业都部署到 Confluent Cloud。使用 parallelism=2、checkpoint-interval=300000、checkpoint-mode=EXACTLY_ONCE。通过 UI 仪表板和指标监控作业。

---

## Summary

Flink 作业可以通过 UI（Web 界面）、CLI（命令行）或 API（REST API）部署到 Confluent Cloud。作业配置包括并行度（MVP 为 2）、检查点间隔（300000 = 5 分钟）和检查点模式（EXACTLY_ONCE）。通过 UI 仪表板、指标和日志监控作业。更新支持版本控制和滚动更新。MarketLag 对所有三个作业使用这些部署方法。理解部署对于生产操作至关重要。

