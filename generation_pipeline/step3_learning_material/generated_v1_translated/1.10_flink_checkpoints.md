# Flink Checkpoints

**Learning Point**: 1.10 from knowledge_and_skills_needed_v2.md
**Prerequisites**: 1.8 (State Backend), 理解容错
**Version**: Flink 2.2.0

---

## Definition (Engineering Language)

**Checkpoint**: 在特定时间点整个 Flink 作业状态的一致快照。Checkpoint 包括算子状态、键控状态和 source 位置。Checkpoint 使 Flink 能够通过将状态恢复到最后一个成功的 checkpoint 来从故障中恢复。

**Checkpoint Interval**: checkpoint 之间的时间间隔。较短的间隔提供更快的恢复但开销更高。

**Checkpoint Mode**: EXACTLY_ONCE（默认）确保恢复后每条记录恰好处理一次。AT_LEAST_ONCE 允许重复但更快。

**Checkpoint Storage**: checkpoint 数据持久化的位置（文件系统、S3 等）。**Confluent Cloud 使用 S3 兼容存储**。

**Savepoint**: 手动触发的 checkpoint，用于计划停止、版本升级或状态迁移。与 checkpoint 不同，savepoint 在作业取消后仍然存在。

---

## Plain Language Explanation

将 checkpoint 想象成保存游戏进度：

- **Checkpoint** = 保存游戏：此时所有内容（状态、位置）的快照
- **Checkpoint Interval** = 保存频率：每 5 分钟（MarketLag 使用此方式）
- **Recovery** = 加载游戏：如果出现问题，从上次保存恢复
- **Savepoint** = 手动保存：您决定何时保存（用于升级、迁移）

**为什么需要**: 如果 TaskManager 崩溃，Flink 可以从最后一个 checkpoint 恢复并从中断处继续处理，确保没有数据丢失。

---

## Analogy

如果您了解**数据库备份**（您了解），checkpoint 类似：
- 数据库备份 = Flink checkpoint（状态快照）
- 备份频率 = Checkpoint interval（备份频率）
- 从备份恢复 = Flink recovery（从 checkpoint 恢复）

关键区别：Flink checkpoint 是增量的且轻量级，在后台自动进行。

---

## Relationship to Already Learned Topics

- **1.8 State Backend**: Checkpoint 从 state backend 保存状态（MarketLag 中使用 RocksDB）
- **1.7 State Types**: 所有状态类型（ValueState、MapState）都包含在 checkpoint 中
- **Fault Tolerance**: Checkpoint 实现容错 - Flink 核心功能
- **S3 Storage**: 您从 AWS 了解 S3 - Confluent Cloud 使用 S3 进行 checkpoint 存储

---

## Pseudocode (From Source Code)

Based on Flink 2.2.0 source code:

```java
// Checkpoint (simplified)
class Checkpoint {
    long checkpointId;
    long timestamp;
    Map<OperatorID, OperatorState> operatorStates;
    Map<SourceID, SourcePosition> sourcePositions;

    void save() {
        // 1. Trigger checkpoint at all operators
        // 2. Each operator saves its state
        // 3. Save source positions (Kafka offsets)
        // 4. Write to checkpoint storage (S3)
    }
}

// Checkpoint coordinator (simplified)
class CheckpointCoordinator {
    void triggerCheckpoint(long interval) {
        while (jobRunning) {
            Thread.sleep(interval);
            Checkpoint checkpoint = createCheckpoint();
            checkpoint.save();
        }
    }
}
```

---

## Source Code References

**Local Path**: `generation_pipeline/step0_context/source_codes/flink/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java`

**GitHub URL**: https://github.com/apache/flink/blob/release-2.2/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java

**Key Classes**:
- `CheckpointCoordinator`: 协调 checkpoint 创建
- `CompletedCheckpoint`: 表示已完成的 checkpoint
- `CheckpointStorage`: checkpoint 存储接口

---

## Checkpoint Concept: Consistent Snapshots

### What Gets Checkpointed

1. **Operator State**: 所有算子的状态（ValueState、MapState 等）
2. **Source Positions**: Kafka 偏移量、文件位置等
3. **Metadata**: Checkpoint ID、时间戳、作业图

### Consistency Guarantee

Checkpoint 是**一致的** - 所有算子在相同的逻辑点保存状态：
- 所有算子看到相同的输入记录集
- 状态反映到该点的处理
- checkpoint 中没有部分更新

**Example**: 如果在处理 1000 条记录后获取 checkpoint，所有算子的状态恰好反映这 1000 条记录。

---

## Checkpoint Configuration

### Checkpoint Interval

```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Enable checkpoints
env.enableCheckpointing(300000);  // 5 minutes (300,000 ms) - MarketLag uses this
```

**MarketLag Project**: Uses 5-minute checkpoint interval (300,000 milliseconds).

### Checkpoint Mode

```java
// EXACTLY_ONCE (default) - each record processed exactly once
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

// AT_LEAST_ONCE - faster, but may process records multiple times
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.AT_LEAST_ONCE);
```

**MarketLag Project**: Uses EXACTLY_ONCE mode for correctness.

### Checkpoint Timeout

```java
// How long to wait for checkpoint to complete
env.getCheckpointConfig().setCheckpointTimeout(600000);  // 10 minutes
```

If checkpoint doesn't complete within timeout, it's aborted.

---

## Checkpoint Storage: Filesystem, S3-Compatible

### Filesystem Storage (Local/Development)

```java
env.getCheckpointConfig().setCheckpointStorage("file:///path/to/checkpoints");
```

### S3-Compatible Storage (Confluent Cloud)

```java
// Confluent Cloud automatically configures S3 checkpoint storage
env.getCheckpointConfig().setCheckpointStorage("s3://bucket-name/checkpoints");
```

**Confluent Cloud**: Automatically uses S3-compatible storage for checkpoints. No manual configuration needed.

### Checkpoint Storage Configuration

```java
CheckpointStorage checkpointStorage = new FileSystemCheckpointStorage("s3://bucket/checkpoints");
env.getCheckpointConfig().setCheckpointStorage(checkpointStorage);
```

---

## Checkpoint Recovery: How Flink Restores from Checkpoints

### Recovery Process

1. **Job Restart**: Job is restarted (after failure or manual restart)
2. **Find Latest Checkpoint**: Flink finds the most recent successful checkpoint
3. **Restore State**: All operators restore their state from checkpoint
4. **Restore Source Positions**: Sources resume from saved positions (Kafka offsets)
5. **Continue Processing**: Job continues from checkpoint position

### Automatic Recovery

Flink automatically recovers from the latest checkpoint on:
- TaskManager failure
- JobManager failure (with high availability)
- Manual job restart

**No data loss**: Processing continues exactly from where it left off.

---

## Savepoints vs Checkpoints

### Checkpoints
- **Trigger**: Automatic (based on interval)
- **Purpose**: Fault tolerance, fast recovery
- **Retention**: Automatically cleaned up (keep last N)
- **Survival**: Lost when job is cancelled
- **Use Case**: Production fault tolerance

### Savepoints
- **Trigger**: Manual (via CLI or API)
- **Purpose**: Planned stops, version upgrades, state migration
- **Retention**: Manual cleanup
- **Survival**: Survive job cancellation
- **Use Case**: Version upgrades, A/B testing, state migration

### Creating Savepoints

```bash
# Create savepoint
flink savepoint <job-id> <savepoint-path>

# Stop job with savepoint
flink cancel -s <savepoint-path> <job-id>

# Restore from savepoint
flink run -s <savepoint-path> <jar-file>
```

---

## Minimum Viable Code

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.CheckpointingMode;
import org.apache.flink.core.fs.Path;

public class CheckpointDemo {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // Enable checkpoints with 5-minute interval (MarketLag pattern)
        env.enableCheckpointing(300000);  // 300,000 ms = 5 minutes

        // Configure checkpoint mode
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

        // Set checkpoint timeout
        env.getCheckpointConfig().setCheckpointTimeout(600000);  // 10 minutes

        // Configure checkpoint storage (S3 in Confluent Cloud)
        env.getCheckpointConfig().setCheckpointStorage("s3://my-bucket/checkpoints");

        // Keep only last 3 checkpoints
        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);
        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);

        // Your job logic
        DataStream<Event> stream = env.fromElements(...);
        stream.process(...);

        env.execute("Checkpoint Demo");
    }
}
```

---

## Checkpoint Configuration in Confluent Cloud

Confluent Cloud automatically configures checkpoints:

- **Checkpoint Interval**: Configurable (MarketLag uses 5 minutes)
- **Checkpoint Mode**: EXACTLY_ONCE
- **Checkpoint Storage**: S3-compatible (automatic)
- **Checkpoint Retention**: Automatic cleanup

**MarketLag Configuration**:
- Checkpoint interval: 300,000 ms (5 minutes)
- Checkpoint mode: EXACTLY_ONCE
- Storage: S3 (automatic in Confluent Cloud)

---

## Common Mistakes

1. **Checkpoint 间隔太短**:
   - ❌ 1 秒间隔（高开销，影响性能）
   - ✅ 平衡恢复时间与开销（MarketLag 使用 5 分钟）

2. **Checkpoint 超时太短**:
   - ❌ 1 分钟超时（大状态时 checkpoint 失败）
   - ✅ 根据状态大小设置超时（大状态为 10+ 分钟）

3. **不配置 checkpoint 存储**:
   - ❌ 使用默认值（可能不会在重启后持久化）
   - ✅ 配置 S3 或文件系统存储（Confluent Cloud 这样做）

4. **Checkpoint 静默失败**:
   - ❌ 不监控 checkpoint 成功率
   - ✅ 监控 checkpoint 指标，设置警报

5. **不理解 savepoint**:
   - ❌ 使用 checkpoint 进行版本升级
   - ✅ 对计划停止、升级使用 savepoint

---

## Mind Trigger: When to Think About This

在以下情况下考虑 checkpoint：
- **配置容错**: MarketLag 使用 5 分钟 checkpoint
- **部署到生产环境**: Checkpoint 对生产可靠性至关重要
- **故障排查**: 检查 checkpoint 成功率，调查故障
- **规划升级**: 对版本升级使用 savepoint
- **状态大小规划**: 大状态影响 checkpoint 持续时间

**在 MarketLag 项目中**: 使用 5 分钟 checkpoint，模式为 EXACTLY_ONCE。Checkpoint 存储在 S3 中（在 Confluent Cloud 中自动）。状态（max_signal_delta 的 MapState）包含在 checkpoint 中，实现无数据丢失的恢复。

---

## Summary

Checkpoint 是 Flink 作业状态的一致快照，实现容错。MarketLag 使用 5 分钟 checkpoint 间隔，模式为 EXACTLY_ONCE。Checkpoint 在 Confluent Cloud 中存储在 S3 中。Flink 在故障时自动从最新的 checkpoint 恢复。Savepoint 是用于计划停止和升级的手动 checkpoint。理解 checkpoint 对于生产部署和容错至关重要。

